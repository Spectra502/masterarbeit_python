{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e019071b-b915-4e05-93ac-b1d4ccc67773",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetFolderHBK = r\"G:\\Semester Arbeit\\Programming\\Extracted_Features\\HBK\\HBK_14285Hz_original_all_features\\features\"\n",
    "targetFolderMCC5 = r\"G:\\Semester Arbeit\\Programming\\Extracted_Features\\MCC5\\MCC5_12800Hz_original_all_features_motor_vibration_x\\features\"\n",
    "targetFolderSIZA = r\"G:\\Semester Arbeit\\Programming\\Extracted_Features\\SIZA\\SIZA_original_all_features\\features\"\n",
    "normalization_method = \"robust_scaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24c9ea-6eb1-4abc-a4a4-735cf1178187",
   "metadata": {},
   "source": [
    "# Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eaba3c8-a35f-4004-b6a5-a62b3eec640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, hamming_loss, hinge_loss, jaccard_score, log_loss, precision_score, recall_score, f1_score, make_scorer\n",
    "from pathlib import Path\n",
    "from pycaret.classification import * \n",
    "from torch import tensor\n",
    "from torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\n",
    "import optuna\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import GANDALFConfig, CategoryEmbeddingModel,GatedAdditiveTreeEnsembleConfig, NodeConfig, FTTransformerConfig, TabNetModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    ModelConfig,\n",
    "    TrainerConfig,\n",
    "    ExperimentConfig,\n",
    ")\n",
    "from collections import Counter\n",
    "from data_loader import load_feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3d6c1-e829-4aee-b5f4-19a277312efd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2758484f-d1a4-477c-a32f-e760037badc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataframe(dataframe, normalization_method):\n",
    "    \"\"\"\n",
    "    Normalizes the features of a dataframe using a specified method.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with a 'Label' column.\n",
    "        normalization_method (str): The method to use (\"min_max\", \"z_score\", \"robust_scaling\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with scaled features.\n",
    "    \"\"\"\n",
    "    # Separate features (X) and the target variable (y)\n",
    "    y = dataframe['Label']\n",
    "    X = dataframe.drop(columns=['Label'])\n",
    "\n",
    "    # Select the scaler based on the chosen method\n",
    "    if normalization_method == \"min_max\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif normalization_method == \"z_score\":\n",
    "        scaler = StandardScaler()\n",
    "    elif normalization_method == \"robust_scaling\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        # Raise an error for an invalid method name\n",
    "        raise ValueError(f\"Unknown normalization_method: '{normalization_method}'\")\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Rejoin the scaled features with the label column\n",
    "    df_scaled = X_scaled.join(y)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44aae2d5-3c4b-4bbb-bbbc-6621172653d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredictionHistograms(df, domain, normalization):\n",
    "    # 1) mark correct vs incorrect\n",
    "    df = df.copy()\n",
    "    df['prediction_quality'] = np.where(\n",
    "        df['Label'] == df['prediction_label'],\n",
    "        'correct',\n",
    "        'incorrect'\n",
    "    )\n",
    "    \n",
    "    # 2) choose a palette (you can override these colors if you like)\n",
    "    pal = dict(zip(\n",
    "        ['correct','incorrect'],\n",
    "        sns.color_palette(n_colors=2)\n",
    "    ))\n",
    "    \n",
    "    skip = {'Label','prediction_label','prediction_score','prediction_quality'}\n",
    "    for col in df.columns:\n",
    "        if col in skip:\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        sns.histplot(\n",
    "            data=df, x=col, hue='prediction_quality',\n",
    "            palette=pal,\n",
    "            kde=True, multiple='layer', element='step',\n",
    "            alpha=0.5,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        # 3) build a manual legend using the same palette\n",
    "        handles = [\n",
    "            mpatches.Patch(color=pal[k], label=k)\n",
    "            for k in ['correct','incorrect']\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=handles,\n",
    "            title='Prediction Quality'\n",
    "        )\n",
    "        \n",
    "        ax.set_title(\n",
    "            f\"Distribution of {col} in the '{domain}' domain\\n\"\n",
    "            f\"(normalization = '{normalization}')\"\n",
    "        )\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7180ffc-82c3-4100-a659-90dfe8bdeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_predictions(df):\n",
    "    return df[\n",
    "        ((df['Label'] == 'damaged')   & (df['prediction_label'] == 'healthy'))\n",
    "      | ((df['Label'] == 'healthy')  & (df['prediction_label'] == 'damaged'))\n",
    "    ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6effaf0-7730-46e4-9b29-a6133fc41f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_df(model, df):\n",
    "    importance = model.feature_importances_\n",
    "    n = len(importance)\n",
    "    features = df.columns[:n]\n",
    "    fi_df = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'importance': importance\n",
    "    })\n",
    "    return fi_df.sort_values(by='importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d896eca8-c332-4e1a-8643-647da5fb5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_feature_importance_df(model, df):\n",
    "    if not hasattr(model, 'coef_'):\n",
    "        raise ValueError(\"This SVM model has no coefficients. Use a linear kernel.\")\n",
    "    \n",
    "    importance = model.coef_.ravel()  # Flatten in case of binary classification\n",
    "    n = len(importance)\n",
    "    features = df.columns[:n]\n",
    "    fi_df = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'importance': abs(importance)\n",
    "    })\n",
    "    return fi_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3ffb6f-eed8-4a5d-9b91-737370fc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counter = Counter()\n",
    "def add_top_features(feature_df: pd.DataFrame, top_n: int):\n",
    "    top_features = feature_df.nlargest(top_n, 'importance')['Features']\n",
    "    feature_counter.update(top_features)\n",
    "    \n",
    "def plot_feature_importance():\n",
    "    feature_freq = pd.DataFrame(feature_counter.items(), columns=['Feature', 'Count'])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=feature_freq.sort_values(by='Count', ascending=False),\n",
    "                x='Feature', y='Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Feature Frequency Across Experiments')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dc31c-45ef-49c4-9f58-7890b966ccd7",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13cd2ff-10b7-4db8-9fc8-f968e9c5324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"Best_Hyperparameters_z_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ba897d-d88c-410a-9597-96585f9a7b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 161 files into a DataFrame with shape (200093, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (200093, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_HBK = load_feature_data(\n",
    "    features_path=targetFolderHBK,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b37f31-8878-4168-aa49-a720052e2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 36 files into a DataFrame with shape (53928, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (53928, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_SIZA = load_feature_data(\n",
    "    features_path=targetFolderSIZA,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87904925-1ca9-40f1-adec-82912fb5bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 36 files into a DataFrame with shape (53928, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (53928, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_MCC5 = load_feature_data(\n",
    "    features_path=targetFolderMCC5,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb35e28-7098-45a1-bb31-899fd8610880",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_binary_HBK, df_binary_SIZA, df_binary_MCC5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d994ee23-ed86-4e93-b379-c145716083cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_df = normalizeDataframe(combined_df, normalization_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d0ded1-6f65-4a37-bb3c-5a59f1d1964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_training_normalized, features_df_testing_normalized = train_test_split(\n",
    "    normalized_df, \n",
    "    test_size=0.2,    # e.g., 20% for testing\n",
    "    random_state=42   # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27e3ca-e055-4837-9084-3b94820aec39",
   "metadata": {},
   "source": [
    "# Experiment Setup (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3aea3-e600-4a35-92b7-ea5d9981ebe5",
   "metadata": {},
   "source": [
    "## Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4ff813-0eb2-45b8-a876-ec6929d1b040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3467_row9_col1, #T_d3467_row16_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3467\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d3467_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_d3467_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d3467_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_d3467_row0_col1\" class=\"data row0 col1\" >7641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d3467_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_d3467_row1_col1\" class=\"data row1 col1\" >Label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d3467_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_d3467_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d3467_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_d3467_row3_col1\" class=\"data row3 col1\" >damaged: 0, healthy: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d3467_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_d3467_row4_col1\" class=\"data row4 col1\" >(246359, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d3467_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_d3467_row5_col1\" class=\"data row5 col1\" >(246359, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d3467_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_d3467_row6_col1\" class=\"data row6 col1\" >(172451, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d3467_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_d3467_row7_col1\" class=\"data row7 col1\" >(73908, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d3467_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_d3467_row8_col1\" class=\"data row8 col1\" >27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d3467_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_d3467_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d3467_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_d3467_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d3467_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_d3467_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_d3467_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_d3467_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_d3467_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_d3467_row13_col1\" class=\"data row13 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_d3467_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_d3467_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_d3467_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_d3467_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_d3467_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_d3467_row16_col1\" class=\"data row16 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_d3467_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_d3467_row17_col1\" class=\"data row17 col1\" >MlflowLogger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_d3467_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_d3467_row18_col1\" class=\"data row18 col1\" >Best_Hyperparameters_z_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3467_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_d3467_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_d3467_row19_col1\" class=\"data row19 col1\" >8e8a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x228292803d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    }
   ],
   "source": [
    "experiment = setup(features_df_training_normalized, target='Label', log_experiment = True, experiment_name = experiment_name, use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6300a-a54c-480c-8735-026fdbb0fd45",
   "metadata": {},
   "source": [
    "## Add aditional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb3d97b-cb84-42ed-b00b-ec01fcc7bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                                                          Log Loss\n",
       "Display Name                                                  Log Loss\n",
       "Score Function       <pycaret.internal.metrics.EncodedDecodedLabels...\n",
       "Scorer               make_scorer(log_loss, greater_is_better=False,...\n",
       "Target                                                      pred_proba\n",
       "Args                                                                {}\n",
       "Greater is Better                                                False\n",
       "Multiclass                                                        True\n",
       "Custom                                                            True\n",
       "Name: log_loss, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification metrics\n",
    "add_metric('balanced_acc', 'Balance Acc', balanced_accuracy_score, target='pred', greater_is_better=True)\n",
    "add_metric('hamming_loss', 'Hamming Loss', hamming_loss, target='pred', greater_is_better=False)\n",
    "add_metric('jaccard_score', 'Jaccard Score', jaccard_score, target='pred', greater_is_better=True)\n",
    "add_metric('log_loss', 'Log Loss', log_loss, target='pred_proba', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe742ef7-ec02-4e73-b2a5-f2b36eee0066",
   "metadata": {},
   "source": [
    "# Macro\n",
    "add_metric('precision_macro', 'Precision Macro', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_macro', 'Recall Macro', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_macro', 'F1 Macro', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "# Micro\n",
    "add_metric('precision_micro', 'Precision Micro', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_micro', 'Recall Micro', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_micro', 'F1 Micro', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "# Weighted\n",
    "add_metric('precision_weighted', 'Precision Weighted', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_weighted', 'Recall Weighted', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_weighted', 'F1 Weighted', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9faa1a3-cf7b-46ea-84b4-6b15167e1e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Display Name</th>\n",
       "      <th>Score Function</th>\n",
       "      <th>Scorer</th>\n",
       "      <th>Target</th>\n",
       "      <th>Args</th>\n",
       "      <th>Greater is Better</th>\n",
       "      <th>Multiclass</th>\n",
       "      <th>Custom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>&lt;function accuracy_score at 0x0000022812267370&gt;</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>AUC</td>\n",
       "      <td>AUC</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(roc_auc_score, response_method=('d...</td>\n",
       "      <td>pred_proba</td>\n",
       "      <td>{'average': 'weighted', 'multi_class': 'ovr'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>Recall</td>\n",
       "      <td>Recall</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(recall_score, response_method='pre...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>Precision</td>\n",
       "      <td>Prec.</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(precision_score, response_method='...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>F1</td>\n",
       "      <td>F1</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(f1_score, response_method='predict...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>Kappa</td>\n",
       "      <td>Kappa</td>\n",
       "      <td>&lt;function cohen_kappa_score at 0x0000022812267...</td>\n",
       "      <td>make_scorer(cohen_kappa_score, response_method...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcc</th>\n",
       "      <td>MCC</td>\n",
       "      <td>MCC</td>\n",
       "      <td>&lt;function matthews_corrcoef at 0x0000022812267...</td>\n",
       "      <td>make_scorer(matthews_corrcoef, response_method...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_acc</th>\n",
       "      <td>Balance Acc</td>\n",
       "      <td>Balance Acc</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(balanced_accuracy_score, response_...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamming_loss</th>\n",
       "      <td>Hamming Loss</td>\n",
       "      <td>Hamming Loss</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(hamming_loss, greater_is_better=Fa...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_score</th>\n",
       "      <td>Jaccard Score</td>\n",
       "      <td>Jaccard Score</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(jaccard_score, response_method='pr...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_loss</th>\n",
       "      <td>Log Loss</td>\n",
       "      <td>Log Loss</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(log_loss, greater_is_better=False,...</td>\n",
       "      <td>pred_proba</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name   Display Name  \\\n",
       "ID                                            \n",
       "acc                 Accuracy       Accuracy   \n",
       "auc                      AUC            AUC   \n",
       "recall                Recall         Recall   \n",
       "precision          Precision          Prec.   \n",
       "f1                        F1             F1   \n",
       "kappa                  Kappa          Kappa   \n",
       "mcc                      MCC            MCC   \n",
       "balanced_acc     Balance Acc    Balance Acc   \n",
       "hamming_loss    Hamming Loss   Hamming Loss   \n",
       "jaccard_score  Jaccard Score  Jaccard Score   \n",
       "log_loss            Log Loss       Log Loss   \n",
       "\n",
       "                                                  Score Function  \\\n",
       "ID                                                                 \n",
       "acc              <function accuracy_score at 0x0000022812267370>   \n",
       "auc            <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "recall         <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "precision      <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "f1             <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "kappa          <function cohen_kappa_score at 0x0000022812267...   \n",
       "mcc            <function matthews_corrcoef at 0x0000022812267...   \n",
       "balanced_acc   <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "hamming_loss   <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "jaccard_score  <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "log_loss       <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "\n",
       "                                                          Scorer      Target  \\\n",
       "ID                                                                             \n",
       "acc                                                     accuracy        pred   \n",
       "auc            make_scorer(roc_auc_score, response_method=('d...  pred_proba   \n",
       "recall         make_scorer(recall_score, response_method='pre...        pred   \n",
       "precision      make_scorer(precision_score, response_method='...        pred   \n",
       "f1             make_scorer(f1_score, response_method='predict...        pred   \n",
       "kappa          make_scorer(cohen_kappa_score, response_method...        pred   \n",
       "mcc            make_scorer(matthews_corrcoef, response_method...        pred   \n",
       "balanced_acc   make_scorer(balanced_accuracy_score, response_...        pred   \n",
       "hamming_loss   make_scorer(hamming_loss, greater_is_better=Fa...        pred   \n",
       "jaccard_score  make_scorer(jaccard_score, response_method='pr...        pred   \n",
       "log_loss       make_scorer(log_loss, greater_is_better=False,...  pred_proba   \n",
       "\n",
       "                                                        Args  \\\n",
       "ID                                                             \n",
       "acc                                                       {}   \n",
       "auc            {'average': 'weighted', 'multi_class': 'ovr'}   \n",
       "recall                               {'average': 'weighted'}   \n",
       "precision                            {'average': 'weighted'}   \n",
       "f1                                   {'average': 'weighted'}   \n",
       "kappa                                                     {}   \n",
       "mcc                                                       {}   \n",
       "balanced_acc                                              {}   \n",
       "hamming_loss                                              {}   \n",
       "jaccard_score                                             {}   \n",
       "log_loss                                                  {}   \n",
       "\n",
       "               Greater is Better  Multiclass  Custom  \n",
       "ID                                                    \n",
       "acc                         True        True   False  \n",
       "auc                         True        True   False  \n",
       "recall                      True        True   False  \n",
       "precision                   True        True   False  \n",
       "f1                          True        True   False  \n",
       "kappa                       True        True   False  \n",
       "mcc                         True        True   False  \n",
       "balanced_acc                True        True    True  \n",
       "hamming_loss               False        True    True  \n",
       "jaccard_score               True        True    True  \n",
       "log_loss                   False        True    True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics = get_metrics()\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a5e119-0bbb-4ee9-8da6-e10a63eaaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations_tuning = 10\n",
    "optimized_metric = 'F1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de3f05-6198-4ae2-adba-d294aee48b08",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0ce27b-40e1-468f-945e-ee949316a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8640_row10_col0, #T_e8640_row10_col1, #T_e8640_row10_col2, #T_e8640_row10_col3, #T_e8640_row10_col4, #T_e8640_row10_col5, #T_e8640_row10_col6, #T_e8640_row10_col7, #T_e8640_row10_col8, #T_e8640_row10_col9, #T_e8640_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8640\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8640_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_e8640_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_e8640_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_e8640_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_e8640_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_e8640_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_e8640_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_e8640_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_e8640_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_e8640_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_e8640_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e8640_row0_col0\" class=\"data row0 col0\" >0.9533</td>\n",
       "      <td id=\"T_e8640_row0_col1\" class=\"data row0 col1\" >0.9868</td>\n",
       "      <td id=\"T_e8640_row0_col2\" class=\"data row0 col2\" >0.9533</td>\n",
       "      <td id=\"T_e8640_row0_col3\" class=\"data row0 col3\" >0.9541</td>\n",
       "      <td id=\"T_e8640_row0_col4\" class=\"data row0 col4\" >0.9518</td>\n",
       "      <td id=\"T_e8640_row0_col5\" class=\"data row0 col5\" >0.8579</td>\n",
       "      <td id=\"T_e8640_row0_col6\" class=\"data row0 col6\" >0.8632</td>\n",
       "      <td id=\"T_e8640_row0_col7\" class=\"data row0 col7\" >0.9044</td>\n",
       "      <td id=\"T_e8640_row0_col8\" class=\"data row0 col8\" >0.0467</td>\n",
       "      <td id=\"T_e8640_row0_col9\" class=\"data row0 col9\" >0.7971</td>\n",
       "      <td id=\"T_e8640_row0_col10\" class=\"data row0 col10\" >0.1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e8640_row1_col0\" class=\"data row1 col0\" >0.9509</td>\n",
       "      <td id=\"T_e8640_row1_col1\" class=\"data row1 col1\" >0.9861</td>\n",
       "      <td id=\"T_e8640_row1_col2\" class=\"data row1 col2\" >0.9509</td>\n",
       "      <td id=\"T_e8640_row1_col3\" class=\"data row1 col3\" >0.9518</td>\n",
       "      <td id=\"T_e8640_row1_col4\" class=\"data row1 col4\" >0.9493</td>\n",
       "      <td id=\"T_e8640_row1_col5\" class=\"data row1 col5\" >0.8505</td>\n",
       "      <td id=\"T_e8640_row1_col6\" class=\"data row1 col6\" >0.8562</td>\n",
       "      <td id=\"T_e8640_row1_col7\" class=\"data row1 col7\" >0.8999</td>\n",
       "      <td id=\"T_e8640_row1_col8\" class=\"data row1 col8\" >0.0491</td>\n",
       "      <td id=\"T_e8640_row1_col9\" class=\"data row1 col9\" >0.7874</td>\n",
       "      <td id=\"T_e8640_row1_col10\" class=\"data row1 col10\" >0.1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e8640_row2_col0\" class=\"data row2 col0\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row2_col1\" class=\"data row2 col1\" >0.9870</td>\n",
       "      <td id=\"T_e8640_row2_col2\" class=\"data row2 col2\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row2_col3\" class=\"data row2 col3\" >0.9538</td>\n",
       "      <td id=\"T_e8640_row2_col4\" class=\"data row2 col4\" >0.9512</td>\n",
       "      <td id=\"T_e8640_row2_col5\" class=\"data row2 col5\" >0.8561</td>\n",
       "      <td id=\"T_e8640_row2_col6\" class=\"data row2 col6\" >0.8619</td>\n",
       "      <td id=\"T_e8640_row2_col7\" class=\"data row2 col7\" >0.9025</td>\n",
       "      <td id=\"T_e8640_row2_col8\" class=\"data row2 col8\" >0.0472</td>\n",
       "      <td id=\"T_e8640_row2_col9\" class=\"data row2 col9\" >0.7945</td>\n",
       "      <td id=\"T_e8640_row2_col10\" class=\"data row2 col10\" >0.1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e8640_row3_col0\" class=\"data row3 col0\" >0.9532</td>\n",
       "      <td id=\"T_e8640_row3_col1\" class=\"data row3 col1\" >0.9870</td>\n",
       "      <td id=\"T_e8640_row3_col2\" class=\"data row3 col2\" >0.9532</td>\n",
       "      <td id=\"T_e8640_row3_col3\" class=\"data row3 col3\" >0.9542</td>\n",
       "      <td id=\"T_e8640_row3_col4\" class=\"data row3 col4\" >0.9517</td>\n",
       "      <td id=\"T_e8640_row3_col5\" class=\"data row3 col5\" >0.8575</td>\n",
       "      <td id=\"T_e8640_row3_col6\" class=\"data row3 col6\" >0.8631</td>\n",
       "      <td id=\"T_e8640_row3_col7\" class=\"data row3 col7\" >0.9037</td>\n",
       "      <td id=\"T_e8640_row3_col8\" class=\"data row3 col8\" >0.0468</td>\n",
       "      <td id=\"T_e8640_row3_col9\" class=\"data row3 col9\" >0.7965</td>\n",
       "      <td id=\"T_e8640_row3_col10\" class=\"data row3 col10\" >0.1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e8640_row4_col0\" class=\"data row4 col0\" >0.9519</td>\n",
       "      <td id=\"T_e8640_row4_col1\" class=\"data row4 col1\" >0.9870</td>\n",
       "      <td id=\"T_e8640_row4_col2\" class=\"data row4 col2\" >0.9519</td>\n",
       "      <td id=\"T_e8640_row4_col3\" class=\"data row4 col3\" >0.9531</td>\n",
       "      <td id=\"T_e8640_row4_col4\" class=\"data row4 col4\" >0.9501</td>\n",
       "      <td id=\"T_e8640_row4_col5\" class=\"data row4 col5\" >0.8528</td>\n",
       "      <td id=\"T_e8640_row4_col6\" class=\"data row4 col6\" >0.8592</td>\n",
       "      <td id=\"T_e8640_row4_col7\" class=\"data row4 col7\" >0.8998</td>\n",
       "      <td id=\"T_e8640_row4_col8\" class=\"data row4 col8\" >0.0481</td>\n",
       "      <td id=\"T_e8640_row4_col9\" class=\"data row4 col9\" >0.7901</td>\n",
       "      <td id=\"T_e8640_row4_col10\" class=\"data row4 col10\" >0.1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e8640_row5_col0\" class=\"data row5 col0\" >0.9540</td>\n",
       "      <td id=\"T_e8640_row5_col1\" class=\"data row5 col1\" >0.9871</td>\n",
       "      <td id=\"T_e8640_row5_col2\" class=\"data row5 col2\" >0.9540</td>\n",
       "      <td id=\"T_e8640_row5_col3\" class=\"data row5 col3\" >0.9549</td>\n",
       "      <td id=\"T_e8640_row5_col4\" class=\"data row5 col4\" >0.9525</td>\n",
       "      <td id=\"T_e8640_row5_col5\" class=\"data row5 col5\" >0.8603</td>\n",
       "      <td id=\"T_e8640_row5_col6\" class=\"data row5 col6\" >0.8654</td>\n",
       "      <td id=\"T_e8640_row5_col7\" class=\"data row5 col7\" >0.9059</td>\n",
       "      <td id=\"T_e8640_row5_col8\" class=\"data row5 col8\" >0.0460</td>\n",
       "      <td id=\"T_e8640_row5_col9\" class=\"data row5 col9\" >0.8003</td>\n",
       "      <td id=\"T_e8640_row5_col10\" class=\"data row5 col10\" >0.1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e8640_row6_col0\" class=\"data row6 col0\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row6_col1\" class=\"data row6 col1\" >0.9866</td>\n",
       "      <td id=\"T_e8640_row6_col2\" class=\"data row6 col2\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row6_col3\" class=\"data row6 col3\" >0.9538</td>\n",
       "      <td id=\"T_e8640_row6_col4\" class=\"data row6 col4\" >0.9512</td>\n",
       "      <td id=\"T_e8640_row6_col5\" class=\"data row6 col5\" >0.8562</td>\n",
       "      <td id=\"T_e8640_row6_col6\" class=\"data row6 col6\" >0.8619</td>\n",
       "      <td id=\"T_e8640_row6_col7\" class=\"data row6 col7\" >0.9027</td>\n",
       "      <td id=\"T_e8640_row6_col8\" class=\"data row6 col8\" >0.0472</td>\n",
       "      <td id=\"T_e8640_row6_col9\" class=\"data row6 col9\" >0.7947</td>\n",
       "      <td id=\"T_e8640_row6_col10\" class=\"data row6 col10\" >0.1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e8640_row7_col0\" class=\"data row7 col0\" >0.9555</td>\n",
       "      <td id=\"T_e8640_row7_col1\" class=\"data row7 col1\" >0.9876</td>\n",
       "      <td id=\"T_e8640_row7_col2\" class=\"data row7 col2\" >0.9555</td>\n",
       "      <td id=\"T_e8640_row7_col3\" class=\"data row7 col3\" >0.9561</td>\n",
       "      <td id=\"T_e8640_row7_col4\" class=\"data row7 col4\" >0.9542</td>\n",
       "      <td id=\"T_e8640_row7_col5\" class=\"data row7 col5\" >0.8653</td>\n",
       "      <td id=\"T_e8640_row7_col6\" class=\"data row7 col6\" >0.8697</td>\n",
       "      <td id=\"T_e8640_row7_col7\" class=\"data row7 col7\" >0.9101</td>\n",
       "      <td id=\"T_e8640_row7_col8\" class=\"data row7 col8\" >0.0445</td>\n",
       "      <td id=\"T_e8640_row7_col9\" class=\"data row7 col9\" >0.8071</td>\n",
       "      <td id=\"T_e8640_row7_col10\" class=\"data row7 col10\" >0.1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e8640_row8_col0\" class=\"data row8 col0\" >0.9519</td>\n",
       "      <td id=\"T_e8640_row8_col1\" class=\"data row8 col1\" >0.9869</td>\n",
       "      <td id=\"T_e8640_row8_col2\" class=\"data row8 col2\" >0.9519</td>\n",
       "      <td id=\"T_e8640_row8_col3\" class=\"data row8 col3\" >0.9527</td>\n",
       "      <td id=\"T_e8640_row8_col4\" class=\"data row8 col4\" >0.9503</td>\n",
       "      <td id=\"T_e8640_row8_col5\" class=\"data row8 col5\" >0.8535</td>\n",
       "      <td id=\"T_e8640_row8_col6\" class=\"data row8 col6\" >0.8590</td>\n",
       "      <td id=\"T_e8640_row8_col7\" class=\"data row8 col7\" >0.9018</td>\n",
       "      <td id=\"T_e8640_row8_col8\" class=\"data row8 col8\" >0.0481</td>\n",
       "      <td id=\"T_e8640_row8_col9\" class=\"data row8 col9\" >0.7914</td>\n",
       "      <td id=\"T_e8640_row8_col10\" class=\"data row8 col10\" >0.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e8640_row9_col0\" class=\"data row9 col0\" >0.9518</td>\n",
       "      <td id=\"T_e8640_row9_col1\" class=\"data row9 col1\" >0.9873</td>\n",
       "      <td id=\"T_e8640_row9_col2\" class=\"data row9 col2\" >0.9518</td>\n",
       "      <td id=\"T_e8640_row9_col3\" class=\"data row9 col3\" >0.9529</td>\n",
       "      <td id=\"T_e8640_row9_col4\" class=\"data row9 col4\" >0.9501</td>\n",
       "      <td id=\"T_e8640_row9_col5\" class=\"data row9 col5\" >0.8529</td>\n",
       "      <td id=\"T_e8640_row9_col6\" class=\"data row9 col6\" >0.8590</td>\n",
       "      <td id=\"T_e8640_row9_col7\" class=\"data row9 col7\" >0.9004</td>\n",
       "      <td id=\"T_e8640_row9_col8\" class=\"data row9 col8\" >0.0482</td>\n",
       "      <td id=\"T_e8640_row9_col9\" class=\"data row9 col9\" >0.7904</td>\n",
       "      <td id=\"T_e8640_row9_col10\" class=\"data row9 col10\" >0.1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_e8640_row10_col0\" class=\"data row10 col0\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row10_col1\" class=\"data row10 col1\" >0.9869</td>\n",
       "      <td id=\"T_e8640_row10_col2\" class=\"data row10 col2\" >0.9528</td>\n",
       "      <td id=\"T_e8640_row10_col3\" class=\"data row10 col3\" >0.9537</td>\n",
       "      <td id=\"T_e8640_row10_col4\" class=\"data row10 col4\" >0.9512</td>\n",
       "      <td id=\"T_e8640_row10_col5\" class=\"data row10 col5\" >0.8563</td>\n",
       "      <td id=\"T_e8640_row10_col6\" class=\"data row10 col6\" >0.8618</td>\n",
       "      <td id=\"T_e8640_row10_col7\" class=\"data row10 col7\" >0.9031</td>\n",
       "      <td id=\"T_e8640_row10_col8\" class=\"data row10 col8\" >0.0472</td>\n",
       "      <td id=\"T_e8640_row10_col9\" class=\"data row10 col9\" >0.7949</td>\n",
       "      <td id=\"T_e8640_row10_col10\" class=\"data row10 col10\" >0.1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8640_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_e8640_row11_col0\" class=\"data row11 col0\" >0.0012</td>\n",
       "      <td id=\"T_e8640_row11_col1\" class=\"data row11 col1\" >0.0004</td>\n",
       "      <td id=\"T_e8640_row11_col2\" class=\"data row11 col2\" >0.0012</td>\n",
       "      <td id=\"T_e8640_row11_col3\" class=\"data row11 col3\" >0.0011</td>\n",
       "      <td id=\"T_e8640_row11_col4\" class=\"data row11 col4\" >0.0013</td>\n",
       "      <td id=\"T_e8640_row11_col5\" class=\"data row11 col5\" >0.0041</td>\n",
       "      <td id=\"T_e8640_row11_col6\" class=\"data row11 col6\" >0.0037</td>\n",
       "      <td id=\"T_e8640_row11_col7\" class=\"data row11 col7\" >0.0030</td>\n",
       "      <td id=\"T_e8640_row11_col8\" class=\"data row11 col8\" >0.0012</td>\n",
       "      <td id=\"T_e8640_row11_col9\" class=\"data row11 col9\" >0.0055</td>\n",
       "      <td id=\"T_e8640_row11_col10\" class=\"data row11 col10\" >0.0019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2281288e0b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 16:37:35 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "lightgbm = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9639f68f-46f5-4854-b520-3fc11e40f3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1ae28_row10_col0, #T_1ae28_row10_col1, #T_1ae28_row10_col2, #T_1ae28_row10_col3, #T_1ae28_row10_col4, #T_1ae28_row10_col5, #T_1ae28_row10_col6, #T_1ae28_row10_col7, #T_1ae28_row10_col8, #T_1ae28_row10_col9, #T_1ae28_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1ae28\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ae28_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_1ae28_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_1ae28_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_1ae28_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_1ae28_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_1ae28_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_1ae28_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_1ae28_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_1ae28_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_1ae28_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_1ae28_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1ae28_row0_col0\" class=\"data row0 col0\" >0.9636</td>\n",
       "      <td id=\"T_1ae28_row0_col1\" class=\"data row0 col1\" >0.9921</td>\n",
       "      <td id=\"T_1ae28_row0_col2\" class=\"data row0 col2\" >0.9636</td>\n",
       "      <td id=\"T_1ae28_row0_col3\" class=\"data row0 col3\" >0.9637</td>\n",
       "      <td id=\"T_1ae28_row0_col4\" class=\"data row0 col4\" >0.9628</td>\n",
       "      <td id=\"T_1ae28_row0_col5\" class=\"data row0 col5\" >0.8916</td>\n",
       "      <td id=\"T_1ae28_row0_col6\" class=\"data row0 col6\" >0.8938</td>\n",
       "      <td id=\"T_1ae28_row0_col7\" class=\"data row0 col7\" >0.9291</td>\n",
       "      <td id=\"T_1ae28_row0_col8\" class=\"data row0 col8\" >0.0364</td>\n",
       "      <td id=\"T_1ae28_row0_col9\" class=\"data row0 col9\" >0.8427</td>\n",
       "      <td id=\"T_1ae28_row0_col10\" class=\"data row0 col10\" >0.0899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1ae28_row1_col0\" class=\"data row1 col0\" >0.9604</td>\n",
       "      <td id=\"T_1ae28_row1_col1\" class=\"data row1 col1\" >0.9912</td>\n",
       "      <td id=\"T_1ae28_row1_col2\" class=\"data row1 col2\" >0.9604</td>\n",
       "      <td id=\"T_1ae28_row1_col3\" class=\"data row1 col3\" >0.9607</td>\n",
       "      <td id=\"T_1ae28_row1_col4\" class=\"data row1 col4\" >0.9595</td>\n",
       "      <td id=\"T_1ae28_row1_col5\" class=\"data row1 col5\" >0.8814</td>\n",
       "      <td id=\"T_1ae28_row1_col6\" class=\"data row1 col6\" >0.8843</td>\n",
       "      <td id=\"T_1ae28_row1_col7\" class=\"data row1 col7\" >0.9219</td>\n",
       "      <td id=\"T_1ae28_row1_col8\" class=\"data row1 col8\" >0.0396</td>\n",
       "      <td id=\"T_1ae28_row1_col9\" class=\"data row1 col9\" >0.8288</td>\n",
       "      <td id=\"T_1ae28_row1_col10\" class=\"data row1 col10\" >0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1ae28_row2_col0\" class=\"data row2 col0\" >0.9607</td>\n",
       "      <td id=\"T_1ae28_row2_col1\" class=\"data row2 col1\" >0.9917</td>\n",
       "      <td id=\"T_1ae28_row2_col2\" class=\"data row2 col2\" >0.9607</td>\n",
       "      <td id=\"T_1ae28_row2_col3\" class=\"data row2 col3\" >0.9608</td>\n",
       "      <td id=\"T_1ae28_row2_col4\" class=\"data row2 col4\" >0.9598</td>\n",
       "      <td id=\"T_1ae28_row2_col5\" class=\"data row2 col5\" >0.8827</td>\n",
       "      <td id=\"T_1ae28_row2_col6\" class=\"data row2 col6\" >0.8852</td>\n",
       "      <td id=\"T_1ae28_row2_col7\" class=\"data row2 col7\" >0.9238</td>\n",
       "      <td id=\"T_1ae28_row2_col8\" class=\"data row2 col8\" >0.0393</td>\n",
       "      <td id=\"T_1ae28_row2_col9\" class=\"data row2 col9\" >0.8307</td>\n",
       "      <td id=\"T_1ae28_row2_col10\" class=\"data row2 col10\" >0.0928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1ae28_row3_col0\" class=\"data row3 col0\" >0.9615</td>\n",
       "      <td id=\"T_1ae28_row3_col1\" class=\"data row3 col1\" >0.9922</td>\n",
       "      <td id=\"T_1ae28_row3_col2\" class=\"data row3 col2\" >0.9615</td>\n",
       "      <td id=\"T_1ae28_row3_col3\" class=\"data row3 col3\" >0.9617</td>\n",
       "      <td id=\"T_1ae28_row3_col4\" class=\"data row3 col4\" >0.9607</td>\n",
       "      <td id=\"T_1ae28_row3_col5\" class=\"data row3 col5\" >0.8850</td>\n",
       "      <td id=\"T_1ae28_row3_col6\" class=\"data row3 col6\" >0.8876</td>\n",
       "      <td id=\"T_1ae28_row3_col7\" class=\"data row3 col7\" >0.9247</td>\n",
       "      <td id=\"T_1ae28_row3_col8\" class=\"data row3 col8\" >0.0385</td>\n",
       "      <td id=\"T_1ae28_row3_col9\" class=\"data row3 col9\" >0.8338</td>\n",
       "      <td id=\"T_1ae28_row3_col10\" class=\"data row3 col10\" >0.0908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1ae28_row4_col0\" class=\"data row4 col0\" >0.9620</td>\n",
       "      <td id=\"T_1ae28_row4_col1\" class=\"data row4 col1\" >0.9921</td>\n",
       "      <td id=\"T_1ae28_row4_col2\" class=\"data row4 col2\" >0.9620</td>\n",
       "      <td id=\"T_1ae28_row4_col3\" class=\"data row4 col3\" >0.9623</td>\n",
       "      <td id=\"T_1ae28_row4_col4\" class=\"data row4 col4\" >0.9611</td>\n",
       "      <td id=\"T_1ae28_row4_col5\" class=\"data row4 col5\" >0.8864</td>\n",
       "      <td id=\"T_1ae28_row4_col6\" class=\"data row4 col6\" >0.8892</td>\n",
       "      <td id=\"T_1ae28_row4_col7\" class=\"data row4 col7\" >0.9246</td>\n",
       "      <td id=\"T_1ae28_row4_col8\" class=\"data row4 col8\" >0.0380</td>\n",
       "      <td id=\"T_1ae28_row4_col9\" class=\"data row4 col9\" >0.8354</td>\n",
       "      <td id=\"T_1ae28_row4_col10\" class=\"data row4 col10\" >0.0922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1ae28_row5_col0\" class=\"data row5 col0\" >0.9629</td>\n",
       "      <td id=\"T_1ae28_row5_col1\" class=\"data row5 col1\" >0.9918</td>\n",
       "      <td id=\"T_1ae28_row5_col2\" class=\"data row5 col2\" >0.9629</td>\n",
       "      <td id=\"T_1ae28_row5_col3\" class=\"data row5 col3\" >0.9631</td>\n",
       "      <td id=\"T_1ae28_row5_col4\" class=\"data row5 col4\" >0.9621</td>\n",
       "      <td id=\"T_1ae28_row5_col5\" class=\"data row5 col5\" >0.8892</td>\n",
       "      <td id=\"T_1ae28_row5_col6\" class=\"data row5 col6\" >0.8918</td>\n",
       "      <td id=\"T_1ae28_row5_col7\" class=\"data row5 col7\" >0.9268</td>\n",
       "      <td id=\"T_1ae28_row5_col8\" class=\"data row5 col8\" >0.0371</td>\n",
       "      <td id=\"T_1ae28_row5_col9\" class=\"data row5 col9\" >0.8393</td>\n",
       "      <td id=\"T_1ae28_row5_col10\" class=\"data row5 col10\" >0.0909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1ae28_row6_col0\" class=\"data row6 col0\" >0.9624</td>\n",
       "      <td id=\"T_1ae28_row6_col1\" class=\"data row6 col1\" >0.9918</td>\n",
       "      <td id=\"T_1ae28_row6_col2\" class=\"data row6 col2\" >0.9624</td>\n",
       "      <td id=\"T_1ae28_row6_col3\" class=\"data row6 col3\" >0.9625</td>\n",
       "      <td id=\"T_1ae28_row6_col4\" class=\"data row6 col4\" >0.9616</td>\n",
       "      <td id=\"T_1ae28_row6_col5\" class=\"data row6 col5\" >0.8878</td>\n",
       "      <td id=\"T_1ae28_row6_col6\" class=\"data row6 col6\" >0.8902</td>\n",
       "      <td id=\"T_1ae28_row6_col7\" class=\"data row6 col7\" >0.9267</td>\n",
       "      <td id=\"T_1ae28_row6_col8\" class=\"data row6 col8\" >0.0376</td>\n",
       "      <td id=\"T_1ae28_row6_col9\" class=\"data row6 col9\" >0.8375</td>\n",
       "      <td id=\"T_1ae28_row6_col10\" class=\"data row6 col10\" >0.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1ae28_row7_col0\" class=\"data row7 col0\" >0.9648</td>\n",
       "      <td id=\"T_1ae28_row7_col1\" class=\"data row7 col1\" >0.9924</td>\n",
       "      <td id=\"T_1ae28_row7_col2\" class=\"data row7 col2\" >0.9648</td>\n",
       "      <td id=\"T_1ae28_row7_col3\" class=\"data row7 col3\" >0.9649</td>\n",
       "      <td id=\"T_1ae28_row7_col4\" class=\"data row7 col4\" >0.9641</td>\n",
       "      <td id=\"T_1ae28_row7_col5\" class=\"data row7 col5\" >0.8954</td>\n",
       "      <td id=\"T_1ae28_row7_col6\" class=\"data row7 col6\" >0.8974</td>\n",
       "      <td id=\"T_1ae28_row7_col7\" class=\"data row7 col7\" >0.9320</td>\n",
       "      <td id=\"T_1ae28_row7_col8\" class=\"data row7 col8\" >0.0352</td>\n",
       "      <td id=\"T_1ae28_row7_col9\" class=\"data row7 col9\" >0.8480</td>\n",
       "      <td id=\"T_1ae28_row7_col10\" class=\"data row7 col10\" >0.0878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1ae28_row8_col0\" class=\"data row8 col0\" >0.9614</td>\n",
       "      <td id=\"T_1ae28_row8_col1\" class=\"data row8 col1\" >0.9917</td>\n",
       "      <td id=\"T_1ae28_row8_col2\" class=\"data row8 col2\" >0.9614</td>\n",
       "      <td id=\"T_1ae28_row8_col3\" class=\"data row8 col3\" >0.9616</td>\n",
       "      <td id=\"T_1ae28_row8_col4\" class=\"data row8 col4\" >0.9606</td>\n",
       "      <td id=\"T_1ae28_row8_col5\" class=\"data row8 col5\" >0.8848</td>\n",
       "      <td id=\"T_1ae28_row8_col6\" class=\"data row8 col6\" >0.8874</td>\n",
       "      <td id=\"T_1ae28_row8_col7\" class=\"data row8 col7\" >0.9245</td>\n",
       "      <td id=\"T_1ae28_row8_col8\" class=\"data row8 col8\" >0.0386</td>\n",
       "      <td id=\"T_1ae28_row8_col9\" class=\"data row8 col9\" >0.8335</td>\n",
       "      <td id=\"T_1ae28_row8_col10\" class=\"data row8 col10\" >0.0924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1ae28_row9_col0\" class=\"data row9 col0\" >0.9625</td>\n",
       "      <td id=\"T_1ae28_row9_col1\" class=\"data row9 col1\" >0.9918</td>\n",
       "      <td id=\"T_1ae28_row9_col2\" class=\"data row9 col2\" >0.9625</td>\n",
       "      <td id=\"T_1ae28_row9_col3\" class=\"data row9 col3\" >0.9628</td>\n",
       "      <td id=\"T_1ae28_row9_col4\" class=\"data row9 col4\" >0.9616</td>\n",
       "      <td id=\"T_1ae28_row9_col5\" class=\"data row9 col5\" >0.8878</td>\n",
       "      <td id=\"T_1ae28_row9_col6\" class=\"data row9 col6\" >0.8906</td>\n",
       "      <td id=\"T_1ae28_row9_col7\" class=\"data row9 col7\" >0.9254</td>\n",
       "      <td id=\"T_1ae28_row9_col8\" class=\"data row9 col8\" >0.0375</td>\n",
       "      <td id=\"T_1ae28_row9_col9\" class=\"data row9 col9\" >0.8374</td>\n",
       "      <td id=\"T_1ae28_row9_col10\" class=\"data row9 col10\" >0.0921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_1ae28_row10_col0\" class=\"data row10 col0\" >0.9622</td>\n",
       "      <td id=\"T_1ae28_row10_col1\" class=\"data row10 col1\" >0.9919</td>\n",
       "      <td id=\"T_1ae28_row10_col2\" class=\"data row10 col2\" >0.9622</td>\n",
       "      <td id=\"T_1ae28_row10_col3\" class=\"data row10 col3\" >0.9624</td>\n",
       "      <td id=\"T_1ae28_row10_col4\" class=\"data row10 col4\" >0.9614</td>\n",
       "      <td id=\"T_1ae28_row10_col5\" class=\"data row10 col5\" >0.8872</td>\n",
       "      <td id=\"T_1ae28_row10_col6\" class=\"data row10 col6\" >0.8897</td>\n",
       "      <td id=\"T_1ae28_row10_col7\" class=\"data row10 col7\" >0.9259</td>\n",
       "      <td id=\"T_1ae28_row10_col8\" class=\"data row10 col8\" >0.0378</td>\n",
       "      <td id=\"T_1ae28_row10_col9\" class=\"data row10 col9\" >0.8367</td>\n",
       "      <td id=\"T_1ae28_row10_col10\" class=\"data row10 col10\" >0.0916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae28_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_1ae28_row11_col0\" class=\"data row11 col0\" >0.0013</td>\n",
       "      <td id=\"T_1ae28_row11_col1\" class=\"data row11 col1\" >0.0003</td>\n",
       "      <td id=\"T_1ae28_row11_col2\" class=\"data row11 col2\" >0.0013</td>\n",
       "      <td id=\"T_1ae28_row11_col3\" class=\"data row11 col3\" >0.0012</td>\n",
       "      <td id=\"T_1ae28_row11_col4\" class=\"data row11 col4\" >0.0013</td>\n",
       "      <td id=\"T_1ae28_row11_col5\" class=\"data row11 col5\" >0.0040</td>\n",
       "      <td id=\"T_1ae28_row11_col6\" class=\"data row11 col6\" >0.0038</td>\n",
       "      <td id=\"T_1ae28_row11_col7\" class=\"data row11 col7\" >0.0027</td>\n",
       "      <td id=\"T_1ae28_row11_col8\" class=\"data row11 col8\" >0.0013</td>\n",
       "      <td id=\"T_1ae28_row11_col9\" class=\"data row11 col9\" >0.0054</td>\n",
       "      <td id=\"T_1ae28_row11_col10\" class=\"data row11 col10\" >0.0017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2282e9d1b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005373 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005275 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005333 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.006094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.004934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005331 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.006226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.008360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.006106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005321 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.75 MB) transferred to GPU in 0.005310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.76 MB) transferred to GPU in 0.005508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5437803609036268, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5437803609036268\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4228228034056274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4228228034056274\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006176 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006102 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006046 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.009009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008655 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.009550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008594 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008259 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.012356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012518 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005875 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.011986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008719 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012049 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008375 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.008583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012265 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005810 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005864 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.007967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012621 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008174 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.012365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.013406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.013164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012512 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008230 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008746 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013373 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.011878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.011977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.008198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008165 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.009054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008259 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009131 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008415 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.010201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.010790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008340 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012810 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.012186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012331 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.007202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006309 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.012285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008312 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.012065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.007102 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005913 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.008626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.007128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.013380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005746 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008567 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008419 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008538 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.013384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005944 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006442 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006652 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.009030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012518 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012345 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.013052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.012167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.014254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.013190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.012563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.014086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006162 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006102 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.006277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.010380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008225 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.008835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008331 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.012770 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.012597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.012291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.007954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.006229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.23 MB) transferred to GPU in 0.005985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.005834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.009112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.008249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.21 MB) transferred to GPU in 0.008751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.20 MB) transferred to GPU in 0.005976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.005735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.22 MB) transferred to GPU in 0.006298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7053544065214616, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7053544065214616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5350602601131251, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5350602601131251\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005656 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005875 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009404 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005943 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005525 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005552 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005490 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005889 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4108648495306758, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4108648495306758\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46426398909099653, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46426398909099653\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009475 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.008958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009322 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009049 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013725 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013539 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.021146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013632 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020857 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009166 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.008934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.008951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013481 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.021991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013650 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.016391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010039 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.019915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014017 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009650 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013797 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.021243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.023483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014105 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009173 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.024293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.022330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.024966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.021168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.024879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009329 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009049 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.028421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.021149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.015239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014105 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014481 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.022956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.018181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.023666 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.021531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014162 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013754 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013852 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013666 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.015476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.016884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.015478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.038176 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.021096 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.008880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009265 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013658 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013993 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.021087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.014493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.015752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009114 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.009215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.020299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013523 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.022639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.015098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.020668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.01 MB) transferred to GPU in 0.013827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210788104520184, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210788104520184\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9685974697496598, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9685974697496598\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009695 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009436 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009259 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009548 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.015268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009309 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009209 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009176 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.015326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009069 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009474 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009722 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009096 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009110 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009309 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009265 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009114 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013601 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008887 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009069 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008944 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009512 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009666 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.014084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008861 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013740 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009344 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009074 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009266 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.021030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010344 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009259 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009139 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009509 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009081 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009176 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013312 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.015019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.011057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009473 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009548 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009867 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009474 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009413 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009209 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009205 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009321 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009852 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009490 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009117 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009159 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009413 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010436 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009745 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.014488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013616 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009442 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009265 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009117 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009051 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009525 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009937 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009225 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013867 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.015816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.011334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009463 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009039 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009545 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009770 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009174 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008937 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009081 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009649 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009117 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009174 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.014858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.012935 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.012871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.012970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009754 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013523 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009166 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009313 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009509 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009525 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008868 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009333 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009406 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.015144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013722 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.012890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009329 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009545 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009415 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009102 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009331 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009139 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009074 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008990 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009719 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009527 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009340 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.012533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009745 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013331 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.015082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009525 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009512 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009153 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009321 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009166 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009548 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009039 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.014187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.010101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008848 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009174 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009666 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009313 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.008976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.010904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.009502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.013970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.02 MB) transferred to GPU in 0.013211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.008044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.008228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007795 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009475 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009265 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009509 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.14 MB) transferred to GPU in 0.007762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.15 MB) transferred to GPU in 0.007210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48885620619700654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48885620619700654\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561034340991285, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561034340991285\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009525 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005701 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005813 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007788 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.011468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005833 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.009505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007475 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007699 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007647 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007875 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007887 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006139 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005656 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.014712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005788 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.006084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007655 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007538 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.008132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011795 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007523 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008105 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005802 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.006076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005993 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.008524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007489 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.008127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.010996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010913 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007807 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.009196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005802 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005699 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.006059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007632 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.011753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008074 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005473 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007544 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008344 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008725 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007699 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007973 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.012420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.013561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005489 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005748 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006548 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.009090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011360 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.012141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.012282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.010906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005736 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005935 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005828 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007631 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.008359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008049 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.010927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.012229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010523 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005803 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.006178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.006281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.005543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.005661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.005878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007803 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.007451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.007579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.94 MB) transferred to GPU in 0.011193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.011260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.011068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.008263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.008208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.92 MB) transferred to GPU in 0.007688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.93 MB) transferred to GPU in 0.010983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (1.91 MB) transferred to GPU in 0.011270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4581218661787383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4581218661787383\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.46505932958220575, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46505932958220575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006304 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005993 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006131 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009740 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.022291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006068 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006021 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006069 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.007254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.005961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006173 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.006129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.19 MB) transferred to GPU in 0.005996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.005951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.18 MB) transferred to GPU in 0.006216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.17 MB) transferred to GPU in 0.006430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4617345325653795, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4617345325653795\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5259458872488469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5259458872488469\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008021 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007868 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007655 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.021385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007722 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.009001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007701 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007419 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.009047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007545 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.009147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.008511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.008398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.31 MB) transferred to GPU in 0.007736 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.30 MB) transferred to GPU in 0.007895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9154013414173596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9154013414173596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7971925590425888, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7971925590425888\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225122 -> initscore=-1.236065\n",
      "[LightGBM] [Info] Start training from score -1.236065\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009695 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.011412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34941, number of negative: 120265\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225127 -> initscore=-1.236037\n",
      "[LightGBM] [Info] Start training from score -1.236037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34940, number of negative: 120266\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225120 -> initscore=-1.236074\n",
      "[LightGBM] [Info] Start training from score -1.236074\n",
      "[LightGBM] [Warning] feature_fraction is set=0.455456147857072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.455456147857072\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6314684408489295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6314684408489295\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 16:55:58 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "lightgbm_tuned_model, lightgbm_tuner = tune_model(lightgbm, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07469b55-3844-4312-b370-4abd01748ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(bagging_fraction=0.9708703362894384, bagging_freq=2,\n",
      "               boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               device='gpu', feature_fraction=0.507007220773583,\n",
      "               importance_type='split', learning_rate=0.09688952429372566,\n",
      "               max_depth=-1, min_child_samples=33, min_child_weight=0.001,\n",
      "               min_split_gain=0.484949793073682, n_estimators=299, n_jobs=-1,\n",
      "               num_leaves=49, objective=None, random_state=7641,\n",
      "               reg_alpha=3.5114421971391514e-05,\n",
      "               reg_lambda=0.000615806794641032, subsample=1.0,\n",
      "               subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "print(lightgbm_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8650ba01-16a1-440d-a42d-514881b9ca7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8e379\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8e379_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8e379_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_8e379_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_8e379_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_8e379_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_8e379_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_8e379_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_8e379_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_8e379_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_8e379_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_8e379_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_8e379_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8e379_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8e379_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_8e379_row0_col1\" class=\"data row0 col1\" >0.9617</td>\n",
       "      <td id=\"T_8e379_row0_col2\" class=\"data row0 col2\" >0.9920</td>\n",
       "      <td id=\"T_8e379_row0_col3\" class=\"data row0 col3\" >0.9617</td>\n",
       "      <td id=\"T_8e379_row0_col4\" class=\"data row0 col4\" >0.9620</td>\n",
       "      <td id=\"T_8e379_row0_col5\" class=\"data row0 col5\" >0.9609</td>\n",
       "      <td id=\"T_8e379_row0_col6\" class=\"data row0 col6\" >0.8860</td>\n",
       "      <td id=\"T_8e379_row0_col7\" class=\"data row0 col7\" >0.8887</td>\n",
       "      <td id=\"T_8e379_row0_col8\" class=\"data row0 col8\" >0.9250</td>\n",
       "      <td id=\"T_8e379_row0_col9\" class=\"data row0 col9\" >0.0383</td>\n",
       "      <td id=\"T_8e379_row0_col10\" class=\"data row0 col10\" >0.8353</td>\n",
       "      <td id=\"T_8e379_row0_col11\" class=\"data row0 col11\" >0.0914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22886f439a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.507007220773583, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.507007220773583\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708703362894384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708703362894384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    }
   ],
   "source": [
    "predictions_lightgbm = predict_model(lightgbm_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6de13ba-59cf-4048-8b12-75912f3fd0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f448388dc5bd428bb8fae517c2c70ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31cadee3-f0e3-4f9d-89ba-517d8944a485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewness</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mean</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shape</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clearance</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ptp</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>crest</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>energy</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>std</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>impulse</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rms</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0             meanFreq         303\n",
       "1            bandwidth         300\n",
       "2           medianFreq         289\n",
       "3          meanWavelet         278\n",
       "4    spectral_flatness         271\n",
       "5             skewness         191\n",
       "6    spectral_skewness         154\n",
       "7       entropyWavelet         150\n",
       "8    spectral_kurtosis         130\n",
       "9           varWavelet         122\n",
       "10             entropy         100\n",
       "11                mean          96\n",
       "12    spectral_entropy          78\n",
       "13               shape          66\n",
       "14            kurtosis          60\n",
       "15           clearance          51\n",
       "16                 ptp          48\n",
       "17      varSpectrogram          40\n",
       "18               crest          37\n",
       "19   energySpectrogram          36\n",
       "20              energy          33\n",
       "21                 std          32\n",
       "22     meanSpectrogram          30\n",
       "23  entropySpectrogram          30\n",
       "24             impulse          29\n",
       "25       energyWavelet          27\n",
       "26                 rms          19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm_top_features = get_feature_importance_df(lightgbm, features_df_training_normalized)\n",
    "lightgbm_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba6b509-625a-40ed-be97-3ff2178817cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9197d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9197d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_9197d_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_9197d_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_9197d_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_9197d_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_9197d_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_9197d_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_9197d_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_9197d_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_9197d_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_9197d_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_9197d_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9197d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9197d_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_9197d_row0_col1\" class=\"data row0 col1\" >0.9518</td>\n",
       "      <td id=\"T_9197d_row0_col2\" class=\"data row0 col2\" >0.9866</td>\n",
       "      <td id=\"T_9197d_row0_col3\" class=\"data row0 col3\" >0.9518</td>\n",
       "      <td id=\"T_9197d_row0_col4\" class=\"data row0 col4\" >0.9526</td>\n",
       "      <td id=\"T_9197d_row0_col5\" class=\"data row0 col5\" >0.9502</td>\n",
       "      <td id=\"T_9197d_row0_col6\" class=\"data row0 col6\" >0.8539</td>\n",
       "      <td id=\"T_9197d_row0_col7\" class=\"data row0 col7\" >0.8592</td>\n",
       "      <td id=\"T_9197d_row0_col8\" class=\"data row0 col8\" >0.9024</td>\n",
       "      <td id=\"T_9197d_row0_col9\" class=\"data row0 col9\" >0.0482</td>\n",
       "      <td id=\"T_9197d_row0_col10\" class=\"data row0 col10\" >0.7921</td>\n",
       "      <td id=\"T_9197d_row0_col11\" class=\"data row0 col11\" >0.1144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2281288e0b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_lightgbm = predict_model(lightgbm, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34173f0e-3b07-47ec-9bc1-6a932d4a3f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93960</th>\n",
       "      <td>6.357815</td>\n",
       "      <td>2.343891</td>\n",
       "      <td>2.344943</td>\n",
       "      <td>0.635143</td>\n",
       "      <td>0.131432</td>\n",
       "      <td>2.410167</td>\n",
       "      <td>0.497478</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>1.546793</td>\n",
       "      <td>0.243375</td>\n",
       "      <td>...</td>\n",
       "      <td>3.512821</td>\n",
       "      <td>-0.984801</td>\n",
       "      <td>5.694981</td>\n",
       "      <td>5.858073</td>\n",
       "      <td>3.476256</td>\n",
       "      <td>5.900208</td>\n",
       "      <td>5.697627</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-1.272307</td>\n",
       "      <td>-0.564226</td>\n",
       "      <td>-0.568537</td>\n",
       "      <td>1.528791</td>\n",
       "      <td>0.216020</td>\n",
       "      <td>-0.510851</td>\n",
       "      <td>0.125702</td>\n",
       "      <td>0.104172</td>\n",
       "      <td>-0.696706</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280870</td>\n",
       "      <td>-0.318820</td>\n",
       "      <td>-0.302703</td>\n",
       "      <td>-0.292659</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>-0.093596</td>\n",
       "      <td>-0.079652</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33266</th>\n",
       "      <td>0.645020</td>\n",
       "      <td>-0.632693</td>\n",
       "      <td>-0.633274</td>\n",
       "      <td>-0.551617</td>\n",
       "      <td>-0.094884</td>\n",
       "      <td>-0.575886</td>\n",
       "      <td>-0.487928</td>\n",
       "      <td>-0.467216</td>\n",
       "      <td>-0.944098</td>\n",
       "      <td>-0.281379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282883</td>\n",
       "      <td>-0.354399</td>\n",
       "      <td>-0.306426</td>\n",
       "      <td>-0.297714</td>\n",
       "      <td>-0.080904</td>\n",
       "      <td>-0.093679</td>\n",
       "      <td>-0.079699</td>\n",
       "      <td>healthy</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.8216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238774</th>\n",
       "      <td>-0.578217</td>\n",
       "      <td>0.212273</td>\n",
       "      <td>0.214511</td>\n",
       "      <td>0.427125</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.290723</td>\n",
       "      <td>0.816735</td>\n",
       "      <td>0.688420</td>\n",
       "      <td>0.395715</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271075</td>\n",
       "      <td>0.898755</td>\n",
       "      <td>0.259001</td>\n",
       "      <td>0.245505</td>\n",
       "      <td>0.127312</td>\n",
       "      <td>0.152609</td>\n",
       "      <td>0.132502</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210317</th>\n",
       "      <td>-0.568005</td>\n",
       "      <td>-0.318859</td>\n",
       "      <td>-0.317333</td>\n",
       "      <td>0.193032</td>\n",
       "      <td>0.447690</td>\n",
       "      <td>-0.287186</td>\n",
       "      <td>-0.137432</td>\n",
       "      <td>0.016710</td>\n",
       "      <td>-0.330195</td>\n",
       "      <td>0.947099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194162</td>\n",
       "      <td>-0.020849</td>\n",
       "      <td>-0.221469</td>\n",
       "      <td>-0.218766</td>\n",
       "      <td>-0.069007</td>\n",
       "      <td>-0.078770</td>\n",
       "      <td>-0.068269</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241132</th>\n",
       "      <td>-0.461992</td>\n",
       "      <td>-0.303484</td>\n",
       "      <td>-0.301819</td>\n",
       "      <td>-0.136055</td>\n",
       "      <td>2.333936</td>\n",
       "      <td>-0.163282</td>\n",
       "      <td>1.232286</td>\n",
       "      <td>1.508070</td>\n",
       "      <td>-0.000384</td>\n",
       "      <td>2.301808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180947</td>\n",
       "      <td>-0.034000</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>-0.202451</td>\n",
       "      <td>-0.056868</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>-0.056839</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.8141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214791</th>\n",
       "      <td>-0.497348</td>\n",
       "      <td>-0.266087</td>\n",
       "      <td>-0.264392</td>\n",
       "      <td>-0.137965</td>\n",
       "      <td>2.325956</td>\n",
       "      <td>-0.112111</td>\n",
       "      <td>1.130247</td>\n",
       "      <td>1.260849</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>1.544519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164856</td>\n",
       "      <td>0.034796</td>\n",
       "      <td>-0.192630</td>\n",
       "      <td>-0.176194</td>\n",
       "      <td>-0.042728</td>\n",
       "      <td>-0.051209</td>\n",
       "      <td>-0.043369</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.8668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165165</th>\n",
       "      <td>2.155673</td>\n",
       "      <td>1.023697</td>\n",
       "      <td>1.026224</td>\n",
       "      <td>1.294237</td>\n",
       "      <td>1.292390</td>\n",
       "      <td>1.168869</td>\n",
       "      <td>0.929289</td>\n",
       "      <td>1.075880</td>\n",
       "      <td>1.148403</td>\n",
       "      <td>1.532599</td>\n",
       "      <td>...</td>\n",
       "      <td>2.056622</td>\n",
       "      <td>-1.219416</td>\n",
       "      <td>1.771152</td>\n",
       "      <td>1.718675</td>\n",
       "      <td>11.675074</td>\n",
       "      <td>8.020503</td>\n",
       "      <td>11.074836</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217690</th>\n",
       "      <td>-0.375742</td>\n",
       "      <td>0.747205</td>\n",
       "      <td>0.750013</td>\n",
       "      <td>1.155028</td>\n",
       "      <td>-0.124393</td>\n",
       "      <td>0.656685</td>\n",
       "      <td>-0.064616</td>\n",
       "      <td>-0.092430</td>\n",
       "      <td>0.465135</td>\n",
       "      <td>-0.163915</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155234</td>\n",
       "      <td>0.974433</td>\n",
       "      <td>1.168414</td>\n",
       "      <td>1.209354</td>\n",
       "      <td>2.298496</td>\n",
       "      <td>2.102510</td>\n",
       "      <td>2.272903</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187170</th>\n",
       "      <td>-0.223249</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.482127</td>\n",
       "      <td>-1.562037</td>\n",
       "      <td>-0.434486</td>\n",
       "      <td>0.399730</td>\n",
       "      <td>-0.317860</td>\n",
       "      <td>-0.407201</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>-0.846915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676820</td>\n",
       "      <td>0.931763</td>\n",
       "      <td>0.623523</td>\n",
       "      <td>0.617371</td>\n",
       "      <td>0.936763</td>\n",
       "      <td>0.891771</td>\n",
       "      <td>0.917739</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61590 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "93960   6.357815  2.343891  2.344943  0.635143  0.131432  2.410167  0.497478   \n",
       "371    -1.272307 -0.564226 -0.568537  1.528791  0.216020 -0.510851  0.125702   \n",
       "33266   0.645020 -0.632693 -0.633274 -0.551617 -0.094884 -0.575886 -0.487928   \n",
       "238774 -0.578217  0.212273  0.214511  0.427125  0.010503  0.290723  0.816735   \n",
       "210317 -0.568005 -0.318859 -0.317333  0.193032  0.447690 -0.287186 -0.137432   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "241132 -0.461992 -0.303484 -0.301819 -0.136055  2.333936 -0.163282  1.232286   \n",
       "214791 -0.497348 -0.266087 -0.264392 -0.137965  2.325956 -0.112111  1.130247   \n",
       "165165  2.155673  1.023697  1.026224  1.294237  1.292390  1.168869  0.929289   \n",
       "217690 -0.375742  0.747205  0.750013  1.155028 -0.124393  0.656685 -0.064616   \n",
       "187170 -0.223249  0.479528  0.482127 -1.562037 -0.434486  0.399730 -0.317860   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "93960   0.455357   1.546793  0.243375  ...    3.512821       -0.984801   \n",
       "371     0.104172  -0.696706  0.053627  ...   -0.280870       -0.318820   \n",
       "33266  -0.467216  -0.944098 -0.281379  ...   -0.282883       -0.354399   \n",
       "238774  0.688420   0.395715  0.021394  ...    0.271075        0.898755   \n",
       "210317  0.016710  -0.330195  0.947099  ...   -0.194162       -0.020849   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "241132  1.508070  -0.000384  2.301808  ...   -0.180947       -0.034000   \n",
       "214791  1.260849   0.006792  1.544519  ...   -0.164856        0.034796   \n",
       "165165  1.075880   1.148403  1.532599  ...    2.056622       -1.219416   \n",
       "217690 -0.092430   0.465135 -0.163915  ...    1.155234        0.974433   \n",
       "187170 -0.407201   0.182071 -0.846915  ...    0.676820        0.931763   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "93960        5.694981         5.858073        3.476256            5.900208   \n",
       "371         -0.302703        -0.292659       -0.080855           -0.093596   \n",
       "33266       -0.306426        -0.297714       -0.080904           -0.093679   \n",
       "238774       0.259001         0.245505        0.127312            0.152609   \n",
       "210317      -0.221469        -0.218766       -0.069007           -0.078770   \n",
       "...               ...              ...             ...                 ...   \n",
       "241132      -0.213441        -0.202451       -0.056868           -0.065837   \n",
       "214791      -0.192630        -0.176194       -0.042728           -0.051209   \n",
       "165165       1.771152         1.718675       11.675074            8.020503   \n",
       "217690       1.168414         1.209354        2.298496            2.102510   \n",
       "187170       0.623523         0.617371        0.936763            0.891771   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "93960            5.697627  damaged           damaged            0.9999  \n",
       "371             -0.079652  healthy           damaged            0.9059  \n",
       "33266           -0.079699  healthy           healthy            0.8216  \n",
       "238774           0.132502  damaged           damaged            0.9465  \n",
       "210317          -0.068269  damaged           damaged            0.9242  \n",
       "...                   ...      ...               ...               ...  \n",
       "241132          -0.056839  damaged           damaged            0.8141  \n",
       "214791          -0.043369  damaged           damaged            0.8668  \n",
       "165165          11.074836  damaged           damaged            0.9998  \n",
       "217690           2.272903  damaged           damaged            0.6165  \n",
       "187170           0.917739  damaged           damaged            0.9996  \n",
       "\n",
       "[61590 rows x 30 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789da63d-eb26-4f36-9559-4f8dc35b2c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-1.272307</td>\n",
       "      <td>-0.564226</td>\n",
       "      <td>-0.568537</td>\n",
       "      <td>1.528791</td>\n",
       "      <td>0.216020</td>\n",
       "      <td>-0.510851</td>\n",
       "      <td>0.125702</td>\n",
       "      <td>0.104172</td>\n",
       "      <td>-0.696706</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280870</td>\n",
       "      <td>-0.318820</td>\n",
       "      <td>-0.302703</td>\n",
       "      <td>-0.292659</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>-0.093596</td>\n",
       "      <td>-0.079652</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258524</th>\n",
       "      <td>-0.462637</td>\n",
       "      <td>-0.614765</td>\n",
       "      <td>-0.615758</td>\n",
       "      <td>-1.082147</td>\n",
       "      <td>1.396836</td>\n",
       "      <td>-0.548946</td>\n",
       "      <td>1.235335</td>\n",
       "      <td>1.315180</td>\n",
       "      <td>-0.751774</td>\n",
       "      <td>1.342675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282070</td>\n",
       "      <td>-0.345832</td>\n",
       "      <td>-0.305536</td>\n",
       "      <td>-0.296682</td>\n",
       "      <td>-0.080900</td>\n",
       "      <td>-0.093672</td>\n",
       "      <td>-0.079696</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207745</th>\n",
       "      <td>-0.473546</td>\n",
       "      <td>0.357993</td>\n",
       "      <td>0.360417</td>\n",
       "      <td>-0.554096</td>\n",
       "      <td>-0.787129</td>\n",
       "      <td>0.162519</td>\n",
       "      <td>-0.831262</td>\n",
       "      <td>-0.831097</td>\n",
       "      <td>-0.045980</td>\n",
       "      <td>-0.864700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.665827</td>\n",
       "      <td>0.465029</td>\n",
       "      <td>0.464907</td>\n",
       "      <td>0.870633</td>\n",
       "      <td>0.803324</td>\n",
       "      <td>0.839726</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.7560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>-0.490795</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>0.125355</td>\n",
       "      <td>0.375499</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>0.076991</td>\n",
       "      <td>-0.343168</td>\n",
       "      <td>-0.295428</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>0.060085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.706681</td>\n",
       "      <td>0.148892</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.092594</td>\n",
       "      <td>0.108320</td>\n",
       "      <td>0.094701</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.7437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207040</th>\n",
       "      <td>-0.353068</td>\n",
       "      <td>0.308735</td>\n",
       "      <td>0.311149</td>\n",
       "      <td>0.414371</td>\n",
       "      <td>0.260009</td>\n",
       "      <td>0.414386</td>\n",
       "      <td>0.282649</td>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.326729</td>\n",
       "      <td>0.291802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395646</td>\n",
       "      <td>1.035493</td>\n",
       "      <td>0.392030</td>\n",
       "      <td>0.422492</td>\n",
       "      <td>0.317823</td>\n",
       "      <td>0.349905</td>\n",
       "      <td>0.323606</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.5595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206916</th>\n",
       "      <td>-0.283023</td>\n",
       "      <td>0.173179</td>\n",
       "      <td>0.175487</td>\n",
       "      <td>0.596202</td>\n",
       "      <td>-0.180180</td>\n",
       "      <td>0.145434</td>\n",
       "      <td>-0.281675</td>\n",
       "      <td>-0.259991</td>\n",
       "      <td>0.039367</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222261</td>\n",
       "      <td>0.832072</td>\n",
       "      <td>0.209463</td>\n",
       "      <td>0.246596</td>\n",
       "      <td>0.172595</td>\n",
       "      <td>0.192210</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200497</th>\n",
       "      <td>-0.576046</td>\n",
       "      <td>-0.231117</td>\n",
       "      <td>-0.229435</td>\n",
       "      <td>0.114024</td>\n",
       "      <td>0.946886</td>\n",
       "      <td>-0.155104</td>\n",
       "      <td>0.381899</td>\n",
       "      <td>0.562282</td>\n",
       "      <td>-0.103635</td>\n",
       "      <td>1.438474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131645</td>\n",
       "      <td>0.055914</td>\n",
       "      <td>-0.171216</td>\n",
       "      <td>-0.158665</td>\n",
       "      <td>-0.042632</td>\n",
       "      <td>-0.049115</td>\n",
       "      <td>-0.042980</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258687</th>\n",
       "      <td>-0.421962</td>\n",
       "      <td>-0.277288</td>\n",
       "      <td>-0.275546</td>\n",
       "      <td>0.249954</td>\n",
       "      <td>0.933059</td>\n",
       "      <td>-0.222218</td>\n",
       "      <td>-0.017272</td>\n",
       "      <td>0.138013</td>\n",
       "      <td>-0.257450</td>\n",
       "      <td>1.040160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162309</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.199004</td>\n",
       "      <td>-0.182658</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.058800</td>\n",
       "      <td>-0.051380</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205706</th>\n",
       "      <td>-0.496038</td>\n",
       "      <td>-0.302834</td>\n",
       "      <td>-0.301201</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>0.769237</td>\n",
       "      <td>-0.220468</td>\n",
       "      <td>0.313372</td>\n",
       "      <td>0.362557</td>\n",
       "      <td>-0.235371</td>\n",
       "      <td>0.629632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184069</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.213110</td>\n",
       "      <td>-0.198862</td>\n",
       "      <td>-0.062567</td>\n",
       "      <td>-0.071403</td>\n",
       "      <td>-0.062071</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201271</th>\n",
       "      <td>-0.526296</td>\n",
       "      <td>-0.264085</td>\n",
       "      <td>-0.262413</td>\n",
       "      <td>-0.084871</td>\n",
       "      <td>0.369411</td>\n",
       "      <td>-0.234605</td>\n",
       "      <td>-0.231204</td>\n",
       "      <td>-0.123989</td>\n",
       "      <td>-0.298620</td>\n",
       "      <td>0.564308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155067</td>\n",
       "      <td>0.028787</td>\n",
       "      <td>-0.191423</td>\n",
       "      <td>-0.181192</td>\n",
       "      <td>-0.052162</td>\n",
       "      <td>-0.059515</td>\n",
       "      <td>-0.052184</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2969 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "371    -1.272307 -0.564226 -0.568537  1.528791  0.216020 -0.510851  0.125702   \n",
       "258524 -0.462637 -0.614765 -0.615758 -1.082147  1.396836 -0.548946  1.235335   \n",
       "207745 -0.473546  0.357993  0.360417 -0.554096 -0.787129  0.162519 -0.831262   \n",
       "255723 -0.490795  0.123172  0.125355  0.375499 -0.076538  0.076991 -0.343168   \n",
       "207040 -0.353068  0.308735  0.311149  0.414371  0.260009  0.414386  0.282649   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "206916 -0.283023  0.173179  0.175487  0.596202 -0.180180  0.145434 -0.281675   \n",
       "200497 -0.576046 -0.231117 -0.229435  0.114024  0.946886 -0.155104  0.381899   \n",
       "258687 -0.421962 -0.277288 -0.275546  0.249954  0.933059 -0.222218 -0.017272   \n",
       "205706 -0.496038 -0.302834 -0.301201  0.114997  0.769237 -0.220468  0.313372   \n",
       "201271 -0.526296 -0.264085 -0.262413 -0.084871  0.369411 -0.234605 -0.231204   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "371     0.104172  -0.696706  0.053627  ...   -0.280870       -0.318820   \n",
       "258524  1.315180  -0.751774  1.342675  ...   -0.282070       -0.345832   \n",
       "207745 -0.831097  -0.045980 -0.864700  ...    0.543103        0.665827   \n",
       "255723 -0.295428  -0.008315  0.060085  ...    0.171299        0.706681   \n",
       "207040  0.278528   0.326729  0.291802  ...    0.395646        1.035493   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "206916 -0.259991   0.039367 -0.055206  ...    0.222261        0.832072   \n",
       "200497  0.562282  -0.103635  1.438474  ...   -0.131645        0.055914   \n",
       "258687  0.138013  -0.257450  1.040160  ...   -0.162309        0.002188   \n",
       "205706  0.362557  -0.235371  0.629632  ...   -0.184069        0.001882   \n",
       "201271 -0.123989  -0.298620  0.564308  ...   -0.155067        0.028787   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "371         -0.302703        -0.292659       -0.080855           -0.093596   \n",
       "258524      -0.305536        -0.296682       -0.080900           -0.093672   \n",
       "207745       0.465029         0.464907        0.870633            0.803324   \n",
       "255723       0.148892         0.159414        0.092594            0.108320   \n",
       "207040       0.392030         0.422492        0.317823            0.349905   \n",
       "...               ...              ...             ...                 ...   \n",
       "206916       0.209463         0.246596        0.172595            0.192210   \n",
       "200497      -0.171216        -0.158665       -0.042632           -0.049115   \n",
       "258687      -0.199004        -0.182658       -0.051269           -0.058800   \n",
       "205706      -0.213110        -0.198862       -0.062567           -0.071403   \n",
       "201271      -0.191423        -0.181192       -0.052162           -0.059515   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "371             -0.079652  healthy           damaged            0.9059  \n",
       "258524          -0.079696  healthy           damaged            0.6379  \n",
       "207745           0.839726  healthy           damaged            0.7560  \n",
       "255723           0.094701  healthy           damaged            0.7437  \n",
       "207040           0.323606  healthy           damaged            0.5595  \n",
       "...                   ...      ...               ...               ...  \n",
       "206916           0.174519  healthy           damaged            0.9315  \n",
       "200497          -0.042980  healthy           damaged            0.6273  \n",
       "258687          -0.051380  healthy           damaged            0.6073  \n",
       "205706          -0.062071  healthy           damaged            0.6771  \n",
       "201271          -0.052184  healthy           damaged            0.6408  \n",
       "\n",
       "[2969 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d3df1-3151-4046-8abc-d66039145875",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6aa747c-a901-435f-b4ae-0e0cf24659a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5ac91_row10_col0, #T_5ac91_row10_col1, #T_5ac91_row10_col2, #T_5ac91_row10_col3, #T_5ac91_row10_col4, #T_5ac91_row10_col5, #T_5ac91_row10_col6, #T_5ac91_row10_col7, #T_5ac91_row10_col8, #T_5ac91_row10_col9, #T_5ac91_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5ac91\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5ac91_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_5ac91_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_5ac91_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_5ac91_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_5ac91_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_5ac91_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_5ac91_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_5ac91_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_5ac91_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_5ac91_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_5ac91_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5ac91_row0_col0\" class=\"data row0 col0\" >0.9780</td>\n",
       "      <td id=\"T_5ac91_row0_col1\" class=\"data row0 col1\" >0.9959</td>\n",
       "      <td id=\"T_5ac91_row0_col2\" class=\"data row0 col2\" >0.9780</td>\n",
       "      <td id=\"T_5ac91_row0_col3\" class=\"data row0 col3\" >0.9782</td>\n",
       "      <td id=\"T_5ac91_row0_col4\" class=\"data row0 col4\" >0.9777</td>\n",
       "      <td id=\"T_5ac91_row0_col5\" class=\"data row0 col5\" >0.9351</td>\n",
       "      <td id=\"T_5ac91_row0_col6\" class=\"data row0 col6\" >0.9363</td>\n",
       "      <td id=\"T_5ac91_row0_col7\" class=\"data row0 col7\" >0.9553</td>\n",
       "      <td id=\"T_5ac91_row0_col8\" class=\"data row0 col8\" >0.0220</td>\n",
       "      <td id=\"T_5ac91_row0_col9\" class=\"data row0 col9\" >0.9033</td>\n",
       "      <td id=\"T_5ac91_row0_col10\" class=\"data row0 col10\" >0.0746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5ac91_row1_col0\" class=\"data row1 col0\" >0.9754</td>\n",
       "      <td id=\"T_5ac91_row1_col1\" class=\"data row1 col1\" >0.9950</td>\n",
       "      <td id=\"T_5ac91_row1_col2\" class=\"data row1 col2\" >0.9754</td>\n",
       "      <td id=\"T_5ac91_row1_col3\" class=\"data row1 col3\" >0.9755</td>\n",
       "      <td id=\"T_5ac91_row1_col4\" class=\"data row1 col4\" >0.9750</td>\n",
       "      <td id=\"T_5ac91_row1_col5\" class=\"data row1 col5\" >0.9273</td>\n",
       "      <td id=\"T_5ac91_row1_col6\" class=\"data row1 col6\" >0.9286</td>\n",
       "      <td id=\"T_5ac91_row1_col7\" class=\"data row1 col7\" >0.9507</td>\n",
       "      <td id=\"T_5ac91_row1_col8\" class=\"data row1 col8\" >0.0246</td>\n",
       "      <td id=\"T_5ac91_row1_col9\" class=\"data row1 col9\" >0.8922</td>\n",
       "      <td id=\"T_5ac91_row1_col10\" class=\"data row1 col10\" >0.0754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5ac91_row2_col0\" class=\"data row2 col0\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row2_col1\" class=\"data row2 col1\" >0.9953</td>\n",
       "      <td id=\"T_5ac91_row2_col2\" class=\"data row2 col2\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row2_col3\" class=\"data row2 col3\" >0.9761</td>\n",
       "      <td id=\"T_5ac91_row2_col4\" class=\"data row2 col4\" >0.9757</td>\n",
       "      <td id=\"T_5ac91_row2_col5\" class=\"data row2 col5\" >0.9293</td>\n",
       "      <td id=\"T_5ac91_row2_col6\" class=\"data row2 col6\" >0.9305</td>\n",
       "      <td id=\"T_5ac91_row2_col7\" class=\"data row2 col7\" >0.9522</td>\n",
       "      <td id=\"T_5ac91_row2_col8\" class=\"data row2 col8\" >0.0240</td>\n",
       "      <td id=\"T_5ac91_row2_col9\" class=\"data row2 col9\" >0.8950</td>\n",
       "      <td id=\"T_5ac91_row2_col10\" class=\"data row2 col10\" >0.0742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5ac91_row3_col0\" class=\"data row3 col0\" >0.9758</td>\n",
       "      <td id=\"T_5ac91_row3_col1\" class=\"data row3 col1\" >0.9958</td>\n",
       "      <td id=\"T_5ac91_row3_col2\" class=\"data row3 col2\" >0.9758</td>\n",
       "      <td id=\"T_5ac91_row3_col3\" class=\"data row3 col3\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row3_col4\" class=\"data row3 col4\" >0.9755</td>\n",
       "      <td id=\"T_5ac91_row3_col5\" class=\"data row3 col5\" >0.9288</td>\n",
       "      <td id=\"T_5ac91_row3_col6\" class=\"data row3 col6\" >0.9299</td>\n",
       "      <td id=\"T_5ac91_row3_col7\" class=\"data row3 col7\" >0.9520</td>\n",
       "      <td id=\"T_5ac91_row3_col8\" class=\"data row3 col8\" >0.0242</td>\n",
       "      <td id=\"T_5ac91_row3_col9\" class=\"data row3 col9\" >0.8943</td>\n",
       "      <td id=\"T_5ac91_row3_col10\" class=\"data row3 col10\" >0.0736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5ac91_row4_col0\" class=\"data row4 col0\" >0.9763</td>\n",
       "      <td id=\"T_5ac91_row4_col1\" class=\"data row4 col1\" >0.9960</td>\n",
       "      <td id=\"T_5ac91_row4_col2\" class=\"data row4 col2\" >0.9763</td>\n",
       "      <td id=\"T_5ac91_row4_col3\" class=\"data row4 col3\" >0.9765</td>\n",
       "      <td id=\"T_5ac91_row4_col4\" class=\"data row4 col4\" >0.9759</td>\n",
       "      <td id=\"T_5ac91_row4_col5\" class=\"data row4 col5\" >0.9300</td>\n",
       "      <td id=\"T_5ac91_row4_col6\" class=\"data row4 col6\" >0.9313</td>\n",
       "      <td id=\"T_5ac91_row4_col7\" class=\"data row4 col7\" >0.9519</td>\n",
       "      <td id=\"T_5ac91_row4_col8\" class=\"data row4 col8\" >0.0237</td>\n",
       "      <td id=\"T_5ac91_row4_col9\" class=\"data row4 col9\" >0.8960</td>\n",
       "      <td id=\"T_5ac91_row4_col10\" class=\"data row4 col10\" >0.0732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5ac91_row5_col0\" class=\"data row5 col0\" >0.9773</td>\n",
       "      <td id=\"T_5ac91_row5_col1\" class=\"data row5 col1\" >0.9953</td>\n",
       "      <td id=\"T_5ac91_row5_col2\" class=\"data row5 col2\" >0.9773</td>\n",
       "      <td id=\"T_5ac91_row5_col3\" class=\"data row5 col3\" >0.9775</td>\n",
       "      <td id=\"T_5ac91_row5_col4\" class=\"data row5 col4\" >0.9769</td>\n",
       "      <td id=\"T_5ac91_row5_col5\" class=\"data row5 col5\" >0.9331</td>\n",
       "      <td id=\"T_5ac91_row5_col6\" class=\"data row5 col6\" >0.9342</td>\n",
       "      <td id=\"T_5ac91_row5_col7\" class=\"data row5 col7\" >0.9541</td>\n",
       "      <td id=\"T_5ac91_row5_col8\" class=\"data row5 col8\" >0.0227</td>\n",
       "      <td id=\"T_5ac91_row5_col9\" class=\"data row5 col9\" >0.9003</td>\n",
       "      <td id=\"T_5ac91_row5_col10\" class=\"data row5 col10\" >0.0743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5ac91_row6_col0\" class=\"data row6 col0\" >0.9749</td>\n",
       "      <td id=\"T_5ac91_row6_col1\" class=\"data row6 col1\" >0.9953</td>\n",
       "      <td id=\"T_5ac91_row6_col2\" class=\"data row6 col2\" >0.9749</td>\n",
       "      <td id=\"T_5ac91_row6_col3\" class=\"data row6 col3\" >0.9751</td>\n",
       "      <td id=\"T_5ac91_row6_col4\" class=\"data row6 col4\" >0.9746</td>\n",
       "      <td id=\"T_5ac91_row6_col5\" class=\"data row6 col5\" >0.9262</td>\n",
       "      <td id=\"T_5ac91_row6_col6\" class=\"data row6 col6\" >0.9274</td>\n",
       "      <td id=\"T_5ac91_row6_col7\" class=\"data row6 col7\" >0.9505</td>\n",
       "      <td id=\"T_5ac91_row6_col8\" class=\"data row6 col8\" >0.0251</td>\n",
       "      <td id=\"T_5ac91_row6_col9\" class=\"data row6 col9\" >0.8906</td>\n",
       "      <td id=\"T_5ac91_row6_col10\" class=\"data row6 col10\" >0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5ac91_row7_col0\" class=\"data row7 col0\" >0.9782</td>\n",
       "      <td id=\"T_5ac91_row7_col1\" class=\"data row7 col1\" >0.9957</td>\n",
       "      <td id=\"T_5ac91_row7_col2\" class=\"data row7 col2\" >0.9782</td>\n",
       "      <td id=\"T_5ac91_row7_col3\" class=\"data row7 col3\" >0.9783</td>\n",
       "      <td id=\"T_5ac91_row7_col4\" class=\"data row7 col4\" >0.9779</td>\n",
       "      <td id=\"T_5ac91_row7_col5\" class=\"data row7 col5\" >0.9360</td>\n",
       "      <td id=\"T_5ac91_row7_col6\" class=\"data row7 col6\" >0.9369</td>\n",
       "      <td id=\"T_5ac91_row7_col7\" class=\"data row7 col7\" >0.9567</td>\n",
       "      <td id=\"T_5ac91_row7_col8\" class=\"data row7 col8\" >0.0218</td>\n",
       "      <td id=\"T_5ac91_row7_col9\" class=\"data row7 col9\" >0.9045</td>\n",
       "      <td id=\"T_5ac91_row7_col10\" class=\"data row7 col10\" >0.0722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5ac91_row8_col0\" class=\"data row8 col0\" >0.9754</td>\n",
       "      <td id=\"T_5ac91_row8_col1\" class=\"data row8 col1\" >0.9953</td>\n",
       "      <td id=\"T_5ac91_row8_col2\" class=\"data row8 col2\" >0.9754</td>\n",
       "      <td id=\"T_5ac91_row8_col3\" class=\"data row8 col3\" >0.9755</td>\n",
       "      <td id=\"T_5ac91_row8_col4\" class=\"data row8 col4\" >0.9750</td>\n",
       "      <td id=\"T_5ac91_row8_col5\" class=\"data row8 col5\" >0.9274</td>\n",
       "      <td id=\"T_5ac91_row8_col6\" class=\"data row8 col6\" >0.9286</td>\n",
       "      <td id=\"T_5ac91_row8_col7\" class=\"data row8 col7\" >0.9510</td>\n",
       "      <td id=\"T_5ac91_row8_col8\" class=\"data row8 col8\" >0.0246</td>\n",
       "      <td id=\"T_5ac91_row8_col9\" class=\"data row8 col9\" >0.8923</td>\n",
       "      <td id=\"T_5ac91_row8_col10\" class=\"data row8 col10\" >0.0769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5ac91_row9_col0\" class=\"data row9 col0\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row9_col1\" class=\"data row9 col1\" >0.9956</td>\n",
       "      <td id=\"T_5ac91_row9_col2\" class=\"data row9 col2\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row9_col3\" class=\"data row9 col3\" >0.9763</td>\n",
       "      <td id=\"T_5ac91_row9_col4\" class=\"data row9 col4\" >0.9756</td>\n",
       "      <td id=\"T_5ac91_row9_col5\" class=\"data row9 col5\" >0.9291</td>\n",
       "      <td id=\"T_5ac91_row9_col6\" class=\"data row9 col6\" >0.9305</td>\n",
       "      <td id=\"T_5ac91_row9_col7\" class=\"data row9 col7\" >0.9508</td>\n",
       "      <td id=\"T_5ac91_row9_col8\" class=\"data row9 col8\" >0.0240</td>\n",
       "      <td id=\"T_5ac91_row9_col9\" class=\"data row9 col9\" >0.8946</td>\n",
       "      <td id=\"T_5ac91_row9_col10\" class=\"data row9 col10\" >0.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_5ac91_row10_col0\" class=\"data row10 col0\" >0.9763</td>\n",
       "      <td id=\"T_5ac91_row10_col1\" class=\"data row10 col1\" >0.9955</td>\n",
       "      <td id=\"T_5ac91_row10_col2\" class=\"data row10 col2\" >0.9763</td>\n",
       "      <td id=\"T_5ac91_row10_col3\" class=\"data row10 col3\" >0.9765</td>\n",
       "      <td id=\"T_5ac91_row10_col4\" class=\"data row10 col4\" >0.9760</td>\n",
       "      <td id=\"T_5ac91_row10_col5\" class=\"data row10 col5\" >0.9302</td>\n",
       "      <td id=\"T_5ac91_row10_col6\" class=\"data row10 col6\" >0.9314</td>\n",
       "      <td id=\"T_5ac91_row10_col7\" class=\"data row10 col7\" >0.9525</td>\n",
       "      <td id=\"T_5ac91_row10_col8\" class=\"data row10 col8\" >0.0237</td>\n",
       "      <td id=\"T_5ac91_row10_col9\" class=\"data row10 col9\" >0.8963</td>\n",
       "      <td id=\"T_5ac91_row10_col10\" class=\"data row10 col10\" >0.0742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ac91_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_5ac91_row11_col0\" class=\"data row11 col0\" >0.0011</td>\n",
       "      <td id=\"T_5ac91_row11_col1\" class=\"data row11 col1\" >0.0003</td>\n",
       "      <td id=\"T_5ac91_row11_col2\" class=\"data row11 col2\" >0.0011</td>\n",
       "      <td id=\"T_5ac91_row11_col3\" class=\"data row11 col3\" >0.0011</td>\n",
       "      <td id=\"T_5ac91_row11_col4\" class=\"data row11 col4\" >0.0011</td>\n",
       "      <td id=\"T_5ac91_row11_col5\" class=\"data row11 col5\" >0.0032</td>\n",
       "      <td id=\"T_5ac91_row11_col6\" class=\"data row11 col6\" >0.0031</td>\n",
       "      <td id=\"T_5ac91_row11_col7\" class=\"data row11 col7\" >0.0020</td>\n",
       "      <td id=\"T_5ac91_row11_col8\" class=\"data row11 col8\" >0.0011</td>\n",
       "      <td id=\"T_5ac91_row11_col9\" class=\"data row11 col9\" >0.0045</td>\n",
       "      <td id=\"T_5ac91_row11_col10\" class=\"data row11 col10\" >0.0012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22836ec86a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 16:58:48 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "rf = create_model('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7824407f-4b19-4413-aa1a-12494fe53e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_61c4b_row10_col0, #T_61c4b_row10_col1, #T_61c4b_row10_col2, #T_61c4b_row10_col3, #T_61c4b_row10_col4, #T_61c4b_row10_col5, #T_61c4b_row10_col6, #T_61c4b_row10_col7, #T_61c4b_row10_col8, #T_61c4b_row10_col9, #T_61c4b_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_61c4b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_61c4b_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_61c4b_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_61c4b_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_61c4b_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_61c4b_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_61c4b_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_61c4b_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_61c4b_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_61c4b_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_61c4b_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_61c4b_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_61c4b_row0_col0\" class=\"data row0 col0\" >0.9396</td>\n",
       "      <td id=\"T_61c4b_row0_col1\" class=\"data row0 col1\" >0.9766</td>\n",
       "      <td id=\"T_61c4b_row0_col2\" class=\"data row0 col2\" >0.9396</td>\n",
       "      <td id=\"T_61c4b_row0_col3\" class=\"data row0 col3\" >0.9387</td>\n",
       "      <td id=\"T_61c4b_row0_col4\" class=\"data row0 col4\" >0.9383</td>\n",
       "      <td id=\"T_61c4b_row0_col5\" class=\"data row0 col5\" >0.8198</td>\n",
       "      <td id=\"T_61c4b_row0_col6\" class=\"data row0 col6\" >0.8221</td>\n",
       "      <td id=\"T_61c4b_row0_col7\" class=\"data row0 col7\" >0.8940</td>\n",
       "      <td id=\"T_61c4b_row0_col8\" class=\"data row0 col8\" >0.0604</td>\n",
       "      <td id=\"T_61c4b_row0_col9\" class=\"data row0 col9\" >0.7514</td>\n",
       "      <td id=\"T_61c4b_row0_col10\" class=\"data row0 col10\" >0.1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_61c4b_row1_col0\" class=\"data row1 col0\" >0.9400</td>\n",
       "      <td id=\"T_61c4b_row1_col1\" class=\"data row1 col1\" >0.9755</td>\n",
       "      <td id=\"T_61c4b_row1_col2\" class=\"data row1 col2\" >0.9400</td>\n",
       "      <td id=\"T_61c4b_row1_col3\" class=\"data row1 col3\" >0.9392</td>\n",
       "      <td id=\"T_61c4b_row1_col4\" class=\"data row1 col4\" >0.9385</td>\n",
       "      <td id=\"T_61c4b_row1_col5\" class=\"data row1 col5\" >0.8201</td>\n",
       "      <td id=\"T_61c4b_row1_col6\" class=\"data row1 col6\" >0.8229</td>\n",
       "      <td id=\"T_61c4b_row1_col7\" class=\"data row1 col7\" >0.8920</td>\n",
       "      <td id=\"T_61c4b_row1_col8\" class=\"data row1 col8\" >0.0600</td>\n",
       "      <td id=\"T_61c4b_row1_col9\" class=\"data row1 col9\" >0.7511</td>\n",
       "      <td id=\"T_61c4b_row1_col10\" class=\"data row1 col10\" >0.1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_61c4b_row2_col0\" class=\"data row2 col0\" >0.9366</td>\n",
       "      <td id=\"T_61c4b_row2_col1\" class=\"data row2 col1\" >0.9755</td>\n",
       "      <td id=\"T_61c4b_row2_col2\" class=\"data row2 col2\" >0.9366</td>\n",
       "      <td id=\"T_61c4b_row2_col3\" class=\"data row2 col3\" >0.9355</td>\n",
       "      <td id=\"T_61c4b_row2_col4\" class=\"data row2 col4\" >0.9352</td>\n",
       "      <td id=\"T_61c4b_row2_col5\" class=\"data row2 col5\" >0.8108</td>\n",
       "      <td id=\"T_61c4b_row2_col6\" class=\"data row2 col6\" >0.8130</td>\n",
       "      <td id=\"T_61c4b_row2_col7\" class=\"data row2 col7\" >0.8896</td>\n",
       "      <td id=\"T_61c4b_row2_col8\" class=\"data row2 col8\" >0.0634</td>\n",
       "      <td id=\"T_61c4b_row2_col9\" class=\"data row2 col9\" >0.7405</td>\n",
       "      <td id=\"T_61c4b_row2_col10\" class=\"data row2 col10\" >0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_61c4b_row3_col0\" class=\"data row3 col0\" >0.9361</td>\n",
       "      <td id=\"T_61c4b_row3_col1\" class=\"data row3 col1\" >0.9756</td>\n",
       "      <td id=\"T_61c4b_row3_col2\" class=\"data row3 col2\" >0.9361</td>\n",
       "      <td id=\"T_61c4b_row3_col3\" class=\"data row3 col3\" >0.9350</td>\n",
       "      <td id=\"T_61c4b_row3_col4\" class=\"data row3 col4\" >0.9348</td>\n",
       "      <td id=\"T_61c4b_row3_col5\" class=\"data row3 col5\" >0.8096</td>\n",
       "      <td id=\"T_61c4b_row3_col6\" class=\"data row3 col6\" >0.8117</td>\n",
       "      <td id=\"T_61c4b_row3_col7\" class=\"data row3 col7\" >0.8895</td>\n",
       "      <td id=\"T_61c4b_row3_col8\" class=\"data row3 col8\" >0.0639</td>\n",
       "      <td id=\"T_61c4b_row3_col9\" class=\"data row3 col9\" >0.7392</td>\n",
       "      <td id=\"T_61c4b_row3_col10\" class=\"data row3 col10\" >0.1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_61c4b_row4_col0\" class=\"data row4 col0\" >0.9396</td>\n",
       "      <td id=\"T_61c4b_row4_col1\" class=\"data row4 col1\" >0.9771</td>\n",
       "      <td id=\"T_61c4b_row4_col2\" class=\"data row4 col2\" >0.9396</td>\n",
       "      <td id=\"T_61c4b_row4_col3\" class=\"data row4 col3\" >0.9390</td>\n",
       "      <td id=\"T_61c4b_row4_col4\" class=\"data row4 col4\" >0.9379</td>\n",
       "      <td id=\"T_61c4b_row4_col5\" class=\"data row4 col5\" >0.8178</td>\n",
       "      <td id=\"T_61c4b_row4_col6\" class=\"data row4 col6\" >0.8215</td>\n",
       "      <td id=\"T_61c4b_row4_col7\" class=\"data row4 col7\" >0.8887</td>\n",
       "      <td id=\"T_61c4b_row4_col8\" class=\"data row4 col8\" >0.0604</td>\n",
       "      <td id=\"T_61c4b_row4_col9\" class=\"data row4 col9\" >0.7479</td>\n",
       "      <td id=\"T_61c4b_row4_col10\" class=\"data row4 col10\" >0.1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_61c4b_row5_col0\" class=\"data row5 col0\" >0.9352</td>\n",
       "      <td id=\"T_61c4b_row5_col1\" class=\"data row5 col1\" >0.9762</td>\n",
       "      <td id=\"T_61c4b_row5_col2\" class=\"data row5 col2\" >0.9352</td>\n",
       "      <td id=\"T_61c4b_row5_col3\" class=\"data row5 col3\" >0.9344</td>\n",
       "      <td id=\"T_61c4b_row5_col4\" class=\"data row5 col4\" >0.9347</td>\n",
       "      <td id=\"T_61c4b_row5_col5\" class=\"data row5 col5\" >0.8113</td>\n",
       "      <td id=\"T_61c4b_row5_col6\" class=\"data row5 col6\" >0.8116</td>\n",
       "      <td id=\"T_61c4b_row5_col7\" class=\"data row5 col7\" >0.8994</td>\n",
       "      <td id=\"T_61c4b_row5_col8\" class=\"data row5 col8\" >0.0648</td>\n",
       "      <td id=\"T_61c4b_row5_col9\" class=\"data row5 col9\" >0.7434</td>\n",
       "      <td id=\"T_61c4b_row5_col10\" class=\"data row5 col10\" >0.1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_61c4b_row6_col0\" class=\"data row6 col0\" >0.9402</td>\n",
       "      <td id=\"T_61c4b_row6_col1\" class=\"data row6 col1\" >0.9756</td>\n",
       "      <td id=\"T_61c4b_row6_col2\" class=\"data row6 col2\" >0.9402</td>\n",
       "      <td id=\"T_61c4b_row6_col3\" class=\"data row6 col3\" >0.9394</td>\n",
       "      <td id=\"T_61c4b_row6_col4\" class=\"data row6 col4\" >0.9387</td>\n",
       "      <td id=\"T_61c4b_row6_col5\" class=\"data row6 col5\" >0.8206</td>\n",
       "      <td id=\"T_61c4b_row6_col6\" class=\"data row6 col6\" >0.8235</td>\n",
       "      <td id=\"T_61c4b_row6_col7\" class=\"data row6 col7\" >0.8923</td>\n",
       "      <td id=\"T_61c4b_row6_col8\" class=\"data row6 col8\" >0.0598</td>\n",
       "      <td id=\"T_61c4b_row6_col9\" class=\"data row6 col9\" >0.7518</td>\n",
       "      <td id=\"T_61c4b_row6_col10\" class=\"data row6 col10\" >0.1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_61c4b_row7_col0\" class=\"data row7 col0\" >0.9395</td>\n",
       "      <td id=\"T_61c4b_row7_col1\" class=\"data row7 col1\" >0.9766</td>\n",
       "      <td id=\"T_61c4b_row7_col2\" class=\"data row7 col2\" >0.9395</td>\n",
       "      <td id=\"T_61c4b_row7_col3\" class=\"data row7 col3\" >0.9387</td>\n",
       "      <td id=\"T_61c4b_row7_col4\" class=\"data row7 col4\" >0.9382</td>\n",
       "      <td id=\"T_61c4b_row7_col5\" class=\"data row7 col5\" >0.8192</td>\n",
       "      <td id=\"T_61c4b_row7_col6\" class=\"data row7 col6\" >0.8217</td>\n",
       "      <td id=\"T_61c4b_row7_col7\" class=\"data row7 col7\" >0.8928</td>\n",
       "      <td id=\"T_61c4b_row7_col8\" class=\"data row7 col8\" >0.0605</td>\n",
       "      <td id=\"T_61c4b_row7_col9\" class=\"data row7 col9\" >0.7504</td>\n",
       "      <td id=\"T_61c4b_row7_col10\" class=\"data row7 col10\" >0.1909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_61c4b_row8_col0\" class=\"data row8 col0\" >0.9424</td>\n",
       "      <td id=\"T_61c4b_row8_col1\" class=\"data row8 col1\" >0.9773</td>\n",
       "      <td id=\"T_61c4b_row8_col2\" class=\"data row8 col2\" >0.9424</td>\n",
       "      <td id=\"T_61c4b_row8_col3\" class=\"data row8 col3\" >0.9420</td>\n",
       "      <td id=\"T_61c4b_row8_col4\" class=\"data row8 col4\" >0.9409</td>\n",
       "      <td id=\"T_61c4b_row8_col5\" class=\"data row8 col5\" >0.8265</td>\n",
       "      <td id=\"T_61c4b_row8_col6\" class=\"data row8 col6\" >0.8301</td>\n",
       "      <td id=\"T_61c4b_row8_col7\" class=\"data row8 col7\" >0.8931</td>\n",
       "      <td id=\"T_61c4b_row8_col8\" class=\"data row8 col8\" >0.0576</td>\n",
       "      <td id=\"T_61c4b_row8_col9\" class=\"data row8 col9\" >0.7586</td>\n",
       "      <td id=\"T_61c4b_row8_col10\" class=\"data row8 col10\" >0.1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_61c4b_row9_col0\" class=\"data row9 col0\" >0.9386</td>\n",
       "      <td id=\"T_61c4b_row9_col1\" class=\"data row9 col1\" >0.9749</td>\n",
       "      <td id=\"T_61c4b_row9_col2\" class=\"data row9 col2\" >0.9386</td>\n",
       "      <td id=\"T_61c4b_row9_col3\" class=\"data row9 col3\" >0.9378</td>\n",
       "      <td id=\"T_61c4b_row9_col4\" class=\"data row9 col4\" >0.9372</td>\n",
       "      <td id=\"T_61c4b_row9_col5\" class=\"data row9 col5\" >0.8161</td>\n",
       "      <td id=\"T_61c4b_row9_col6\" class=\"data row9 col6\" >0.8190</td>\n",
       "      <td id=\"T_61c4b_row9_col7\" class=\"data row9 col7\" >0.8902</td>\n",
       "      <td id=\"T_61c4b_row9_col8\" class=\"data row9 col8\" >0.0614</td>\n",
       "      <td id=\"T_61c4b_row9_col9\" class=\"data row9 col9\" >0.7464</td>\n",
       "      <td id=\"T_61c4b_row9_col10\" class=\"data row9 col10\" >0.1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_61c4b_row10_col0\" class=\"data row10 col0\" >0.9388</td>\n",
       "      <td id=\"T_61c4b_row10_col1\" class=\"data row10 col1\" >0.9761</td>\n",
       "      <td id=\"T_61c4b_row10_col2\" class=\"data row10 col2\" >0.9388</td>\n",
       "      <td id=\"T_61c4b_row10_col3\" class=\"data row10 col3\" >0.9380</td>\n",
       "      <td id=\"T_61c4b_row10_col4\" class=\"data row10 col4\" >0.9374</td>\n",
       "      <td id=\"T_61c4b_row10_col5\" class=\"data row10 col5\" >0.8172</td>\n",
       "      <td id=\"T_61c4b_row10_col6\" class=\"data row10 col6\" >0.8197</td>\n",
       "      <td id=\"T_61c4b_row10_col7\" class=\"data row10 col7\" >0.8922</td>\n",
       "      <td id=\"T_61c4b_row10_col8\" class=\"data row10 col8\" >0.0612</td>\n",
       "      <td id=\"T_61c4b_row10_col9\" class=\"data row10 col9\" >0.7481</td>\n",
       "      <td id=\"T_61c4b_row10_col10\" class=\"data row10 col10\" >0.1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61c4b_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_61c4b_row11_col0\" class=\"data row11 col0\" >0.0021</td>\n",
       "      <td id=\"T_61c4b_row11_col1\" class=\"data row11 col1\" >0.0008</td>\n",
       "      <td id=\"T_61c4b_row11_col2\" class=\"data row11 col2\" >0.0021</td>\n",
       "      <td id=\"T_61c4b_row11_col3\" class=\"data row11 col3\" >0.0022</td>\n",
       "      <td id=\"T_61c4b_row11_col4\" class=\"data row11 col4\" >0.0019</td>\n",
       "      <td id=\"T_61c4b_row11_col5\" class=\"data row11 col5\" >0.0050</td>\n",
       "      <td id=\"T_61c4b_row11_col6\" class=\"data row11 col6\" >0.0057</td>\n",
       "      <td id=\"T_61c4b_row11_col7\" class=\"data row11 col7\" >0.0029</td>\n",
       "      <td id=\"T_61c4b_row11_col8\" class=\"data row11 col8\" >0.0021</td>\n",
       "      <td id=\"T_61c4b_row11_col9\" class=\"data row11 col9\" >0.0056</td>\n",
       "      <td id=\"T_61c4b_row11_col10\" class=\"data row11 col10\" >0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2283565d8a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:09:47 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "rf_tuned_model, rf_tuner = tune_model(rf, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae1bcd83-0a9d-430d-b4eb-1068f65eebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_samples_leaf=1,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n",
      "                       oob_score=False, random_state=7641, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rf_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d170d33-8e8f-466a-8545-fdbcce7ba2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4dce7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4dce7_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_4dce7_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_4dce7_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_4dce7_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_4dce7_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_4dce7_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_4dce7_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_4dce7_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_4dce7_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_4dce7_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_4dce7_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_4dce7_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4dce7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4dce7_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_4dce7_row0_col1\" class=\"data row0 col1\" >0.9787</td>\n",
       "      <td id=\"T_4dce7_row0_col2\" class=\"data row0 col2\" >0.9961</td>\n",
       "      <td id=\"T_4dce7_row0_col3\" class=\"data row0 col3\" >0.9787</td>\n",
       "      <td id=\"T_4dce7_row0_col4\" class=\"data row0 col4\" >0.9788</td>\n",
       "      <td id=\"T_4dce7_row0_col5\" class=\"data row0 col5\" >0.9784</td>\n",
       "      <td id=\"T_4dce7_row0_col6\" class=\"data row0 col6\" >0.9376</td>\n",
       "      <td id=\"T_4dce7_row0_col7\" class=\"data row0 col7\" >0.9385</td>\n",
       "      <td id=\"T_4dce7_row0_col8\" class=\"data row0 col8\" >0.9574</td>\n",
       "      <td id=\"T_4dce7_row0_col9\" class=\"data row0 col9\" >0.0213</td>\n",
       "      <td id=\"T_4dce7_row0_col10\" class=\"data row0 col10\" >0.9069</td>\n",
       "      <td id=\"T_4dce7_row0_col11\" class=\"data row0 col11\" >0.0714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22898309450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_rf = predict_model(rf_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50123496-3039-4e57-9520-b92cc6865e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cf44ba42dc4577b25f8015a3e00642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a31be2f-b8c2-46e9-92bc-4ba605a72cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>0.117837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>0.079955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>0.058169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>0.054457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>0.052832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>std</td>\n",
       "      <td>0.050229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>0.046179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>0.046031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>0.045034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>energy</td>\n",
       "      <td>0.040885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rms</td>\n",
       "      <td>0.037744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>0.031996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.031209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>skewness</td>\n",
       "      <td>0.030648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>0.028161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>0.027806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>0.026152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>0.024550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>0.024366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>0.022826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.020564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shape</td>\n",
       "      <td>0.019946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>clearance</td>\n",
       "      <td>0.019590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>0.019540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.015888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>crest</td>\n",
       "      <td>0.014140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>impulse</td>\n",
       "      <td>0.013265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0          meanWavelet    0.117837\n",
       "1           medianFreq    0.079955\n",
       "2        energyWavelet    0.058169\n",
       "3             meanFreq    0.054457\n",
       "4     spectral_entropy    0.052832\n",
       "5                  std    0.050229\n",
       "6    spectral_flatness    0.046179\n",
       "7            bandwidth    0.046031\n",
       "8      meanSpectrogram    0.045034\n",
       "9               energy    0.040885\n",
       "10                 rms    0.037744\n",
       "11      entropyWavelet    0.031996\n",
       "12                 ptp    0.031209\n",
       "13            skewness    0.030648\n",
       "14      varSpectrogram    0.028161\n",
       "15          varWavelet    0.027806\n",
       "16   spectral_kurtosis    0.026152\n",
       "17   spectral_skewness    0.024550\n",
       "18  entropySpectrogram    0.024366\n",
       "19            kurtosis    0.022826\n",
       "20             entropy    0.020564\n",
       "21               shape    0.019946\n",
       "22           clearance    0.019590\n",
       "23   energySpectrogram    0.019540\n",
       "24                mean    0.015888\n",
       "25               crest    0.014140\n",
       "26             impulse    0.013265"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_top_features = get_feature_importance_df(rf, features_df_training_normalized)\n",
    "rf_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e8fcdb-1e5a-4ef1-9c0f-538581f3f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_27c81\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_27c81_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_27c81_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_27c81_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_27c81_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_27c81_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_27c81_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_27c81_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_27c81_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_27c81_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_27c81_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_27c81_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_27c81_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_27c81_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_27c81_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_27c81_row0_col1\" class=\"data row0 col1\" >0.9787</td>\n",
       "      <td id=\"T_27c81_row0_col2\" class=\"data row0 col2\" >0.9961</td>\n",
       "      <td id=\"T_27c81_row0_col3\" class=\"data row0 col3\" >0.9787</td>\n",
       "      <td id=\"T_27c81_row0_col4\" class=\"data row0 col4\" >0.9788</td>\n",
       "      <td id=\"T_27c81_row0_col5\" class=\"data row0 col5\" >0.9784</td>\n",
       "      <td id=\"T_27c81_row0_col6\" class=\"data row0 col6\" >0.9376</td>\n",
       "      <td id=\"T_27c81_row0_col7\" class=\"data row0 col7\" >0.9385</td>\n",
       "      <td id=\"T_27c81_row0_col8\" class=\"data row0 col8\" >0.9574</td>\n",
       "      <td id=\"T_27c81_row0_col9\" class=\"data row0 col9\" >0.0213</td>\n",
       "      <td id=\"T_27c81_row0_col10\" class=\"data row0 col10\" >0.9069</td>\n",
       "      <td id=\"T_27c81_row0_col11\" class=\"data row0 col11\" >0.0714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22839419e40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_rf = predict_model(rf, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2f4d83-272f-4b78-a3fa-f351cddf7020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>-0.490795</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>0.125355</td>\n",
       "      <td>0.375499</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>0.076991</td>\n",
       "      <td>-0.343168</td>\n",
       "      <td>-0.295428</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>0.060085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.706681</td>\n",
       "      <td>0.148892</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.092594</td>\n",
       "      <td>0.108320</td>\n",
       "      <td>0.094701</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207040</th>\n",
       "      <td>-0.353068</td>\n",
       "      <td>0.308735</td>\n",
       "      <td>0.311149</td>\n",
       "      <td>0.414371</td>\n",
       "      <td>0.260009</td>\n",
       "      <td>0.414386</td>\n",
       "      <td>0.282649</td>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.326729</td>\n",
       "      <td>0.291802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395646</td>\n",
       "      <td>1.035493</td>\n",
       "      <td>0.392030</td>\n",
       "      <td>0.422492</td>\n",
       "      <td>0.317823</td>\n",
       "      <td>0.349905</td>\n",
       "      <td>0.323606</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215923</th>\n",
       "      <td>-0.566900</td>\n",
       "      <td>0.279868</td>\n",
       "      <td>0.282180</td>\n",
       "      <td>-0.605398</td>\n",
       "      <td>-0.248029</td>\n",
       "      <td>0.227013</td>\n",
       "      <td>-0.263786</td>\n",
       "      <td>-0.248145</td>\n",
       "      <td>0.118507</td>\n",
       "      <td>-0.077784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367203</td>\n",
       "      <td>0.918918</td>\n",
       "      <td>0.350482</td>\n",
       "      <td>0.403667</td>\n",
       "      <td>0.371225</td>\n",
       "      <td>0.392265</td>\n",
       "      <td>0.371312</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202856</th>\n",
       "      <td>-0.671359</td>\n",
       "      <td>0.200738</td>\n",
       "      <td>0.202919</td>\n",
       "      <td>0.259419</td>\n",
       "      <td>0.332430</td>\n",
       "      <td>0.226696</td>\n",
       "      <td>0.075642</td>\n",
       "      <td>0.120797</td>\n",
       "      <td>0.182180</td>\n",
       "      <td>0.421321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258286</td>\n",
       "      <td>0.854640</td>\n",
       "      <td>0.244118</td>\n",
       "      <td>0.267044</td>\n",
       "      <td>0.189892</td>\n",
       "      <td>0.209489</td>\n",
       "      <td>0.192026</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257105</th>\n",
       "      <td>-0.491878</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>0.022433</td>\n",
       "      <td>0.302523</td>\n",
       "      <td>0.527255</td>\n",
       "      <td>0.101222</td>\n",
       "      <td>0.478098</td>\n",
       "      <td>0.552123</td>\n",
       "      <td>0.168857</td>\n",
       "      <td>0.886951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107578</td>\n",
       "      <td>0.279084</td>\n",
       "      <td>0.036760</td>\n",
       "      <td>0.062195</td>\n",
       "      <td>0.125821</td>\n",
       "      <td>0.128799</td>\n",
       "      <td>0.120267</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202304</th>\n",
       "      <td>-0.467441</td>\n",
       "      <td>0.098287</td>\n",
       "      <td>0.100455</td>\n",
       "      <td>0.445940</td>\n",
       "      <td>0.340490</td>\n",
       "      <td>0.125023</td>\n",
       "      <td>-0.030906</td>\n",
       "      <td>0.021997</td>\n",
       "      <td>0.072717</td>\n",
       "      <td>0.381093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145469</td>\n",
       "      <td>0.667298</td>\n",
       "      <td>0.120250</td>\n",
       "      <td>0.161309</td>\n",
       "      <td>0.120162</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.120347</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200447</th>\n",
       "      <td>-0.460936</td>\n",
       "      <td>-0.285567</td>\n",
       "      <td>-0.283871</td>\n",
       "      <td>0.693839</td>\n",
       "      <td>2.795959</td>\n",
       "      <td>-0.113762</td>\n",
       "      <td>1.463045</td>\n",
       "      <td>1.744825</td>\n",
       "      <td>0.078182</td>\n",
       "      <td>2.398688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169029</td>\n",
       "      <td>-0.022774</td>\n",
       "      <td>-0.203725</td>\n",
       "      <td>-0.189921</td>\n",
       "      <td>-0.048459</td>\n",
       "      <td>-0.057060</td>\n",
       "      <td>-0.048883</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259550</th>\n",
       "      <td>-0.322300</td>\n",
       "      <td>-0.268401</td>\n",
       "      <td>-0.266572</td>\n",
       "      <td>0.080743</td>\n",
       "      <td>0.542818</td>\n",
       "      <td>-0.170980</td>\n",
       "      <td>0.595511</td>\n",
       "      <td>0.592653</td>\n",
       "      <td>-0.137069</td>\n",
       "      <td>0.536099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161250</td>\n",
       "      <td>0.043342</td>\n",
       "      <td>-0.193911</td>\n",
       "      <td>-0.179916</td>\n",
       "      <td>-0.054774</td>\n",
       "      <td>-0.062395</td>\n",
       "      <td>-0.054584</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210944</th>\n",
       "      <td>-0.591265</td>\n",
       "      <td>0.185384</td>\n",
       "      <td>0.187587</td>\n",
       "      <td>-0.365052</td>\n",
       "      <td>-0.196839</td>\n",
       "      <td>0.123479</td>\n",
       "      <td>-0.525343</td>\n",
       "      <td>-0.477898</td>\n",
       "      <td>-0.026761</td>\n",
       "      <td>-0.132084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240576</td>\n",
       "      <td>0.872083</td>\n",
       "      <td>0.224674</td>\n",
       "      <td>0.261468</td>\n",
       "      <td>0.146130</td>\n",
       "      <td>0.172737</td>\n",
       "      <td>0.151084</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206916</th>\n",
       "      <td>-0.283023</td>\n",
       "      <td>0.173179</td>\n",
       "      <td>0.175487</td>\n",
       "      <td>0.596202</td>\n",
       "      <td>-0.180180</td>\n",
       "      <td>0.145434</td>\n",
       "      <td>-0.281675</td>\n",
       "      <td>-0.259991</td>\n",
       "      <td>0.039367</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222261</td>\n",
       "      <td>0.832072</td>\n",
       "      <td>0.209463</td>\n",
       "      <td>0.246596</td>\n",
       "      <td>0.172595</td>\n",
       "      <td>0.192210</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1313 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "255723 -0.490795  0.123172  0.125355  0.375499 -0.076538  0.076991 -0.343168   \n",
       "207040 -0.353068  0.308735  0.311149  0.414371  0.260009  0.414386  0.282649   \n",
       "215923 -0.566900  0.279868  0.282180 -0.605398 -0.248029  0.227013 -0.263786   \n",
       "202856 -0.671359  0.200738  0.202919  0.259419  0.332430  0.226696  0.075642   \n",
       "257105 -0.491878  0.020361  0.022433  0.302523  0.527255  0.101222  0.478098   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "202304 -0.467441  0.098287  0.100455  0.445940  0.340490  0.125023 -0.030906   \n",
       "200447 -0.460936 -0.285567 -0.283871  0.693839  2.795959 -0.113762  1.463045   \n",
       "259550 -0.322300 -0.268401 -0.266572  0.080743  0.542818 -0.170980  0.595511   \n",
       "210944 -0.591265  0.185384  0.187587 -0.365052 -0.196839  0.123479 -0.525343   \n",
       "206916 -0.283023  0.173179  0.175487  0.596202 -0.180180  0.145434 -0.281675   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "255723 -0.295428  -0.008315  0.060085  ...    0.171299        0.706681   \n",
       "207040  0.278528   0.326729  0.291802  ...    0.395646        1.035493   \n",
       "215923 -0.248145   0.118507 -0.077784  ...    0.367203        0.918918   \n",
       "202856  0.120797   0.182180  0.421321  ...    0.258286        0.854640   \n",
       "257105  0.552123   0.168857  0.886951  ...    0.107578        0.279084   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "202304  0.021997   0.072717  0.381093  ...    0.145469        0.667298   \n",
       "200447  1.744825   0.078182  2.398688  ...   -0.169029       -0.022774   \n",
       "259550  0.592653  -0.137069  0.536099  ...   -0.161250        0.043342   \n",
       "210944 -0.477898  -0.026761 -0.132084  ...    0.240576        0.872083   \n",
       "206916 -0.259991   0.039367 -0.055206  ...    0.222261        0.832072   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "255723       0.148892         0.159414        0.092594            0.108320   \n",
       "207040       0.392030         0.422492        0.317823            0.349905   \n",
       "215923       0.350482         0.403667        0.371225            0.392265   \n",
       "202856       0.244118         0.267044        0.189892            0.209489   \n",
       "257105       0.036760         0.062195        0.125821            0.128799   \n",
       "...               ...              ...             ...                 ...   \n",
       "202304       0.120250         0.161309        0.120162            0.130844   \n",
       "200447      -0.203725        -0.189921       -0.048459           -0.057060   \n",
       "259550      -0.193911        -0.179916       -0.054774           -0.062395   \n",
       "210944       0.224674         0.261468        0.146130            0.172737   \n",
       "206916       0.209463         0.246596        0.172595            0.192210   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "255723           0.094701  healthy           damaged              0.63  \n",
       "207040           0.323606  healthy           damaged              0.69  \n",
       "215923           0.371312  damaged           healthy              0.55  \n",
       "202856           0.192026  healthy           damaged              0.83  \n",
       "257105           0.120267  healthy           damaged              0.80  \n",
       "...                   ...      ...               ...               ...  \n",
       "202304           0.120347  healthy           damaged              0.78  \n",
       "200447          -0.048883  healthy           damaged              0.74  \n",
       "259550          -0.054584  healthy           damaged              0.80  \n",
       "210944           0.151084  damaged           healthy              0.52  \n",
       "206916           0.174519  healthy           damaged              0.88  \n",
       "\n",
       "[1313 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc359a-4de6-48f8-a573-41c6d084a7ca",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "811f8d9f-491d-4f38-9ce9-e06344b3ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_da721_row10_col0, #T_da721_row10_col1, #T_da721_row10_col2, #T_da721_row10_col3, #T_da721_row10_col4, #T_da721_row10_col5, #T_da721_row10_col6, #T_da721_row10_col7, #T_da721_row10_col8, #T_da721_row10_col9, #T_da721_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_da721\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_da721_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_da721_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_da721_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_da721_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_da721_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_da721_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_da721_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_da721_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_da721_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_da721_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_da721_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_da721_row0_col0\" class=\"data row0 col0\" >0.8923</td>\n",
       "      <td id=\"T_da721_row0_col1\" class=\"data row0 col1\" >0.9103</td>\n",
       "      <td id=\"T_da721_row0_col2\" class=\"data row0 col2\" >0.8923</td>\n",
       "      <td id=\"T_da721_row0_col3\" class=\"data row0 col3\" >0.8905</td>\n",
       "      <td id=\"T_da721_row0_col4\" class=\"data row0 col4\" >0.8853</td>\n",
       "      <td id=\"T_da721_row0_col5\" class=\"data row0 col5\" >0.6557</td>\n",
       "      <td id=\"T_da721_row0_col6\" class=\"data row0 col6\" >0.6707</td>\n",
       "      <td id=\"T_da721_row0_col7\" class=\"data row0 col7\" >0.7939</td>\n",
       "      <td id=\"T_da721_row0_col8\" class=\"data row0 col8\" >0.1077</td>\n",
       "      <td id=\"T_da721_row0_col9\" class=\"data row0 col9\" >0.5624</td>\n",
       "      <td id=\"T_da721_row0_col10\" class=\"data row0 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_da721_row1_col0\" class=\"data row1 col0\" >0.8980</td>\n",
       "      <td id=\"T_da721_row1_col1\" class=\"data row1 col1\" >0.9102</td>\n",
       "      <td id=\"T_da721_row1_col2\" class=\"data row1 col2\" >0.8980</td>\n",
       "      <td id=\"T_da721_row1_col3\" class=\"data row1 col3\" >0.8959</td>\n",
       "      <td id=\"T_da721_row1_col4\" class=\"data row1 col4\" >0.8923</td>\n",
       "      <td id=\"T_da721_row1_col5\" class=\"data row1 col5\" >0.6783</td>\n",
       "      <td id=\"T_da721_row1_col6\" class=\"data row1 col6\" >0.6899</td>\n",
       "      <td id=\"T_da721_row1_col7\" class=\"data row1 col7\" >0.8082</td>\n",
       "      <td id=\"T_da721_row1_col8\" class=\"data row1 col8\" >0.1020</td>\n",
       "      <td id=\"T_da721_row1_col9\" class=\"data row1 col9\" >0.5873</td>\n",
       "      <td id=\"T_da721_row1_col10\" class=\"data row1 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_da721_row2_col0\" class=\"data row2 col0\" >0.8999</td>\n",
       "      <td id=\"T_da721_row2_col1\" class=\"data row2 col1\" >0.9078</td>\n",
       "      <td id=\"T_da721_row2_col2\" class=\"data row2 col2\" >0.8999</td>\n",
       "      <td id=\"T_da721_row2_col3\" class=\"data row2 col3\" >0.8973</td>\n",
       "      <td id=\"T_da721_row2_col4\" class=\"data row2 col4\" >0.8952</td>\n",
       "      <td id=\"T_da721_row2_col5\" class=\"data row2 col5\" >0.6883</td>\n",
       "      <td id=\"T_da721_row2_col6\" class=\"data row2 col6\" >0.6970</td>\n",
       "      <td id=\"T_da721_row2_col7\" class=\"data row2 col7\" >0.8168</td>\n",
       "      <td id=\"T_da721_row2_col8\" class=\"data row2 col8\" >0.1001</td>\n",
       "      <td id=\"T_da721_row2_col9\" class=\"data row2 col9\" >0.5995</td>\n",
       "      <td id=\"T_da721_row2_col10\" class=\"data row2 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_da721_row3_col0\" class=\"data row3 col0\" >0.7855</td>\n",
       "      <td id=\"T_da721_row3_col1\" class=\"data row3 col1\" >0.7915</td>\n",
       "      <td id=\"T_da721_row3_col2\" class=\"data row3 col2\" >0.7855</td>\n",
       "      <td id=\"T_da721_row3_col3\" class=\"data row3 col3\" >0.7985</td>\n",
       "      <td id=\"T_da721_row3_col4\" class=\"data row3 col4\" >0.7909</td>\n",
       "      <td id=\"T_da721_row3_col5\" class=\"data row3 col5\" >0.4183</td>\n",
       "      <td id=\"T_da721_row3_col6\" class=\"data row3 col6\" >0.4203</td>\n",
       "      <td id=\"T_da721_row3_col7\" class=\"data row3 col7\" >0.7210</td>\n",
       "      <td id=\"T_da721_row3_col8\" class=\"data row3 col8\" >0.2145</td>\n",
       "      <td id=\"T_da721_row3_col9\" class=\"data row3 col9\" >0.3879</td>\n",
       "      <td id=\"T_da721_row3_col10\" class=\"data row3 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_da721_row4_col0\" class=\"data row4 col0\" >0.8429</td>\n",
       "      <td id=\"T_da721_row4_col1\" class=\"data row4 col1\" >0.8320</td>\n",
       "      <td id=\"T_da721_row4_col2\" class=\"data row4 col2\" >0.8429</td>\n",
       "      <td id=\"T_da721_row4_col3\" class=\"data row4 col3\" >0.8379</td>\n",
       "      <td id=\"T_da721_row4_col4\" class=\"data row4 col4\" >0.8399</td>\n",
       "      <td id=\"T_da721_row4_col5\" class=\"data row4 col5\" >0.5331</td>\n",
       "      <td id=\"T_da721_row4_col6\" class=\"data row4 col6\" >0.5343</td>\n",
       "      <td id=\"T_da721_row4_col7\" class=\"data row4 col7\" >0.7571</td>\n",
       "      <td id=\"T_da721_row4_col8\" class=\"data row4 col8\" >0.1571</td>\n",
       "      <td id=\"T_da721_row4_col9\" class=\"data row4 col9\" >0.4627</td>\n",
       "      <td id=\"T_da721_row4_col10\" class=\"data row4 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_da721_row5_col0\" class=\"data row5 col0\" >0.9017</td>\n",
       "      <td id=\"T_da721_row5_col1\" class=\"data row5 col1\" >0.9075</td>\n",
       "      <td id=\"T_da721_row5_col2\" class=\"data row5 col2\" >0.9017</td>\n",
       "      <td id=\"T_da721_row5_col3\" class=\"data row5 col3\" >0.8993</td>\n",
       "      <td id=\"T_da721_row5_col4\" class=\"data row5 col4\" >0.8971</td>\n",
       "      <td id=\"T_da721_row5_col5\" class=\"data row5 col5\" >0.6941</td>\n",
       "      <td id=\"T_da721_row5_col6\" class=\"data row5 col6\" >0.7027</td>\n",
       "      <td id=\"T_da721_row5_col7\" class=\"data row5 col7\" >0.8196</td>\n",
       "      <td id=\"T_da721_row5_col8\" class=\"data row5 col8\" >0.0983</td>\n",
       "      <td id=\"T_da721_row5_col9\" class=\"data row5 col9\" >0.6055</td>\n",
       "      <td id=\"T_da721_row5_col10\" class=\"data row5 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_da721_row6_col0\" class=\"data row6 col0\" >0.8972</td>\n",
       "      <td id=\"T_da721_row6_col1\" class=\"data row6 col1\" >0.9096</td>\n",
       "      <td id=\"T_da721_row6_col2\" class=\"data row6 col2\" >0.8972</td>\n",
       "      <td id=\"T_da721_row6_col3\" class=\"data row6 col3\" >0.8947</td>\n",
       "      <td id=\"T_da721_row6_col4\" class=\"data row6 col4\" >0.8920</td>\n",
       "      <td id=\"T_da721_row6_col5\" class=\"data row6 col5\" >0.6780</td>\n",
       "      <td id=\"T_da721_row6_col6\" class=\"data row6 col6\" >0.6880</td>\n",
       "      <td id=\"T_da721_row6_col7\" class=\"data row6 col7\" >0.8101</td>\n",
       "      <td id=\"T_da721_row6_col8\" class=\"data row6 col8\" >0.1028</td>\n",
       "      <td id=\"T_da721_row6_col9\" class=\"data row6 col9\" >0.5880</td>\n",
       "      <td id=\"T_da721_row6_col10\" class=\"data row6 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_da721_row7_col0\" class=\"data row7 col0\" >0.8903</td>\n",
       "      <td id=\"T_da721_row7_col1\" class=\"data row7 col1\" >0.9029</td>\n",
       "      <td id=\"T_da721_row7_col2\" class=\"data row7 col2\" >0.8903</td>\n",
       "      <td id=\"T_da721_row7_col3\" class=\"data row7 col3\" >0.8869</td>\n",
       "      <td id=\"T_da721_row7_col4\" class=\"data row7 col4\" >0.8877</td>\n",
       "      <td id=\"T_da721_row7_col5\" class=\"data row7 col5\" >0.6710</td>\n",
       "      <td id=\"T_da721_row7_col6\" class=\"data row7 col6\" >0.6735</td>\n",
       "      <td id=\"T_da721_row7_col7\" class=\"data row7 col7\" >0.8206</td>\n",
       "      <td id=\"T_da721_row7_col8\" class=\"data row7 col8\" >0.1097</td>\n",
       "      <td id=\"T_da721_row7_col9\" class=\"data row7 col9\" >0.5875</td>\n",
       "      <td id=\"T_da721_row7_col10\" class=\"data row7 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_da721_row8_col0\" class=\"data row8 col0\" >0.9035</td>\n",
       "      <td id=\"T_da721_row8_col1\" class=\"data row8 col1\" >0.9065</td>\n",
       "      <td id=\"T_da721_row8_col2\" class=\"data row8 col2\" >0.9035</td>\n",
       "      <td id=\"T_da721_row8_col3\" class=\"data row8 col3\" >0.9010</td>\n",
       "      <td id=\"T_da721_row8_col4\" class=\"data row8 col4\" >0.8992</td>\n",
       "      <td id=\"T_da721_row8_col5\" class=\"data row8 col5\" >0.7012</td>\n",
       "      <td id=\"T_da721_row8_col6\" class=\"data row8 col6\" >0.7087</td>\n",
       "      <td id=\"T_da721_row8_col7\" class=\"data row8 col7\" >0.8246</td>\n",
       "      <td id=\"T_da721_row8_col8\" class=\"data row8 col8\" >0.0965</td>\n",
       "      <td id=\"T_da721_row8_col9\" class=\"data row8 col9\" >0.6137</td>\n",
       "      <td id=\"T_da721_row8_col10\" class=\"data row8 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_da721_row9_col0\" class=\"data row9 col0\" >0.8946</td>\n",
       "      <td id=\"T_da721_row9_col1\" class=\"data row9 col1\" >0.9071</td>\n",
       "      <td id=\"T_da721_row9_col2\" class=\"data row9 col2\" >0.8946</td>\n",
       "      <td id=\"T_da721_row9_col3\" class=\"data row9 col3\" >0.8922</td>\n",
       "      <td id=\"T_da721_row9_col4\" class=\"data row9 col4\" >0.8887</td>\n",
       "      <td id=\"T_da721_row9_col5\" class=\"data row9 col5\" >0.6674</td>\n",
       "      <td id=\"T_da721_row9_col6\" class=\"data row9 col6\" >0.6791</td>\n",
       "      <td id=\"T_da721_row9_col7\" class=\"data row9 col7\" >0.8029</td>\n",
       "      <td id=\"T_da721_row9_col8\" class=\"data row9 col8\" >0.1054</td>\n",
       "      <td id=\"T_da721_row9_col9\" class=\"data row9 col9\" >0.5762</td>\n",
       "      <td id=\"T_da721_row9_col10\" class=\"data row9 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_da721_row10_col0\" class=\"data row10 col0\" >0.8806</td>\n",
       "      <td id=\"T_da721_row10_col1\" class=\"data row10 col1\" >0.8885</td>\n",
       "      <td id=\"T_da721_row10_col2\" class=\"data row10 col2\" >0.8806</td>\n",
       "      <td id=\"T_da721_row10_col3\" class=\"data row10 col3\" >0.8794</td>\n",
       "      <td id=\"T_da721_row10_col4\" class=\"data row10 col4\" >0.8768</td>\n",
       "      <td id=\"T_da721_row10_col5\" class=\"data row10 col5\" >0.6385</td>\n",
       "      <td id=\"T_da721_row10_col6\" class=\"data row10 col6\" >0.6464</td>\n",
       "      <td id=\"T_da721_row10_col7\" class=\"data row10 col7\" >0.7975</td>\n",
       "      <td id=\"T_da721_row10_col8\" class=\"data row10 col8\" >0.1194</td>\n",
       "      <td id=\"T_da721_row10_col9\" class=\"data row10 col9\" >0.5571</td>\n",
       "      <td id=\"T_da721_row10_col10\" class=\"data row10 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da721_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_da721_row11_col0\" class=\"data row11 col0\" >0.0358</td>\n",
       "      <td id=\"T_da721_row11_col1\" class=\"data row11 col1\" >0.0395</td>\n",
       "      <td id=\"T_da721_row11_col2\" class=\"data row11 col2\" >0.0358</td>\n",
       "      <td id=\"T_da721_row11_col3\" class=\"data row11 col3\" >0.0321</td>\n",
       "      <td id=\"T_da721_row11_col4\" class=\"data row11 col4\" >0.0329</td>\n",
       "      <td id=\"T_da721_row11_col5\" class=\"data row11 col5\" >0.0863</td>\n",
       "      <td id=\"T_da721_row11_col6\" class=\"data row11 col6\" >0.0891</td>\n",
       "      <td id=\"T_da721_row11_col7\" class=\"data row11 col7\" >0.0315</td>\n",
       "      <td id=\"T_da721_row11_col8\" class=\"data row11 col8\" >0.0358</td>\n",
       "      <td id=\"T_da721_row11_col9\" class=\"data row11 col9\" >0.0693</td>\n",
       "      <td id=\"T_da721_row11_col10\" class=\"data row11 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22839418340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:10:07 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "svm = create_model('svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fd446a9-4a12-4741-ba02-e0ca04b335d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1e213_row10_col0, #T_1e213_row10_col1, #T_1e213_row10_col2, #T_1e213_row10_col3, #T_1e213_row10_col4, #T_1e213_row10_col5, #T_1e213_row10_col6, #T_1e213_row10_col7, #T_1e213_row10_col8, #T_1e213_row10_col9, #T_1e213_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1e213\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1e213_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_1e213_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_1e213_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_1e213_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_1e213_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_1e213_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_1e213_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_1e213_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_1e213_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_1e213_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_1e213_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1e213_row0_col0\" class=\"data row0 col0\" >0.8875</td>\n",
       "      <td id=\"T_1e213_row0_col1\" class=\"data row0 col1\" >0.9062</td>\n",
       "      <td id=\"T_1e213_row0_col2\" class=\"data row0 col2\" >0.8875</td>\n",
       "      <td id=\"T_1e213_row0_col3\" class=\"data row0 col3\" >0.8840</td>\n",
       "      <td id=\"T_1e213_row0_col4\" class=\"data row0 col4\" >0.8849</td>\n",
       "      <td id=\"T_1e213_row0_col5\" class=\"data row0 col5\" >0.6636</td>\n",
       "      <td id=\"T_1e213_row0_col6\" class=\"data row0 col6\" >0.6656</td>\n",
       "      <td id=\"T_1e213_row0_col7\" class=\"data row0 col7\" >0.8181</td>\n",
       "      <td id=\"T_1e213_row0_col8\" class=\"data row0 col8\" >0.1125</td>\n",
       "      <td id=\"T_1e213_row0_col9\" class=\"data row0 col9\" >0.5806</td>\n",
       "      <td id=\"T_1e213_row0_col10\" class=\"data row0 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1e213_row1_col0\" class=\"data row1 col0\" >0.8829</td>\n",
       "      <td id=\"T_1e213_row1_col1\" class=\"data row1 col1\" >0.9004</td>\n",
       "      <td id=\"T_1e213_row1_col2\" class=\"data row1 col2\" >0.8829</td>\n",
       "      <td id=\"T_1e213_row1_col3\" class=\"data row1 col3\" >0.8791</td>\n",
       "      <td id=\"T_1e213_row1_col4\" class=\"data row1 col4\" >0.8801</td>\n",
       "      <td id=\"T_1e213_row1_col5\" class=\"data row1 col5\" >0.6492</td>\n",
       "      <td id=\"T_1e213_row1_col6\" class=\"data row1 col6\" >0.6513</td>\n",
       "      <td id=\"T_1e213_row1_col7\" class=\"data row1 col7\" >0.8106</td>\n",
       "      <td id=\"T_1e213_row1_col8\" class=\"data row1 col8\" >0.1171</td>\n",
       "      <td id=\"T_1e213_row1_col9\" class=\"data row1 col9\" >0.5662</td>\n",
       "      <td id=\"T_1e213_row1_col10\" class=\"data row1 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1e213_row2_col0\" class=\"data row2 col0\" >0.8111</td>\n",
       "      <td id=\"T_1e213_row2_col1\" class=\"data row2 col1\" >0.8123</td>\n",
       "      <td id=\"T_1e213_row2_col2\" class=\"data row2 col2\" >0.8111</td>\n",
       "      <td id=\"T_1e213_row2_col3\" class=\"data row2 col3\" >0.8120</td>\n",
       "      <td id=\"T_1e213_row2_col4\" class=\"data row2 col4\" >0.8116</td>\n",
       "      <td id=\"T_1e213_row2_col5\" class=\"data row2 col5\" >0.4611</td>\n",
       "      <td id=\"T_1e213_row2_col6\" class=\"data row2 col6\" >0.4612</td>\n",
       "      <td id=\"T_1e213_row2_col7\" class=\"data row2 col7\" >0.7316</td>\n",
       "      <td id=\"T_1e213_row2_col8\" class=\"data row2 col8\" >0.1889</td>\n",
       "      <td id=\"T_1e213_row2_col9\" class=\"data row2 col9\" >0.4117</td>\n",
       "      <td id=\"T_1e213_row2_col10\" class=\"data row2 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1e213_row3_col0\" class=\"data row3 col0\" >0.8133</td>\n",
       "      <td id=\"T_1e213_row3_col1\" class=\"data row3 col1\" >0.8151</td>\n",
       "      <td id=\"T_1e213_row3_col2\" class=\"data row3 col2\" >0.8133</td>\n",
       "      <td id=\"T_1e213_row3_col3\" class=\"data row3 col3\" >0.8145</td>\n",
       "      <td id=\"T_1e213_row3_col4\" class=\"data row3 col4\" >0.8139</td>\n",
       "      <td id=\"T_1e213_row3_col5\" class=\"data row3 col5\" >0.4683</td>\n",
       "      <td id=\"T_1e213_row3_col6\" class=\"data row3 col6\" >0.4683</td>\n",
       "      <td id=\"T_1e213_row3_col7\" class=\"data row3 col7\" >0.7357</td>\n",
       "      <td id=\"T_1e213_row3_col8\" class=\"data row3 col8\" >0.1867</td>\n",
       "      <td id=\"T_1e213_row3_col9\" class=\"data row3 col9\" >0.4175</td>\n",
       "      <td id=\"T_1e213_row3_col10\" class=\"data row3 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1e213_row4_col0\" class=\"data row4 col0\" >0.8931</td>\n",
       "      <td id=\"T_1e213_row4_col1\" class=\"data row4 col1\" >0.9182</td>\n",
       "      <td id=\"T_1e213_row4_col2\" class=\"data row4 col2\" >0.8931</td>\n",
       "      <td id=\"T_1e213_row4_col3\" class=\"data row4 col3\" >0.8895</td>\n",
       "      <td id=\"T_1e213_row4_col4\" class=\"data row4 col4\" >0.8894</td>\n",
       "      <td id=\"T_1e213_row4_col5\" class=\"data row4 col5\" >0.6739</td>\n",
       "      <td id=\"T_1e213_row4_col6\" class=\"data row4 col6\" >0.6786</td>\n",
       "      <td id=\"T_1e213_row4_col7\" class=\"data row4 col7\" >0.8165</td>\n",
       "      <td id=\"T_1e213_row4_col8\" class=\"data row4 col8\" >0.1069</td>\n",
       "      <td id=\"T_1e213_row4_col9\" class=\"data row4 col9\" >0.5879</td>\n",
       "      <td id=\"T_1e213_row4_col10\" class=\"data row4 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1e213_row5_col0\" class=\"data row5 col0\" >0.8933</td>\n",
       "      <td id=\"T_1e213_row5_col1\" class=\"data row5 col1\" >0.9152</td>\n",
       "      <td id=\"T_1e213_row5_col2\" class=\"data row5 col2\" >0.8933</td>\n",
       "      <td id=\"T_1e213_row5_col3\" class=\"data row5 col3\" >0.8898</td>\n",
       "      <td id=\"T_1e213_row5_col4\" class=\"data row5 col4\" >0.8902</td>\n",
       "      <td id=\"T_1e213_row5_col5\" class=\"data row5 col5\" >0.6775</td>\n",
       "      <td id=\"T_1e213_row5_col6\" class=\"data row5 col6\" >0.6808</td>\n",
       "      <td id=\"T_1e213_row5_col7\" class=\"data row5 col7\" >0.8212</td>\n",
       "      <td id=\"T_1e213_row5_col8\" class=\"data row5 col8\" >0.1067</td>\n",
       "      <td id=\"T_1e213_row5_col9\" class=\"data row5 col9\" >0.5928</td>\n",
       "      <td id=\"T_1e213_row5_col10\" class=\"data row5 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1e213_row6_col0\" class=\"data row6 col0\" >0.8841</td>\n",
       "      <td id=\"T_1e213_row6_col1\" class=\"data row6 col1\" >0.9031</td>\n",
       "      <td id=\"T_1e213_row6_col2\" class=\"data row6 col2\" >0.8841</td>\n",
       "      <td id=\"T_1e213_row6_col3\" class=\"data row6 col3\" >0.8806</td>\n",
       "      <td id=\"T_1e213_row6_col4\" class=\"data row6 col4\" >0.8816</td>\n",
       "      <td id=\"T_1e213_row6_col5\" class=\"data row6 col5\" >0.6541</td>\n",
       "      <td id=\"T_1e213_row6_col6\" class=\"data row6 col6\" >0.6559</td>\n",
       "      <td id=\"T_1e213_row6_col7\" class=\"data row6 col7\" >0.8139</td>\n",
       "      <td id=\"T_1e213_row6_col8\" class=\"data row6 col8\" >0.1159</td>\n",
       "      <td id=\"T_1e213_row6_col9\" class=\"data row6 col9\" >0.5714</td>\n",
       "      <td id=\"T_1e213_row6_col10\" class=\"data row6 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1e213_row7_col0\" class=\"data row7 col0\" >0.8830</td>\n",
       "      <td id=\"T_1e213_row7_col1\" class=\"data row7 col1\" >0.9107</td>\n",
       "      <td id=\"T_1e213_row7_col2\" class=\"data row7 col2\" >0.8830</td>\n",
       "      <td id=\"T_1e213_row7_col3\" class=\"data row7 col3\" >0.8800</td>\n",
       "      <td id=\"T_1e213_row7_col4\" class=\"data row7 col4\" >0.8810</td>\n",
       "      <td id=\"T_1e213_row7_col5\" class=\"data row7 col5\" >0.6534</td>\n",
       "      <td id=\"T_1e213_row7_col6\" class=\"data row7 col6\" >0.6547</td>\n",
       "      <td id=\"T_1e213_row7_col7\" class=\"data row7 col7\" >0.8161</td>\n",
       "      <td id=\"T_1e213_row7_col8\" class=\"data row7 col8\" >0.1170</td>\n",
       "      <td id=\"T_1e213_row7_col9\" class=\"data row7 col9\" >0.5719</td>\n",
       "      <td id=\"T_1e213_row7_col10\" class=\"data row7 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1e213_row8_col0\" class=\"data row8 col0\" >0.9005</td>\n",
       "      <td id=\"T_1e213_row8_col1\" class=\"data row8 col1\" >0.9187</td>\n",
       "      <td id=\"T_1e213_row8_col2\" class=\"data row8 col2\" >0.9005</td>\n",
       "      <td id=\"T_1e213_row8_col3\" class=\"data row8 col3\" >0.8975</td>\n",
       "      <td id=\"T_1e213_row8_col4\" class=\"data row8 col4\" >0.8975</td>\n",
       "      <td id=\"T_1e213_row8_col5\" class=\"data row8 col5\" >0.6985</td>\n",
       "      <td id=\"T_1e213_row8_col6\" class=\"data row8 col6\" >0.7023</td>\n",
       "      <td id=\"T_1e213_row8_col7\" class=\"data row8 col7\" >0.8303</td>\n",
       "      <td id=\"T_1e213_row8_col8\" class=\"data row8 col8\" >0.0995</td>\n",
       "      <td id=\"T_1e213_row8_col9\" class=\"data row8 col9\" >0.6139</td>\n",
       "      <td id=\"T_1e213_row8_col10\" class=\"data row8 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1e213_row9_col0\" class=\"data row9 col0\" >0.8842</td>\n",
       "      <td id=\"T_1e213_row9_col1\" class=\"data row9 col1\" >0.9010</td>\n",
       "      <td id=\"T_1e213_row9_col2\" class=\"data row9 col2\" >0.8842</td>\n",
       "      <td id=\"T_1e213_row9_col3\" class=\"data row9 col3\" >0.8804</td>\n",
       "      <td id=\"T_1e213_row9_col4\" class=\"data row9 col4\" >0.8812</td>\n",
       "      <td id=\"T_1e213_row9_col5\" class=\"data row9 col5\" >0.6519</td>\n",
       "      <td id=\"T_1e213_row9_col6\" class=\"data row9 col6\" >0.6545</td>\n",
       "      <td id=\"T_1e213_row9_col7\" class=\"data row9 col7\" >0.8108</td>\n",
       "      <td id=\"T_1e213_row9_col8\" class=\"data row9 col8\" >0.1158</td>\n",
       "      <td id=\"T_1e213_row9_col9\" class=\"data row9 col9\" >0.5684</td>\n",
       "      <td id=\"T_1e213_row9_col10\" class=\"data row9 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_1e213_row10_col0\" class=\"data row10 col0\" >0.8733</td>\n",
       "      <td id=\"T_1e213_row10_col1\" class=\"data row10 col1\" >0.8901</td>\n",
       "      <td id=\"T_1e213_row10_col2\" class=\"data row10 col2\" >0.8733</td>\n",
       "      <td id=\"T_1e213_row10_col3\" class=\"data row10 col3\" >0.8707</td>\n",
       "      <td id=\"T_1e213_row10_col4\" class=\"data row10 col4\" >0.8711</td>\n",
       "      <td id=\"T_1e213_row10_col5\" class=\"data row10 col5\" >0.6251</td>\n",
       "      <td id=\"T_1e213_row10_col6\" class=\"data row10 col6\" >0.6273</td>\n",
       "      <td id=\"T_1e213_row10_col7\" class=\"data row10 col7\" >0.8005</td>\n",
       "      <td id=\"T_1e213_row10_col8\" class=\"data row10 col8\" >0.1267</td>\n",
       "      <td id=\"T_1e213_row10_col9\" class=\"data row10 col9\" >0.5482</td>\n",
       "      <td id=\"T_1e213_row10_col10\" class=\"data row10 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e213_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_1e213_row11_col0\" class=\"data row11 col0\" >0.0310</td>\n",
       "      <td id=\"T_1e213_row11_col1\" class=\"data row11 col1\" >0.0387</td>\n",
       "      <td id=\"T_1e213_row11_col2\" class=\"data row11 col2\" >0.0310</td>\n",
       "      <td id=\"T_1e213_row11_col3\" class=\"data row11 col3\" >0.0293</td>\n",
       "      <td id=\"T_1e213_row11_col4\" class=\"data row11 col4\" >0.0297</td>\n",
       "      <td id=\"T_1e213_row11_col5\" class=\"data row11 col5\" >0.0815</td>\n",
       "      <td id=\"T_1e213_row11_col6\" class=\"data row11 col6\" >0.0827</td>\n",
       "      <td id=\"T_1e213_row11_col7\" class=\"data row11 col7\" >0.0338</td>\n",
       "      <td id=\"T_1e213_row11_col8\" class=\"data row11 col8\" >0.0310</td>\n",
       "      <td id=\"T_1e213_row11_col9\" class=\"data row11 col9\" >0.0682</td>\n",
       "      <td id=\"T_1e213_row11_col10\" class=\"data row11 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22816326e60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:12:19 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "svm_tuned_model, svm_tuner = tune_model(svm, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "459771ad-d31e-4e53-aedc-578b117a8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',\n",
      "              power_t=0.5, random_state=7641, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(svm_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0331c3f7-63da-4ae5-b05c-8b29014339ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_804f7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_804f7_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_804f7_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_804f7_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_804f7_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_804f7_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_804f7_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_804f7_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_804f7_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_804f7_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_804f7_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_804f7_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_804f7_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_804f7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_804f7_row0_col0\" class=\"data row0 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_804f7_row0_col1\" class=\"data row0 col1\" >0.8889</td>\n",
       "      <td id=\"T_804f7_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_804f7_row0_col3\" class=\"data row0 col3\" >0.8889</td>\n",
       "      <td id=\"T_804f7_row0_col4\" class=\"data row0 col4\" >0.8863</td>\n",
       "      <td id=\"T_804f7_row0_col5\" class=\"data row0 col5\" >0.8821</td>\n",
       "      <td id=\"T_804f7_row0_col6\" class=\"data row0 col6\" >0.6478</td>\n",
       "      <td id=\"T_804f7_row0_col7\" class=\"data row0 col7\" >0.6613</td>\n",
       "      <td id=\"T_804f7_row0_col8\" class=\"data row0 col8\" >0.7918</td>\n",
       "      <td id=\"T_804f7_row0_col9\" class=\"data row0 col9\" >0.1111</td>\n",
       "      <td id=\"T_804f7_row0_col10\" class=\"data row0 col10\" >0.5558</td>\n",
       "      <td id=\"T_804f7_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2281288e0b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_svm = predict_model(svm_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d6dd58a-03ef-438f-9194-e28dc7e12d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505f8f9fd85042a0b27fd9aae8c487e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9ad3a02-579f-47c2-97b0-1dd8a2079bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>16.222591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>14.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>8.031571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rms</td>\n",
       "      <td>7.827267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>std</td>\n",
       "      <td>6.623096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>6.519983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>5.241671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>4.964353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ptp</td>\n",
       "      <td>4.807109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>3.328294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>2.997427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clearance</td>\n",
       "      <td>2.667385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>1.495123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>energy</td>\n",
       "      <td>1.233966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>1.198181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>impulse</td>\n",
       "      <td>1.074453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>1.047032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>crest</td>\n",
       "      <td>0.655567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>0.602363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.549109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>skewness</td>\n",
       "      <td>0.437550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>0.255262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>0.137788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>0.131904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>0.072622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.026940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shape</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0   entropySpectrogram   16.222591\n",
       "1          meanWavelet   14.193700\n",
       "2       entropyWavelet    8.031571\n",
       "3                  rms    7.827267\n",
       "4                  std    6.623096\n",
       "5    energySpectrogram    6.519983\n",
       "6           varWavelet    5.241671\n",
       "7       varSpectrogram    4.964353\n",
       "8                  ptp    4.807109\n",
       "9     spectral_entropy    3.328294\n",
       "10       energyWavelet    2.997427\n",
       "11           clearance    2.667385\n",
       "12   spectral_flatness    1.495123\n",
       "13              energy    1.233966\n",
       "14          medianFreq    1.198181\n",
       "15             impulse    1.074453\n",
       "16     meanSpectrogram    1.047032\n",
       "17               crest    0.655567\n",
       "18            meanFreq    0.602363\n",
       "19             entropy    0.549109\n",
       "20            skewness    0.437550\n",
       "21   spectral_kurtosis    0.255262\n",
       "22   spectral_skewness    0.137788\n",
       "23            kurtosis    0.131904\n",
       "24           bandwidth    0.072622\n",
       "25                mean    0.026940\n",
       "26               shape    0.000807"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_top_features = get_svm_feature_importance_df(svm, features_df_training_normalized)\n",
    "svm_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "049dda80-ba01-474f-b66d-250dcb0e8d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_78cad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_78cad_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_78cad_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_78cad_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_78cad_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_78cad_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_78cad_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_78cad_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_78cad_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_78cad_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_78cad_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_78cad_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_78cad_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_78cad_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_78cad_row0_col0\" class=\"data row0 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_78cad_row0_col1\" class=\"data row0 col1\" >0.8889</td>\n",
       "      <td id=\"T_78cad_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_78cad_row0_col3\" class=\"data row0 col3\" >0.8889</td>\n",
       "      <td id=\"T_78cad_row0_col4\" class=\"data row0 col4\" >0.8863</td>\n",
       "      <td id=\"T_78cad_row0_col5\" class=\"data row0 col5\" >0.8821</td>\n",
       "      <td id=\"T_78cad_row0_col6\" class=\"data row0 col6\" >0.6478</td>\n",
       "      <td id=\"T_78cad_row0_col7\" class=\"data row0 col7\" >0.6613</td>\n",
       "      <td id=\"T_78cad_row0_col8\" class=\"data row0 col8\" >0.7918</td>\n",
       "      <td id=\"T_78cad_row0_col9\" class=\"data row0 col9\" >0.1111</td>\n",
       "      <td id=\"T_78cad_row0_col10\" class=\"data row0 col10\" >0.5558</td>\n",
       "      <td id=\"T_78cad_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x228353dda20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_svm = predict_model(svm, data=features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d4e08c3-a8c6-4423-a289-1be18b4402e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>meanWavelet</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-1.272307</td>\n",
       "      <td>-0.564226</td>\n",
       "      <td>-0.568537</td>\n",
       "      <td>1.528791</td>\n",
       "      <td>0.216020</td>\n",
       "      <td>-0.510851</td>\n",
       "      <td>0.125702</td>\n",
       "      <td>0.104172</td>\n",
       "      <td>-0.696706</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.532978</td>\n",
       "      <td>-0.280870</td>\n",
       "      <td>-0.318820</td>\n",
       "      <td>-0.302703</td>\n",
       "      <td>-0.292659</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>-0.093596</td>\n",
       "      <td>-0.079652</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261113</th>\n",
       "      <td>-0.464289</td>\n",
       "      <td>0.389850</td>\n",
       "      <td>0.392307</td>\n",
       "      <td>0.236188</td>\n",
       "      <td>-0.322119</td>\n",
       "      <td>0.347214</td>\n",
       "      <td>-0.110471</td>\n",
       "      <td>-0.137664</td>\n",
       "      <td>0.232741</td>\n",
       "      <td>-0.205468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448827</td>\n",
       "      <td>0.510743</td>\n",
       "      <td>1.116157</td>\n",
       "      <td>0.514528</td>\n",
       "      <td>0.549057</td>\n",
       "      <td>0.468693</td>\n",
       "      <td>0.505024</td>\n",
       "      <td>0.476336</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207745</th>\n",
       "      <td>-0.473546</td>\n",
       "      <td>0.357993</td>\n",
       "      <td>0.360417</td>\n",
       "      <td>-0.554096</td>\n",
       "      <td>-0.787129</td>\n",
       "      <td>0.162519</td>\n",
       "      <td>-0.831262</td>\n",
       "      <td>-0.831097</td>\n",
       "      <td>-0.045980</td>\n",
       "      <td>-0.864700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282735</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.665827</td>\n",
       "      <td>0.465029</td>\n",
       "      <td>0.464907</td>\n",
       "      <td>0.870633</td>\n",
       "      <td>0.803324</td>\n",
       "      <td>0.839726</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>-0.490795</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>0.125355</td>\n",
       "      <td>0.375499</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>0.076991</td>\n",
       "      <td>-0.343168</td>\n",
       "      <td>-0.295428</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>0.060085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.706681</td>\n",
       "      <td>0.148892</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.092594</td>\n",
       "      <td>0.108320</td>\n",
       "      <td>0.094701</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207040</th>\n",
       "      <td>-0.353068</td>\n",
       "      <td>0.308735</td>\n",
       "      <td>0.311149</td>\n",
       "      <td>0.414371</td>\n",
       "      <td>0.260009</td>\n",
       "      <td>0.414386</td>\n",
       "      <td>0.282649</td>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.326729</td>\n",
       "      <td>0.291802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359017</td>\n",
       "      <td>0.395646</td>\n",
       "      <td>1.035493</td>\n",
       "      <td>0.392030</td>\n",
       "      <td>0.422492</td>\n",
       "      <td>0.317823</td>\n",
       "      <td>0.349905</td>\n",
       "      <td>0.323606</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258687</th>\n",
       "      <td>-0.421962</td>\n",
       "      <td>-0.277288</td>\n",
       "      <td>-0.275546</td>\n",
       "      <td>0.249954</td>\n",
       "      <td>0.933059</td>\n",
       "      <td>-0.222218</td>\n",
       "      <td>-0.017272</td>\n",
       "      <td>0.138013</td>\n",
       "      <td>-0.257450</td>\n",
       "      <td>1.040160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321609</td>\n",
       "      <td>-0.162309</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.199004</td>\n",
       "      <td>-0.182658</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.058800</td>\n",
       "      <td>-0.051380</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29271</th>\n",
       "      <td>0.422849</td>\n",
       "      <td>-0.305670</td>\n",
       "      <td>-0.303739</td>\n",
       "      <td>-0.067514</td>\n",
       "      <td>-1.538958</td>\n",
       "      <td>-0.378466</td>\n",
       "      <td>-1.560269</td>\n",
       "      <td>-1.551168</td>\n",
       "      <td>-0.647673</td>\n",
       "      <td>-2.065707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351838</td>\n",
       "      <td>-0.178829</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>-0.214714</td>\n",
       "      <td>-0.214982</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.077448</td>\n",
       "      <td>-0.067680</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205706</th>\n",
       "      <td>-0.496038</td>\n",
       "      <td>-0.302834</td>\n",
       "      <td>-0.301201</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>0.769237</td>\n",
       "      <td>-0.220468</td>\n",
       "      <td>0.313372</td>\n",
       "      <td>0.362557</td>\n",
       "      <td>-0.235371</td>\n",
       "      <td>0.629632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311886</td>\n",
       "      <td>-0.184069</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.213110</td>\n",
       "      <td>-0.198862</td>\n",
       "      <td>-0.062567</td>\n",
       "      <td>-0.071403</td>\n",
       "      <td>-0.062071</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201271</th>\n",
       "      <td>-0.526296</td>\n",
       "      <td>-0.264085</td>\n",
       "      <td>-0.262413</td>\n",
       "      <td>-0.084871</td>\n",
       "      <td>0.369411</td>\n",
       "      <td>-0.234605</td>\n",
       "      <td>-0.231204</td>\n",
       "      <td>-0.123989</td>\n",
       "      <td>-0.298620</td>\n",
       "      <td>0.564308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304420</td>\n",
       "      <td>-0.155067</td>\n",
       "      <td>0.028787</td>\n",
       "      <td>-0.191423</td>\n",
       "      <td>-0.181192</td>\n",
       "      <td>-0.052162</td>\n",
       "      <td>-0.059515</td>\n",
       "      <td>-0.052184</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217690</th>\n",
       "      <td>-0.375742</td>\n",
       "      <td>0.747205</td>\n",
       "      <td>0.750013</td>\n",
       "      <td>1.155028</td>\n",
       "      <td>-0.124393</td>\n",
       "      <td>0.656685</td>\n",
       "      <td>-0.064616</td>\n",
       "      <td>-0.092430</td>\n",
       "      <td>0.465135</td>\n",
       "      <td>-0.163915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804869</td>\n",
       "      <td>1.155234</td>\n",
       "      <td>0.974433</td>\n",
       "      <td>1.168414</td>\n",
       "      <td>1.209354</td>\n",
       "      <td>2.298496</td>\n",
       "      <td>2.102510</td>\n",
       "      <td>2.272903</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6840 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "371    -1.272307 -0.564226 -0.568537  1.528791  0.216020 -0.510851  0.125702   \n",
       "261113 -0.464289  0.389850  0.392307  0.236188 -0.322119  0.347214 -0.110471   \n",
       "207745 -0.473546  0.357993  0.360417 -0.554096 -0.787129  0.162519 -0.831262   \n",
       "255723 -0.490795  0.123172  0.125355  0.375499 -0.076538  0.076991 -0.343168   \n",
       "207040 -0.353068  0.308735  0.311149  0.414371  0.260009  0.414386  0.282649   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "258687 -0.421962 -0.277288 -0.275546  0.249954  0.933059 -0.222218 -0.017272   \n",
       "29271   0.422849 -0.305670 -0.303739 -0.067514 -1.538958 -0.378466 -1.560269   \n",
       "205706 -0.496038 -0.302834 -0.301201  0.114997  0.769237 -0.220468  0.313372   \n",
       "201271 -0.526296 -0.264085 -0.262413 -0.084871  0.369411 -0.234605 -0.231204   \n",
       "217690 -0.375742  0.747205  0.750013  1.155028 -0.124393  0.656685 -0.064616   \n",
       "\n",
       "         impulse  clearance     shape  ...  meanWavelet  varWavelet  \\\n",
       "371     0.104172  -0.696706  0.053627  ...    -0.532978   -0.280870   \n",
       "261113 -0.137664   0.232741 -0.205468  ...     0.448827    0.510743   \n",
       "207745 -0.831097  -0.045980 -0.864700  ...     0.282735    0.543103   \n",
       "255723 -0.295428  -0.008315  0.060085  ...     0.142535    0.171299   \n",
       "207040  0.278528   0.326729  0.291802  ...     0.359017    0.395646   \n",
       "...          ...        ...       ...  ...          ...         ...   \n",
       "258687  0.138013  -0.257450  1.040160  ...    -0.321609   -0.162309   \n",
       "29271  -1.551168  -0.647673 -2.065707  ...    -0.351838   -0.178829   \n",
       "205706  0.362557  -0.235371  0.629632  ...    -0.311886   -0.184069   \n",
       "201271 -0.123989  -0.298620  0.564308  ...    -0.304420   -0.155067   \n",
       "217690 -0.092430   0.465135 -0.163915  ...     0.804869    1.155234   \n",
       "\n",
       "        entropyWavelet  energyWavelet  meanSpectrogram  varSpectrogram  \\\n",
       "371          -0.318820      -0.302703        -0.292659       -0.080855   \n",
       "261113        1.116157       0.514528         0.549057        0.468693   \n",
       "207745        0.665827       0.465029         0.464907        0.870633   \n",
       "255723        0.706681       0.148892         0.159414        0.092594   \n",
       "207040        1.035493       0.392030         0.422492        0.317823   \n",
       "...                ...            ...              ...             ...   \n",
       "258687        0.002188      -0.199004        -0.182658       -0.051269   \n",
       "29271         0.007978      -0.214714        -0.214982       -0.068415   \n",
       "205706        0.001882      -0.213110        -0.198862       -0.062567   \n",
       "201271        0.028787      -0.191423        -0.181192       -0.052162   \n",
       "217690        0.974433       1.168414         1.209354        2.298496   \n",
       "\n",
       "        entropySpectrogram  energySpectrogram    Label  prediction_label  \n",
       "371              -0.093596          -0.079652  healthy           damaged  \n",
       "261113            0.505024           0.476336  healthy           damaged  \n",
       "207745            0.803324           0.839726  healthy           damaged  \n",
       "255723            0.108320           0.094701  healthy           damaged  \n",
       "207040            0.349905           0.323606  healthy           damaged  \n",
       "...                    ...                ...      ...               ...  \n",
       "258687           -0.058800          -0.051380  healthy           damaged  \n",
       "29271            -0.077448          -0.067680  healthy           damaged  \n",
       "205706           -0.071403          -0.062071  healthy           damaged  \n",
       "201271           -0.059515          -0.052184  healthy           damaged  \n",
       "217690            2.102510           2.272903  damaged           healthy  \n",
       "\n",
       "[6840 rows x 29 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378342ee-8ffa-4f37-9937-a2051463887a",
   "metadata": {},
   "source": [
    "# Experiment Setup (DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c085121-3acd-4d2e-9fb4-5c19183fe186",
   "metadata": {},
   "source": [
    "## Configure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fee3568a-b1fa-4505-bd7f-88819ba246fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (157669, 28) | Val Shape: (39418, 28) | Test Shape: (49272, 28)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(features_df_training_normalized, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Train Shape: {train.shape} | Val Shape: {val.shape} | Test Shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41773c3d-7a5e-43b7-8682-2aea065f0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Label\"\n",
    "\n",
    "categorical_cols = [\n",
    "    col\n",
    "    for col in features_df_training_normalized.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "    if col != target\n",
    "]\n",
    "\n",
    "continuous_cols = features_df_training_normalized.select_dtypes(include=[\"number\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a28e6e44-1bea-4842-8482-9708cc2c1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Label\n",
      "Categorical inputs: []\n",
      "Continuous inputs: ['mean', 'rms', 'std', 'skewness', 'kurtosis', 'ptp', 'crest', 'impulse', 'clearance', 'shape', 'energy', 'entropy', 'meanFreq', 'medianFreq', 'bandwidth', 'spectral_flatness', 'spectral_entropy', 'spectral_skewness', 'spectral_kurtosis', 'meanWavelet', 'varWavelet', 'entropyWavelet', 'energyWavelet', 'meanSpectrogram', 'varSpectrogram', 'entropySpectrogram', 'energySpectrogram']\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:\", target)\n",
    "print(\"Categorical inputs:\", categorical_cols)  \n",
    "print(\"Continuous inputs:\", continuous_cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f339072-98d8-491e-a93c-cdcce0517bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    target=[target],\n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c70fc27-0191-47e7-b183-e78605871c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU: Yes\n"
     ]
    }
   ],
   "source": [
    "available_gpu=1 if torch.cuda.is_available() else 0\n",
    "print(f\"Available GPU: {'Yes' if available_gpu else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a562ad-72cd-4216-8f77-36c195d0f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    max_epochs=20,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "experiment_config = ExperimentConfig(\n",
    "        project_name=\"TEST\",\n",
    "        run_name=\"test\",\n",
    "        log_target=\"tensorboard\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c24af441-0171-4f3f-a221-95eedb193867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feeb2f9-8fcd-45c6-99f4-566f19f8f14e",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c76181c3-dfb9-4f2f-ae38-efd79cc13346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TabNet_Optimization(trial):\n",
    "    n_d     = trial.suggest_int(\"n_d\", 4, 64)\n",
    "    n_a     = trial.suggest_int(\"n_a\", 4, 64)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 3, 10)\n",
    "    gamma   = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
    "    embedding_dropout = trial.suggest_float(\"embedding_dropout\", 0, 1)\n",
    "    lr      = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "    \n",
    "    tabnet_config = TabNetModelConfig(\n",
    "        task=\"classification\",\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        learning_rate=lr,\n",
    "        n_independent=2,\n",
    "        metrics=[\n",
    "            \"auroc\",\n",
    "            \"recall\",\n",
    "            \"precision\",\n",
    "            \"f1_score\",\n",
    "            \"cohen_kappa\",\n",
    "            \"matthews_corrcoef\",\n",
    "            \"hamming_distance\",\n",
    "            \"jaccard_index\",\n",
    "        ],\n",
    "        metrics_prob_input=[\n",
    "            True,   # auroc\n",
    "            False,  # recall\n",
    "            False,  # precision\n",
    "            False,  # f1_score\n",
    "            False,  # cohen_kappa\n",
    "            False,  # matthews_corrcoef\n",
    "            False,  # hamming_distance\n",
    "            False,  # jaccard_index\n",
    "        ],\n",
    "        metrics_params=[\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # auroc\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # recall\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # precision\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # f1_score\n",
    "            {\"num_classes\": 2},                      # cohen_kappa\n",
    "            {},                                      # matthews_corrcoef\n",
    "            {},                                      # hamming_distance\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # jaccard_index\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tabnet_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=tabnet_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    tabnet_model.fit(train=train, validation=val)\n",
    "\n",
    "    preds_df = tabnet_model.predict(val)\n",
    "    y_pred = preds_df[\"Label_prediction\"].to_numpy()\n",
    "    y_true = val[\"Label\"].to_numpy()\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97f728e4-7cc5-4933-adfd-776aba50e437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">945</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:22\u001b[0m,\u001b[1;36m945\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">983</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:22\u001b[0m,\u001b[1;36m983\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">010</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:23\u001b[0m,\u001b[1;36m010\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:23\u001b[0m,\u001b[1;36m171\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:23\u001b[0m,\u001b[1;36m230\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:23\u001b[0m,\u001b[1;36m280\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca0a3edd3ce440ba785fbe00865ad8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_5d987533-67ed-4ae6-b2c0-f6b2838feca8.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_5d987533-67ed-4ae6-b2c0-f6b2838feca8.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">825</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:38\u001b[0m,\u001b[1;36m825\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:12:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">832</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:12:38\u001b[0m,\u001b[1;36m832\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │ 67.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │ 67.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 67.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 67.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 227                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 67.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 67.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 227                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41957683cb0c4128a6515f066c33fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:18:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">801</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:18:56\u001b[0m,\u001b[1;36m801\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:18:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">803</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:18:56\u001b[0m,\u001b[1;36m803\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">577</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:01\u001b[0m,\u001b[1;36m577\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">613</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:01\u001b[0m,\u001b[1;36m613\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">639</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:01\u001b[0m,\u001b[1;36m639\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">792</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:01\u001b[0m,\u001b[1;36m792\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">993</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:01\u001b[0m,\u001b[1;36m993\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:02</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">033</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:02\u001b[0m,\u001b[1;36m033\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49a07de8b28403cb4d817f25903ed41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.00478630092322638\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_4fef5d16-7ff8-43ac-8356-19c410e07468.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_4fef5d16-7ff8-43ac-8356-19c410e07468.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">422</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00478630092322638</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:16\u001b[0m,\u001b[1;36m422\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.00478630092322638\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:19:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">431</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:19:16\u001b[0m,\u001b[1;36m431\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  283 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  283 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 283 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 283 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 203                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 283 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 283 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 203                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385acbe7b2e84c43a4d0abedca4b3f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">876</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:12\u001b[0m,\u001b[1;36m876\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">878</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:12\u001b[0m,\u001b[1;36m878\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">221</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m221\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m255\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m284\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">442</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m442\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">496</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m496\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">537</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:18\u001b[0m,\u001b[1;36m537\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68b53265806410fa337d0f8304772c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_ea454e4b-37b4-405b-a54b-5d421a3cc4cb.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_ea454e4b-37b4-405b-a54b-5d421a3cc4cb.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">869</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:31\u001b[0m,\u001b[1;36m869\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:25:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">876</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:25:31\u001b[0m,\u001b[1;36m876\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │ 42.6 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │ 42.6 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 42.6 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 42.6 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 179                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 42.6 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 42.6 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 179                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7637597989241ffae9fac98fee01787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">544</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:13\u001b[0m,\u001b[1;36m544\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">546</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:13\u001b[0m,\u001b[1;36m546\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">019</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m019\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">053</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m053\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">084</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m084\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m248\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">310</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m310\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">352</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:18\u001b[0m,\u001b[1;36m352\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516cc29c812e406f98a5ed3322f6cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_e355e37c-bcc6-4772-989c-94f065bf3dab.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_e355e37c-bcc6-4772-989c-94f065bf3dab.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">721</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006918309709189364</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:34\u001b[0m,\u001b[1;36m721\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.006918309709189364\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:39:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">731</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:39:34\u001b[0m,\u001b[1;36m731\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  515 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  515 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 515 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 515 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 2                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 203                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 515 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 515 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 2                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 203                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57c4fde04884e39914bff4464489011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">869</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:37\u001b[0m,\u001b[1;36m869\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">871</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:37\u001b[0m,\u001b[1;36m871\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">015</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m015\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">049</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m049\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">079</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m079\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">235</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m235\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">292</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m292\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">334</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:44\u001b[0m,\u001b[1;36m334\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ae14fc8b73441697aa48dfe6ecc3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_19d2ad92-29d8-41ae-bcd8-c2ddf1cdd010.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_19d2ad92-29d8-41ae-bcd8-c2ddf1cdd010.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">769</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:56\u001b[0m,\u001b[1;36m769\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">776</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:56\u001b[0m,\u001b[1;36m776\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  214 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  214 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 214 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 214 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 155                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 214 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 214 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 155                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f04f40e8b84554b98a15ffe3ed28ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">057</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:10\u001b[0m,\u001b[1;36m057\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">059</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:10\u001b[0m,\u001b[1;36m059\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">322</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m322\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">359</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m359\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">386</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m386\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">549</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m549\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">609</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m609\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">650</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:14\u001b[0m,\u001b[1;36m650\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c518f8536154681a6019f118ef016cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.04365158322401657\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_8555d5cb-44cb-47d9-bebd-b54ec2d580c2.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_8555d5cb-44cb-47d9-bebd-b54ec2d580c2.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">092</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04365158322401657</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:29\u001b[0m,\u001b[1;36m092\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.04365158322401657\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">099</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:59:29\u001b[0m,\u001b[1;36m099\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  160 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  160 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 160 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 160 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 203                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 160 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 160 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 203                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7821856323542b583b79c2a22f37ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">069</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:40\u001b[0m,\u001b[1;36m069\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">071</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:40\u001b[0m,\u001b[1;36m071\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m120\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">155</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m155\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m184\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m340\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">390</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m390\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">431</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:45\u001b[0m,\u001b[1;36m431\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b941ae17c74b3bb63f3dba317ea8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.017378008287493765\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_d8588ac7-d175-40c2-a3f5-093a1fb01be1.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_d8588ac7-d175-40c2-a3f5-093a1fb01be1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">994</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017378008287493765</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:56\u001b[0m,\u001b[1;36m994\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.017378008287493765\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:06:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:06:57\u001b[0m,\u001b[1;36m000\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  130 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  130 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 130 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 130 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 131                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 130 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 130 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 131                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8207de8a92a745c5911ad6adc0b386e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">365</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:48\u001b[0m,\u001b[1;36m365\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">368</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:48\u001b[0m,\u001b[1;36m368\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">272</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m272\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">307</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m307\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">338</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m338\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">507</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m507\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">566</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m566\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:11:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">609</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:11:52\u001b[0m,\u001b[1;36m609\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44af95818bf444f9a2723a68f48a576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.2754228703338169\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_90b8eff0-2ce7-46b5-8c09-c28a219346e4.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_90b8eff0-2ce7-46b5-8c09-c28a219346e4.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:12:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">474</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2754228703338169</span>. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:12:06\u001b[0m,\u001b[1;36m474\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.2754228703338169\u001b[0m. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:12:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">480</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:12:06\u001b[0m,\u001b[1;36m480\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │ 78.2 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │ 78.2 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 78.2 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 78.2 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 155                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 78.2 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 78.2 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 155                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f8b7d83d34561b5b807af7cc9962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">278</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:35\u001b[0m,\u001b[1;36m278\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:35\u001b[0m,\u001b[1;36m280\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">195</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m195\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">231</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m231\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m260\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">419</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m419\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">486</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m486\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">538</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:39\u001b[0m,\u001b[1;36m538\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cce4987f5714664a6d799a741bcac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.05248074602497723\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_f078b9a8-b916-47a9-933b-622b7a587606.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_f078b9a8-b916-47a9-933b-622b7a587606.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">459</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05248074602497723</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:56\u001b[0m,\u001b[1;36m459\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.05248074602497723\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:17:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">469</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:17:56\u001b[0m,\u001b[1;36m469\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  423 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  423 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 423 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 423 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 251                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 423 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 423 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 251                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9490324405cd4f06af978819ed729dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:34:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">459</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:34:56\u001b[0m,\u001b[1;36m459\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:34:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:34:56\u001b[0m,\u001b[1;36m461\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">581</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m581\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">615</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m615\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">647</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m647\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">807</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m807\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">869</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m869\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">911</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:03\u001b[0m,\u001b[1;36m911\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1785f357e75a4597aa78d6db2349c495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_b56d5d5a-1a76-4749-972e-cbbe17c55345.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_b56d5d5a-1a76-4749-972e-cbbe17c55345.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">351</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008317637711026709</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:20\u001b[0m,\u001b[1;36m351\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.008317637711026709\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:35:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">359</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:35:20\u001b[0m,\u001b[1;36m359\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabNetBackbone   │  146 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Identity         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  146 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 146 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 146 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 203                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 146 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 146 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 203                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7dd6d714364f2881c69d0dc4c892b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">798</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:26\u001b[0m,\u001b[1;36m798\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">801</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:26\u001b[0m,\u001b[1;36m801\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabnet_study = optuna.create_study(direction=\"maximize\")\n",
    "tabnet_study.optimize(TabNet_Optimization, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b979953b-bebb-4b1f-90c2-ff9d4ed7d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_d': 8, 'n_a': 25, 'n_steps': 6, 'gamma': 1.1285381196618185, 'embedding_dropout': 0.4518844821358271, 'learning_rate': 0.00010830099505845923}\n",
      "Best F1 score: 0.8915709443898493\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", tabnet_study.best_params)\n",
    "print(\"Best F1 score:\", tabnet_study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a4748-8644-48c4-9cd9-0d63bc5cde54",
   "metadata": {},
   "source": [
    "## GANDALF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26bfc5a2-13df-4eea-bf2c-d2a22cce451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANDALF_Optimization(trial):\n",
    "    gflu_stages               = trial.suggest_int(\"gflu_stages\", 1, 10)\n",
    "    gflu_dropout              = trial.suggest_float(\"gflu_dropout\", 0.0, 0.5)\n",
    "    gflu_feature_init_sparsity = trial.suggest_float(\"gflu_feature_init_sparsity\", 0.1, 0.9)\n",
    "    learnable_sparsity        = trial.suggest_categorical(\"learnable_sparsity\", [True, False])\n",
    "    embedding_dropout         = trial.suggest_float(\"embedding_dropout\", 0.0, 0.5)\n",
    "    batch_norm_continuous     = trial.suggest_categorical(\"batch_norm_continuous_input\", [True, False])\n",
    "    learning_rate             = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    # 2) build Gandalf config\n",
    "    gandalf_config = GANDALFConfig(\n",
    "        task=\"classification\",\n",
    "        gflu_stages=gflu_stages,\n",
    "        gflu_dropout=gflu_dropout,\n",
    "        gflu_feature_init_sparsity=gflu_feature_init_sparsity,\n",
    "        learnable_sparsity=learnable_sparsity,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        batch_norm_continuous_input=batch_norm_continuous,\n",
    "        learning_rate=learning_rate,\n",
    "        metrics=[\n",
    "            \"auroc\",\n",
    "            \"recall\",\n",
    "            \"precision\",\n",
    "            \"f1_score\",\n",
    "            \"cohen_kappa\",\n",
    "            \"matthews_corrcoef\",\n",
    "            \"hamming_distance\",\n",
    "            \"jaccard_index\",\n",
    "        ],\n",
    "        metrics_prob_input=[\n",
    "            True,   # auroc\n",
    "            False,  # recall\n",
    "            False,  # precision\n",
    "            False,  # f1_score\n",
    "            False,  # cohen_kappa\n",
    "            False,  # matthews_corrcoef\n",
    "            False,  # hamming_distance\n",
    "            False,  # jaccard_index\n",
    "        ],\n",
    "        metrics_params=[\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # auroc\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # recall\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # precision\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # f1_score\n",
    "            {\"num_classes\": 2},                      # cohen_kappa\n",
    "            {},                                      # matthews_corrcoef\n",
    "            {},                                      # hamming_distance\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # jaccard_index\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3) instantiate & train\n",
    "    model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=gandalf_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    model.fit(train=train, validation=val)\n",
    "\n",
    "    # 4) predict & return macro-F1\n",
    "    preds = model.predict(val)\n",
    "    y_pred = preds[\"Label_prediction\"].to_numpy()\n",
    "    y_true = val[\"Label\"].to_numpy()\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f7e9818-c3ab-4d02-90d7-8d845fb10055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">932</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:31\u001b[0m,\u001b[1;36m932\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">969</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:31\u001b[0m,\u001b[1;36m969\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">001</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:32\u001b[0m,\u001b[1;36m001\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:32\u001b[0m,\u001b[1;36m161\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:32\u001b[0m,\u001b[1;36m211\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">252</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:32\u001b[0m,\u001b[1;36m252\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a1c0a38eab4e4d837d976c67b6737b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.02089296130854041\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_54748874-d5a3-460a-8d3e-8754ceae5b03.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_54748874-d5a3-460a-8d3e-8754ceae5b03.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">054</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02089296130854041</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:39\u001b[0m,\u001b[1;36m054\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.02089296130854041\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:45:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">057</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:45:39\u001b[0m,\u001b[1;36m057\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │  4.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │  4.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.6 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 1                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.6 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 18                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.6 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 1                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.6 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 18                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714b7e930d154a31b07341a257a52cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">479</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:43\u001b[0m,\u001b[1;36m479\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">481</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:43\u001b[0m,\u001b[1;36m481\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">602</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m602\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">636</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m636\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">668</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m668\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">854</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m854\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">898</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m898\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">941</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:44\u001b[0m,\u001b[1;36m941\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790a08003594f2db529b8c02d97db8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_30d4fc11-be05-4354-a8ae-9f20f616f274.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_30d4fc11-be05-4354-a8ae-9f20f616f274.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">050</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:52\u001b[0m,\u001b[1;36m050\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:51:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">053</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:51:52\u001b[0m,\u001b[1;36m053\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 13.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 13.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 3                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 13.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 20                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 13.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 3                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 13.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 20                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4085f83c93c84af1b508078a73d37c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">469</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:17\u001b[0m,\u001b[1;36m469\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">470</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:17\u001b[0m,\u001b[1;36m470\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">849</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:18\u001b[0m,\u001b[1;36m849\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">881</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:18\u001b[0m,\u001b[1;36m881\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">912</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:18\u001b[0m,\u001b[1;36m912\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">096</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:19\u001b[0m,\u001b[1;36m096\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">143</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:19\u001b[0m,\u001b[1;36m143\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">189</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:19\u001b[0m,\u001b[1;36m189\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77cdb7be3e745ea88804aab66b77d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.012022644346174132\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_9076269a-d0fd-4b87-a714-f97b7000c6d0.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_9076269a-d0fd-4b87-a714-f97b7000c6d0.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">182</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012022644346174132</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:27\u001b[0m,\u001b[1;36m182\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.012022644346174132\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:54:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">185</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:54:27\u001b[0m,\u001b[1;36m185\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 22.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 22.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 22.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 24                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 22.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 22.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 24                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0f76af49374d8287422dc9f29066dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:01\u001b[0m,\u001b[1;36m548\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">550</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:01\u001b[0m,\u001b[1;36m550\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m139\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m170\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m200\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">381</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m381\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">426</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m426\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">470</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:03\u001b[0m,\u001b[1;36m470\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fffce0e5e6d4a3e84432aca2059efbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_b725425c-0204-45d6-99eb-3fdfa1a44d6d.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_b725425c-0204-45d6-99eb-3fdfa1a44d6d.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008317637711026709</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:10\u001b[0m,\u001b[1;36m211\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.008317637711026709\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:57:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">214</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:57:10\u001b[0m,\u001b[1;36m214\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │  9.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │  9.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 9.0 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 2                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 9.0 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 18                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 9.0 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 2                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 9.0 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 18                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110824de33c242c4b45d10df775021ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">328</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:27\u001b[0m,\u001b[1;36m328\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">330</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:27\u001b[0m,\u001b[1;36m330\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">609</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m609\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">641</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m641\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">670</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m670\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">834</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m834\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">881</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m881\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">926</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:28\u001b[0m,\u001b[1;36m926\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec72b4f726c4a80a46225e273f95702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.02089296130854041\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_3066eab2-4150-4224-86cc-d8225bfebdd4.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_3066eab2-4150-4224-86cc-d8225bfebdd4.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">063</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02089296130854041</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:38\u001b[0m,\u001b[1;36m063\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.02089296130854041\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">066</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:38\u001b[0m,\u001b[1;36m066\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 40.3 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 40.3 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 40.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 9                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 40.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 34                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 40.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 9                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 40.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 34                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69de6eace5b44dd95e63a4b14f6a19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:41</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">604</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:41\u001b[0m,\u001b[1;36m604\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:41</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:41\u001b[0m,\u001b[1;36m606\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:43\u001b[0m,\u001b[1;36m689\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">721</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:43\u001b[0m,\u001b[1;36m721\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">752</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:43\u001b[0m,\u001b[1;36m752\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">934</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:43\u001b[0m,\u001b[1;36m934\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">978</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:43\u001b[0m,\u001b[1;36m978\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">019</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:44\u001b[0m,\u001b[1;36m019\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b9ec619835486cb5af7566d81b0945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.02089296130854041\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_5c770d11-c1d9-45c1-9448-8ad8c29ba0c0.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_5c770d11-c1d9-45c1-9448-8ad8c29ba0c0.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">562</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02089296130854041</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:50\u001b[0m,\u001b[1;36m562\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.02089296130854041\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">564</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:50\u001b[0m,\u001b[1;36m564\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │  4.5 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │  4.5 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.5 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.5 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 16                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.5 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.5 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 16                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17845aaec50745248bac2c7eff77fc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">822</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:28\u001b[0m,\u001b[1;36m822\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">824</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:28\u001b[0m,\u001b[1;36m824\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">963</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:29\u001b[0m,\u001b[1;36m963\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">997</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:29\u001b[0m,\u001b[1;36m997\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">026</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:30\u001b[0m,\u001b[1;36m026\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">189</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:30\u001b[0m,\u001b[1;36m189\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:30\u001b[0m,\u001b[1;36m238\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">283</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:30\u001b[0m,\u001b[1;36m283\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a025f05e6de34dd19e47b1bd41408960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_aca88c38-9336-4417-b627-48ac97512772.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_aca88c38-9336-4417-b627-48ac97512772.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:37\u001b[0m,\u001b[1;36m438\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:13:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">441</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:13:37\u001b[0m,\u001b[1;36m441\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 13.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 13.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.6 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 3                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 13.6 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 22                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 13.6 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 3                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 13.6 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 22                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d303f2adac94c3aaeb04e5c6237ceef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">315</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:46\u001b[0m,\u001b[1;36m315\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">318</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:46\u001b[0m,\u001b[1;36m318\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">711</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:47\u001b[0m,\u001b[1;36m711\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">745</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:47\u001b[0m,\u001b[1;36m745\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">774</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:47\u001b[0m,\u001b[1;36m774\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">941</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:47\u001b[0m,\u001b[1;36m941\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">987</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:47\u001b[0m,\u001b[1;36m987\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">034</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:48\u001b[0m,\u001b[1;36m034\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44fcc2c3c614523bbe35dad6cb10069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_75172943-82d7-4e88-8a3f-eabc23ab9697.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_75172943-82d7-4e88-8a3f-eabc23ab9697.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">921</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008317637711026709</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:54\u001b[0m,\u001b[1;36m921\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.008317637711026709\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">924</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:54\u001b[0m,\u001b[1;36m924\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │  9.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │  9.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 9.1 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 2                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 9.1 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 20                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 9.1 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 2                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 9.1 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 20                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a117d530152b4885a9217eba5009e7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">996</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:03\u001b[0m,\u001b[1;36m996\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">998</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:03\u001b[0m,\u001b[1;36m998\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m248\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">281</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m281\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">316</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m316\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">484</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m484\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">531</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m531\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">577</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:05\u001b[0m,\u001b[1;36m577\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50c008ac49e4a36bcc94de83ecd0bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.017378008287493765\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_6fccd785-7f1f-4589-82f2-afd907c7e4cb.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_6fccd785-7f1f-4589-82f2-afd907c7e4cb.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017378008287493765</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:14\u001b[0m,\u001b[1;36m342\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.017378008287493765\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">346</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:14\u001b[0m,\u001b[1;36m346\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 35.9 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 35.9 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │     54 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 36.0 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 8                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 36.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 32                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 36.0 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 8                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 36.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 32                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e594c3744bf4a23b74ffbd04c5cd580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">862</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:17\u001b[0m,\u001b[1;36m862\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:17\u001b[0m,\u001b[1;36m864\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">805</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:19\u001b[0m,\u001b[1;36m805\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">839</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:19\u001b[0m,\u001b[1;36m839\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">867</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:19\u001b[0m,\u001b[1;36m867\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">027</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:20\u001b[0m,\u001b[1;36m027\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">072</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:20\u001b[0m,\u001b[1;36m072\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:20\u001b[0m,\u001b[1;36m117\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da231ebcf3bb42a18383c577bf8f9dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_9c7d531f-373a-42e0-9624-df74968ce71a.ckpt\n",
      "Restored all states from the checkpoint at G:\\Semester Arbeit\\Programming\\masterarbeit_python\\.lr_find_9c7d531f-373a-42e0-9624-df74968ce71a.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">364</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:28\u001b[0m,\u001b[1;36m364\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:28:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:28:28\u001b[0m,\u001b[1;36m367\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ GANDALFBackbone  │ 31.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ Sequential       │     58 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GANDALFBackbone  │ 31.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ Sequential       │     58 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss │      0 │ train │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 31.4 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 7                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 31.4 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 28                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 31.4 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 7                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 31.4 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 28                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bbe120285f4de893ad69da1cb4d47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:31:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">382</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:31:11\u001b[0m,\u001b[1;36m382\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:31:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:31:11\u001b[0m,\u001b[1;36m384\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gandalf_study = optuna.create_study(direction=\"maximize\")\n",
    "gandalf_study.optimize(GANDALF_Optimization, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b78d45bc-c0e6-47bc-bb27-43fa402152d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gflu_stages': 1, 'gflu_dropout': 0.39399066451650944, 'gflu_feature_init_sparsity': 0.21721669194200377, 'learnable_sparsity': True, 'embedding_dropout': 0.034740846371525747, 'batch_norm_continuous_input': False, 'learning_rate': 8.440408399291e-05}\n",
      "Best F1 score: 0.9046138003493449\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", gandalf_study.best_params)\n",
    "print(\"Best F1 score:\", gandalf_study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
