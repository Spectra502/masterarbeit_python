{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e019071b-b915-4e05-93ac-b1d4ccc67773",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetFolderHBK = r\"H:\\Extracted_Features\\HBK\\HBK_14285Hz_original_all_features\\features\"\n",
    "targetFolderMCC5 = r\"H:\\Extracted_Features\\MCC5\\MCC5_12800Hz_original_all_features_motor_vibration_x\\features\"\n",
    "targetFolderSIZA = r\"H:\\Extracted_Features\\SIZA\\SIZA_original_all_features\\features\"\n",
    "normalization_method = \"min_max\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24c9ea-6eb1-4abc-a4a4-735cf1178187",
   "metadata": {},
   "source": [
    "# Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eaba3c8-a35f-4004-b6a5-a62b3eec640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, hamming_loss, hinge_loss, jaccard_score, log_loss, precision_score, recall_score, f1_score, make_scorer\n",
    "from pathlib import Path\n",
    "from pycaret.classification import * \n",
    "from torch import tensor\n",
    "from torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\n",
    "import optuna\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import GANDALFConfig, CategoryEmbeddingModel,GatedAdditiveTreeEnsembleConfig, NodeConfig, FTTransformerConfig, TabNetModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    ModelConfig,\n",
    "    TrainerConfig,\n",
    "    ExperimentConfig,\n",
    ")\n",
    "from collections import Counter\n",
    "from data_loader import load_feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3d6c1-e829-4aee-b5f4-19a277312efd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2758484f-d1a4-477c-a32f-e760037badc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataframe(dataframe, normalization_method):\n",
    "    \"\"\"\n",
    "    Normalizes the features of a dataframe using a specified method.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with a 'Label' column.\n",
    "        normalization_method (str): The method to use (\"min_max\", \"z_score\", \"robust_scaling\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with scaled features.\n",
    "    \"\"\"\n",
    "    # Separate features (X) and the target variable (y)\n",
    "    y = dataframe['Label']\n",
    "    X = dataframe.drop(columns=['Label'])\n",
    "\n",
    "    # Select the scaler based on the chosen method\n",
    "    if normalization_method == \"min_max\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif normalization_method == \"z_score\":\n",
    "        scaler = StandardScaler()\n",
    "    elif normalization_method == \"robust_scaling\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        # Raise an error for an invalid method name\n",
    "        raise ValueError(f\"Unknown normalization_method: '{normalization_method}'\")\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Rejoin the scaled features with the label column\n",
    "    df_scaled = X_scaled.join(y)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44aae2d5-3c4b-4bbb-bbbc-6621172653d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredictionHistograms(df, domain, normalization):\n",
    "    # 1) mark correct vs incorrect\n",
    "    df = df.copy()\n",
    "    df['prediction_quality'] = np.where(\n",
    "        df['Label'] == df['prediction_label'],\n",
    "        'correct',\n",
    "        'incorrect'\n",
    "    )\n",
    "    \n",
    "    # 2) choose a palette (you can override these colors if you like)\n",
    "    pal = dict(zip(\n",
    "        ['correct','incorrect'],\n",
    "        sns.color_palette(n_colors=2)\n",
    "    ))\n",
    "    \n",
    "    skip = {'Label','prediction_label','prediction_score','prediction_quality'}\n",
    "    for col in df.columns:\n",
    "        if col in skip:\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        sns.histplot(\n",
    "            data=df, x=col, hue='prediction_quality',\n",
    "            palette=pal,\n",
    "            kde=True, multiple='layer', element='step',\n",
    "            alpha=0.5,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        # 3) build a manual legend using the same palette\n",
    "        handles = [\n",
    "            mpatches.Patch(color=pal[k], label=k)\n",
    "            for k in ['correct','incorrect']\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=handles,\n",
    "            title='Prediction Quality'\n",
    "        )\n",
    "        \n",
    "        ax.set_title(\n",
    "            f\"Distribution of {col} in the '{domain}' domain\\n\"\n",
    "            f\"(normalization = '{normalization}')\"\n",
    "        )\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7180ffc-82c3-4100-a659-90dfe8bdeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_predictions(df):\n",
    "    return df[\n",
    "        ((df['Label'] == 'damaged')   & (df['prediction_label'] == 'healthy'))\n",
    "      | ((df['Label'] == 'healthy')  & (df['prediction_label'] == 'damaged'))\n",
    "    ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6effaf0-7730-46e4-9b29-a6133fc41f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_df(model, df):\n",
    "    importance = model.feature_importances_\n",
    "    n = len(importance)\n",
    "    features = df.columns[:n]\n",
    "    fi_df = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'importance': importance\n",
    "    })\n",
    "    return fi_df.sort_values(by='importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d896eca8-c332-4e1a-8643-647da5fb5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_feature_importance_df(model, df):\n",
    "    if not hasattr(model, 'coef_'):\n",
    "        raise ValueError(\"This SVM model has no coefficients. Use a linear kernel.\")\n",
    "    \n",
    "    importance = model.coef_.ravel()  # Flatten in case of binary classification\n",
    "    n = len(importance)\n",
    "    features = df.columns[:n]\n",
    "    fi_df = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'importance': abs(importance)\n",
    "    })\n",
    "    return fi_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3ffb6f-eed8-4a5d-9b91-737370fc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counter = Counter()\n",
    "def add_top_features(feature_df: pd.DataFrame, top_n: int):\n",
    "    top_features = feature_df.nlargest(top_n, 'importance')['Features']\n",
    "    feature_counter.update(top_features)\n",
    "    \n",
    "def plot_feature_importance():\n",
    "    feature_freq = pd.DataFrame(feature_counter.items(), columns=['Feature', 'Count'])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=feature_freq.sort_values(by='Count', ascending=False),\n",
    "                x='Feature', y='Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Feature Frequency Across Experiments')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dc31c-45ef-49c4-9f58-7890b966ccd7",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13cd2ff-10b7-4db8-9fc8-f968e9c5324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"Best_Hyperparameters_z_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ba897d-d88c-410a-9597-96585f9a7b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 161 files into a DataFrame with shape (200093, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (200093, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_HBK = load_feature_data(\n",
    "    features_path=targetFolderHBK,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b37f31-8878-4168-aa49-a720052e2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 36 files into a DataFrame with shape (53928, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (53928, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_SIZA = load_feature_data(\n",
    "    features_path=targetFolderSIZA,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87904925-1ca9-40f1-adec-82912fb5bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 36 files into a DataFrame with shape (53928, 30)\n",
      "Applied binary classification: 'healthy' vs 'damaged'.\n",
      "Dropped 'Speed' and 'Torque' columns.\n",
      "Final DataFrame shape: (53928, 28)\n"
     ]
    }
   ],
   "source": [
    "df_binary_MCC5 = load_feature_data(\n",
    "    features_path=targetFolderMCC5,\n",
    "    include_augmentations=False,      # Only 'original' data\n",
    "    include_speed_torque=False,       # Drop operating conditions\n",
    "    binary_classification=True,       # 'healthy' vs 'damaged'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb35e28-7098-45a1-bb31-899fd8610880",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_binary_HBK, df_binary_SIZA, df_binary_MCC5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d994ee23-ed86-4e93-b379-c145716083cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_df = normalizeDataframe(combined_df, normalization_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d0ded1-6f65-4a37-bb3c-5a59f1d1964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_training_normalized, features_df_testing_normalized = train_test_split(\n",
    "    normalized_df, \n",
    "    test_size=0.2,    # e.g., 20% for testing\n",
    "    random_state=42   # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27e3ca-e055-4837-9084-3b94820aec39",
   "metadata": {},
   "source": [
    "# Experiment Setup (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3aea3-e600-4a35-92b7-ea5d9981ebe5",
   "metadata": {},
   "source": [
    "## Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4ff813-0eb2-45b8-a876-ec6929d1b040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f651c_row9_col1, #T_f651c_row16_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f651c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f651c_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_f651c_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f651c_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_f651c_row0_col1\" class=\"data row0 col1\" >5991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f651c_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_f651c_row1_col1\" class=\"data row1 col1\" >Label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f651c_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_f651c_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f651c_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_f651c_row3_col1\" class=\"data row3 col1\" >damaged: 0, healthy: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f651c_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_f651c_row4_col1\" class=\"data row4 col1\" >(246359, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f651c_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_f651c_row5_col1\" class=\"data row5 col1\" >(246359, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f651c_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_f651c_row6_col1\" class=\"data row6 col1\" >(172451, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f651c_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_f651c_row7_col1\" class=\"data row7 col1\" >(73908, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f651c_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_f651c_row8_col1\" class=\"data row8 col1\" >27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f651c_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_f651c_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f651c_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_f651c_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f651c_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_f651c_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f651c_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_f651c_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f651c_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_f651c_row13_col1\" class=\"data row13 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f651c_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_f651c_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f651c_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_f651c_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f651c_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_f651c_row16_col1\" class=\"data row16 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f651c_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_f651c_row17_col1\" class=\"data row17 col1\" >MlflowLogger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f651c_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_f651c_row18_col1\" class=\"data row18 col1\" >Best_Hyperparameters_z_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f651c_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f651c_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_f651c_row19_col1\" class=\"data row19 col1\" >817d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a47134e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    }
   ],
   "source": [
    "experiment = setup(features_df_training_normalized, target='Label', log_experiment = True, experiment_name = experiment_name, use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6300a-a54c-480c-8735-026fdbb0fd45",
   "metadata": {},
   "source": [
    "## Add aditional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb3d97b-cb84-42ed-b00b-ec01fcc7bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                                                          Log Loss\n",
       "Display Name                                                  Log Loss\n",
       "Score Function       <pycaret.internal.metrics.EncodedDecodedLabels...\n",
       "Scorer               make_scorer(log_loss, greater_is_better=False,...\n",
       "Target                                                      pred_proba\n",
       "Args                                                                {}\n",
       "Greater is Better                                                False\n",
       "Multiclass                                                        True\n",
       "Custom                                                            True\n",
       "Name: log_loss, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification metrics\n",
    "add_metric('balanced_acc', 'Balance Acc', balanced_accuracy_score, target='pred', greater_is_better=True)\n",
    "add_metric('hamming_loss', 'Hamming Loss', hamming_loss, target='pred', greater_is_better=False)\n",
    "add_metric('jaccard_score', 'Jaccard Score', jaccard_score, target='pred', greater_is_better=True)\n",
    "add_metric('log_loss', 'Log Loss', log_loss, target='pred_proba', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe742ef7-ec02-4e73-b2a5-f2b36eee0066",
   "metadata": {},
   "source": [
    "# Macro\n",
    "add_metric('precision_macro', 'Precision Macro', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_macro', 'Recall Macro', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_macro', 'F1 Macro', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='macro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "# Micro\n",
    "add_metric('precision_micro', 'Precision Micro', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_micro', 'Recall Micro', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_micro', 'F1 Micro', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='micro'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "# Weighted\n",
    "add_metric('precision_weighted', 'Precision Weighted', \n",
    "           lambda y, y_pred: precision_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('recall_weighted', 'Recall Weighted', \n",
    "           lambda y, y_pred: recall_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)\n",
    "\n",
    "add_metric('f1_weighted', 'F1 Weighted', \n",
    "           lambda y, y_pred: f1_score(y, y_pred, average='weighted'), \n",
    "           greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9faa1a3-cf7b-46ea-84b4-6b15167e1e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Display Name</th>\n",
       "      <th>Score Function</th>\n",
       "      <th>Scorer</th>\n",
       "      <th>Target</th>\n",
       "      <th>Args</th>\n",
       "      <th>Greater is Better</th>\n",
       "      <th>Multiclass</th>\n",
       "      <th>Custom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>&lt;function accuracy_score at 0x0000023A434F2B60&gt;</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>AUC</td>\n",
       "      <td>AUC</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(roc_auc_score, response_method=('d...</td>\n",
       "      <td>pred_proba</td>\n",
       "      <td>{'average': 'weighted', 'multi_class': 'ovr'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>Recall</td>\n",
       "      <td>Recall</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(recall_score, response_method='pre...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>Precision</td>\n",
       "      <td>Prec.</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(precision_score, response_method='...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>F1</td>\n",
       "      <td>F1</td>\n",
       "      <td>&lt;pycaret.internal.metrics.BinaryMulticlassScor...</td>\n",
       "      <td>make_scorer(f1_score, response_method='predict...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{'average': 'weighted'}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>Kappa</td>\n",
       "      <td>Kappa</td>\n",
       "      <td>&lt;function cohen_kappa_score at 0x0000023A434F2...</td>\n",
       "      <td>make_scorer(cohen_kappa_score, response_method...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcc</th>\n",
       "      <td>MCC</td>\n",
       "      <td>MCC</td>\n",
       "      <td>&lt;function matthews_corrcoef at 0x0000023A434F3...</td>\n",
       "      <td>make_scorer(matthews_corrcoef, response_method...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_acc</th>\n",
       "      <td>Balance Acc</td>\n",
       "      <td>Balance Acc</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(balanced_accuracy_score, response_...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamming_loss</th>\n",
       "      <td>Hamming Loss</td>\n",
       "      <td>Hamming Loss</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(hamming_loss, greater_is_better=Fa...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_score</th>\n",
       "      <td>Jaccard Score</td>\n",
       "      <td>Jaccard Score</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(jaccard_score, response_method='pr...</td>\n",
       "      <td>pred</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_loss</th>\n",
       "      <td>Log Loss</td>\n",
       "      <td>Log Loss</td>\n",
       "      <td>&lt;pycaret.internal.metrics.EncodedDecodedLabels...</td>\n",
       "      <td>make_scorer(log_loss, greater_is_better=False,...</td>\n",
       "      <td>pred_proba</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name   Display Name  \\\n",
       "ID                                            \n",
       "acc                 Accuracy       Accuracy   \n",
       "auc                      AUC            AUC   \n",
       "recall                Recall         Recall   \n",
       "precision          Precision          Prec.   \n",
       "f1                        F1             F1   \n",
       "kappa                  Kappa          Kappa   \n",
       "mcc                      MCC            MCC   \n",
       "balanced_acc     Balance Acc    Balance Acc   \n",
       "hamming_loss    Hamming Loss   Hamming Loss   \n",
       "jaccard_score  Jaccard Score  Jaccard Score   \n",
       "log_loss            Log Loss       Log Loss   \n",
       "\n",
       "                                                  Score Function  \\\n",
       "ID                                                                 \n",
       "acc              <function accuracy_score at 0x0000023A434F2B60>   \n",
       "auc            <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "recall         <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "precision      <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "f1             <pycaret.internal.metrics.BinaryMulticlassScor...   \n",
       "kappa          <function cohen_kappa_score at 0x0000023A434F2...   \n",
       "mcc            <function matthews_corrcoef at 0x0000023A434F3...   \n",
       "balanced_acc   <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "hamming_loss   <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "jaccard_score  <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "log_loss       <pycaret.internal.metrics.EncodedDecodedLabels...   \n",
       "\n",
       "                                                          Scorer      Target  \\\n",
       "ID                                                                             \n",
       "acc                                                     accuracy        pred   \n",
       "auc            make_scorer(roc_auc_score, response_method=('d...  pred_proba   \n",
       "recall         make_scorer(recall_score, response_method='pre...        pred   \n",
       "precision      make_scorer(precision_score, response_method='...        pred   \n",
       "f1             make_scorer(f1_score, response_method='predict...        pred   \n",
       "kappa          make_scorer(cohen_kappa_score, response_method...        pred   \n",
       "mcc            make_scorer(matthews_corrcoef, response_method...        pred   \n",
       "balanced_acc   make_scorer(balanced_accuracy_score, response_...        pred   \n",
       "hamming_loss   make_scorer(hamming_loss, greater_is_better=Fa...        pred   \n",
       "jaccard_score  make_scorer(jaccard_score, response_method='pr...        pred   \n",
       "log_loss       make_scorer(log_loss, greater_is_better=False,...  pred_proba   \n",
       "\n",
       "                                                        Args  \\\n",
       "ID                                                             \n",
       "acc                                                       {}   \n",
       "auc            {'average': 'weighted', 'multi_class': 'ovr'}   \n",
       "recall                               {'average': 'weighted'}   \n",
       "precision                            {'average': 'weighted'}   \n",
       "f1                                   {'average': 'weighted'}   \n",
       "kappa                                                     {}   \n",
       "mcc                                                       {}   \n",
       "balanced_acc                                              {}   \n",
       "hamming_loss                                              {}   \n",
       "jaccard_score                                             {}   \n",
       "log_loss                                                  {}   \n",
       "\n",
       "               Greater is Better  Multiclass  Custom  \n",
       "ID                                                    \n",
       "acc                         True        True   False  \n",
       "auc                         True        True   False  \n",
       "recall                      True        True   False  \n",
       "precision                   True        True   False  \n",
       "f1                          True        True   False  \n",
       "kappa                       True        True   False  \n",
       "mcc                         True        True   False  \n",
       "balanced_acc                True        True    True  \n",
       "hamming_loss               False        True    True  \n",
       "jaccard_score               True        True    True  \n",
       "log_loss                   False        True    True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics = get_metrics()\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a5e119-0bbb-4ee9-8da6-e10a63eaaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations_tuning = 10\n",
    "optimized_metric = 'F1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de3f05-6198-4ae2-adba-d294aee48b08",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0ce27b-40e1-468f-945e-ee949316a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_59237_row10_col0, #T_59237_row10_col1, #T_59237_row10_col2, #T_59237_row10_col3, #T_59237_row10_col4, #T_59237_row10_col5, #T_59237_row10_col6, #T_59237_row10_col7, #T_59237_row10_col8, #T_59237_row10_col9, #T_59237_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_59237\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_59237_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_59237_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_59237_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_59237_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_59237_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_59237_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_59237_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_59237_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_59237_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_59237_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_59237_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_59237_row0_col0\" class=\"data row0 col0\" >0.9534</td>\n",
       "      <td id=\"T_59237_row0_col1\" class=\"data row0 col1\" >0.9873</td>\n",
       "      <td id=\"T_59237_row0_col2\" class=\"data row0 col2\" >0.9534</td>\n",
       "      <td id=\"T_59237_row0_col3\" class=\"data row0 col3\" >0.9543</td>\n",
       "      <td id=\"T_59237_row0_col4\" class=\"data row0 col4\" >0.9518</td>\n",
       "      <td id=\"T_59237_row0_col5\" class=\"data row0 col5\" >0.8582</td>\n",
       "      <td id=\"T_59237_row0_col6\" class=\"data row0 col6\" >0.8637</td>\n",
       "      <td id=\"T_59237_row0_col7\" class=\"data row0 col7\" >0.9042</td>\n",
       "      <td id=\"T_59237_row0_col8\" class=\"data row0 col8\" >0.0466</td>\n",
       "      <td id=\"T_59237_row0_col9\" class=\"data row0 col9\" >0.7975</td>\n",
       "      <td id=\"T_59237_row0_col10\" class=\"data row0 col10\" >0.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_59237_row1_col0\" class=\"data row1 col0\" >0.9525</td>\n",
       "      <td id=\"T_59237_row1_col1\" class=\"data row1 col1\" >0.9865</td>\n",
       "      <td id=\"T_59237_row1_col2\" class=\"data row1 col2\" >0.9525</td>\n",
       "      <td id=\"T_59237_row1_col3\" class=\"data row1 col3\" >0.9537</td>\n",
       "      <td id=\"T_59237_row1_col4\" class=\"data row1 col4\" >0.9508</td>\n",
       "      <td id=\"T_59237_row1_col5\" class=\"data row1 col5\" >0.8550</td>\n",
       "      <td id=\"T_59237_row1_col6\" class=\"data row1 col6\" >0.8612</td>\n",
       "      <td id=\"T_59237_row1_col7\" class=\"data row1 col7\" >0.9011</td>\n",
       "      <td id=\"T_59237_row1_col8\" class=\"data row1 col8\" >0.0475</td>\n",
       "      <td id=\"T_59237_row1_col9\" class=\"data row1 col9\" >0.7930</td>\n",
       "      <td id=\"T_59237_row1_col10\" class=\"data row1 col10\" >0.1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_59237_row2_col0\" class=\"data row2 col0\" >0.9490</td>\n",
       "      <td id=\"T_59237_row2_col1\" class=\"data row2 col1\" >0.9867</td>\n",
       "      <td id=\"T_59237_row2_col2\" class=\"data row2 col2\" >0.9490</td>\n",
       "      <td id=\"T_59237_row2_col3\" class=\"data row2 col3\" >0.9502</td>\n",
       "      <td id=\"T_59237_row2_col4\" class=\"data row2 col4\" >0.9471</td>\n",
       "      <td id=\"T_59237_row2_col5\" class=\"data row2 col5\" >0.8439</td>\n",
       "      <td id=\"T_59237_row2_col6\" class=\"data row2 col6\" >0.8506</td>\n",
       "      <td id=\"T_59237_row2_col7\" class=\"data row2 col7\" >0.8947</td>\n",
       "      <td id=\"T_59237_row2_col8\" class=\"data row2 col8\" >0.0510</td>\n",
       "      <td id=\"T_59237_row2_col9\" class=\"data row2 col9\" >0.7786</td>\n",
       "      <td id=\"T_59237_row2_col10\" class=\"data row2 col10\" >0.1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_59237_row3_col0\" class=\"data row3 col0\" >0.9489</td>\n",
       "      <td id=\"T_59237_row3_col1\" class=\"data row3 col1\" >0.9852</td>\n",
       "      <td id=\"T_59237_row3_col2\" class=\"data row3 col2\" >0.9489</td>\n",
       "      <td id=\"T_59237_row3_col3\" class=\"data row3 col3\" >0.9498</td>\n",
       "      <td id=\"T_59237_row3_col4\" class=\"data row3 col4\" >0.9471</td>\n",
       "      <td id=\"T_59237_row3_col5\" class=\"data row3 col5\" >0.8441</td>\n",
       "      <td id=\"T_59237_row3_col6\" class=\"data row3 col6\" >0.8501</td>\n",
       "      <td id=\"T_59237_row3_col7\" class=\"data row3 col7\" >0.8961</td>\n",
       "      <td id=\"T_59237_row3_col8\" class=\"data row3 col8\" >0.0511</td>\n",
       "      <td id=\"T_59237_row3_col9\" class=\"data row3 col9\" >0.7791</td>\n",
       "      <td id=\"T_59237_row3_col10\" class=\"data row3 col10\" >0.1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_59237_row4_col0\" class=\"data row4 col0\" >0.9537</td>\n",
       "      <td id=\"T_59237_row4_col1\" class=\"data row4 col1\" >0.9868</td>\n",
       "      <td id=\"T_59237_row4_col2\" class=\"data row4 col2\" >0.9537</td>\n",
       "      <td id=\"T_59237_row4_col3\" class=\"data row4 col3\" >0.9548</td>\n",
       "      <td id=\"T_59237_row4_col4\" class=\"data row4 col4\" >0.9521</td>\n",
       "      <td id=\"T_59237_row4_col5\" class=\"data row4 col5\" >0.8588</td>\n",
       "      <td id=\"T_59237_row4_col6\" class=\"data row4 col6\" >0.8646</td>\n",
       "      <td id=\"T_59237_row4_col7\" class=\"data row4 col7\" >0.9037</td>\n",
       "      <td id=\"T_59237_row4_col8\" class=\"data row4 col8\" >0.0463</td>\n",
       "      <td id=\"T_59237_row4_col9\" class=\"data row4 col9\" >0.7981</td>\n",
       "      <td id=\"T_59237_row4_col10\" class=\"data row4 col10\" >0.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_59237_row5_col0\" class=\"data row5 col0\" >0.9533</td>\n",
       "      <td id=\"T_59237_row5_col1\" class=\"data row5 col1\" >0.9871</td>\n",
       "      <td id=\"T_59237_row5_col2\" class=\"data row5 col2\" >0.9533</td>\n",
       "      <td id=\"T_59237_row5_col3\" class=\"data row5 col3\" >0.9541</td>\n",
       "      <td id=\"T_59237_row5_col4\" class=\"data row5 col4\" >0.9519</td>\n",
       "      <td id=\"T_59237_row5_col5\" class=\"data row5 col5\" >0.8584</td>\n",
       "      <td id=\"T_59237_row5_col6\" class=\"data row5 col6\" >0.8634</td>\n",
       "      <td id=\"T_59237_row5_col7\" class=\"data row5 col7\" >0.9054</td>\n",
       "      <td id=\"T_59237_row5_col8\" class=\"data row5 col8\" >0.0467</td>\n",
       "      <td id=\"T_59237_row5_col9\" class=\"data row5 col9\" >0.7979</td>\n",
       "      <td id=\"T_59237_row5_col10\" class=\"data row5 col10\" >0.1121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_59237_row6_col0\" class=\"data row6 col0\" >0.9534</td>\n",
       "      <td id=\"T_59237_row6_col1\" class=\"data row6 col1\" >0.9867</td>\n",
       "      <td id=\"T_59237_row6_col2\" class=\"data row6 col2\" >0.9534</td>\n",
       "      <td id=\"T_59237_row6_col3\" class=\"data row6 col3\" >0.9546</td>\n",
       "      <td id=\"T_59237_row6_col4\" class=\"data row6 col4\" >0.9518</td>\n",
       "      <td id=\"T_59237_row6_col5\" class=\"data row6 col5\" >0.8578</td>\n",
       "      <td id=\"T_59237_row6_col6\" class=\"data row6 col6\" >0.8638</td>\n",
       "      <td id=\"T_59237_row6_col7\" class=\"data row6 col7\" >0.9028</td>\n",
       "      <td id=\"T_59237_row6_col8\" class=\"data row6 col8\" >0.0466</td>\n",
       "      <td id=\"T_59237_row6_col9\" class=\"data row6 col9\" >0.7967</td>\n",
       "      <td id=\"T_59237_row6_col10\" class=\"data row6 col10\" >0.1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_59237_row7_col0\" class=\"data row7 col0\" >0.9527</td>\n",
       "      <td id=\"T_59237_row7_col1\" class=\"data row7 col1\" >0.9871</td>\n",
       "      <td id=\"T_59237_row7_col2\" class=\"data row7 col2\" >0.9527</td>\n",
       "      <td id=\"T_59237_row7_col3\" class=\"data row7 col3\" >0.9538</td>\n",
       "      <td id=\"T_59237_row7_col4\" class=\"data row7 col4\" >0.9511</td>\n",
       "      <td id=\"T_59237_row7_col5\" class=\"data row7 col5\" >0.8558</td>\n",
       "      <td id=\"T_59237_row7_col6\" class=\"data row7 col6\" >0.8617</td>\n",
       "      <td id=\"T_59237_row7_col7\" class=\"data row7 col7\" >0.9021</td>\n",
       "      <td id=\"T_59237_row7_col8\" class=\"data row7 col8\" >0.0473</td>\n",
       "      <td id=\"T_59237_row7_col9\" class=\"data row7 col9\" >0.7941</td>\n",
       "      <td id=\"T_59237_row7_col10\" class=\"data row7 col10\" >0.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_59237_row8_col0\" class=\"data row8 col0\" >0.9507</td>\n",
       "      <td id=\"T_59237_row8_col1\" class=\"data row8 col1\" >0.9860</td>\n",
       "      <td id=\"T_59237_row8_col2\" class=\"data row8 col2\" >0.9507</td>\n",
       "      <td id=\"T_59237_row8_col3\" class=\"data row8 col3\" >0.9515</td>\n",
       "      <td id=\"T_59237_row8_col4\" class=\"data row8 col4\" >0.9490</td>\n",
       "      <td id=\"T_59237_row8_col5\" class=\"data row8 col5\" >0.8497</td>\n",
       "      <td id=\"T_59237_row8_col6\" class=\"data row8 col6\" >0.8554</td>\n",
       "      <td id=\"T_59237_row8_col7\" class=\"data row8 col7\" >0.8994</td>\n",
       "      <td id=\"T_59237_row8_col8\" class=\"data row8 col8\" >0.0493</td>\n",
       "      <td id=\"T_59237_row8_col9\" class=\"data row8 col9\" >0.7864</td>\n",
       "      <td id=\"T_59237_row8_col10\" class=\"data row8 col10\" >0.1171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_59237_row9_col0\" class=\"data row9 col0\" >0.9513</td>\n",
       "      <td id=\"T_59237_row9_col1\" class=\"data row9 col1\" >0.9869</td>\n",
       "      <td id=\"T_59237_row9_col2\" class=\"data row9 col2\" >0.9513</td>\n",
       "      <td id=\"T_59237_row9_col3\" class=\"data row9 col3\" >0.9524</td>\n",
       "      <td id=\"T_59237_row9_col4\" class=\"data row9 col4\" >0.9496</td>\n",
       "      <td id=\"T_59237_row9_col5\" class=\"data row9 col5\" >0.8514</td>\n",
       "      <td id=\"T_59237_row9_col6\" class=\"data row9 col6\" >0.8575</td>\n",
       "      <td id=\"T_59237_row9_col7\" class=\"data row9 col7\" >0.8996</td>\n",
       "      <td id=\"T_59237_row9_col8\" class=\"data row9 col8\" >0.0487</td>\n",
       "      <td id=\"T_59237_row9_col9\" class=\"data row9 col9\" >0.7884</td>\n",
       "      <td id=\"T_59237_row9_col10\" class=\"data row9 col10\" >0.1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_59237_row10_col0\" class=\"data row10 col0\" >0.9519</td>\n",
       "      <td id=\"T_59237_row10_col1\" class=\"data row10 col1\" >0.9866</td>\n",
       "      <td id=\"T_59237_row10_col2\" class=\"data row10 col2\" >0.9519</td>\n",
       "      <td id=\"T_59237_row10_col3\" class=\"data row10 col3\" >0.9529</td>\n",
       "      <td id=\"T_59237_row10_col4\" class=\"data row10 col4\" >0.9502</td>\n",
       "      <td id=\"T_59237_row10_col5\" class=\"data row10 col5\" >0.8533</td>\n",
       "      <td id=\"T_59237_row10_col6\" class=\"data row10 col6\" >0.8592</td>\n",
       "      <td id=\"T_59237_row10_col7\" class=\"data row10 col7\" >0.9009</td>\n",
       "      <td id=\"T_59237_row10_col8\" class=\"data row10 col8\" >0.0481</td>\n",
       "      <td id=\"T_59237_row10_col9\" class=\"data row10 col9\" >0.7910</td>\n",
       "      <td id=\"T_59237_row10_col10\" class=\"data row10 col10\" >0.1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59237_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_59237_row11_col0\" class=\"data row11 col0\" >0.0017</td>\n",
       "      <td id=\"T_59237_row11_col1\" class=\"data row11 col1\" >0.0006</td>\n",
       "      <td id=\"T_59237_row11_col2\" class=\"data row11 col2\" >0.0017</td>\n",
       "      <td id=\"T_59237_row11_col3\" class=\"data row11 col3\" >0.0017</td>\n",
       "      <td id=\"T_59237_row11_col4\" class=\"data row11 col4\" >0.0018</td>\n",
       "      <td id=\"T_59237_row11_col5\" class=\"data row11 col5\" >0.0055</td>\n",
       "      <td id=\"T_59237_row11_col6\" class=\"data row11 col6\" >0.0052</td>\n",
       "      <td id=\"T_59237_row11_col7\" class=\"data row11 col7\" >0.0033</td>\n",
       "      <td id=\"T_59237_row11_col8\" class=\"data row11 col8\" >0.0017</td>\n",
       "      <td id=\"T_59237_row11_col9\" class=\"data row11 col9\" >0.0071</td>\n",
       "      <td id=\"T_59237_row11_col10\" class=\"data row11 col10\" >0.0023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a4a27b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 16:59:36 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "lightgbm = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9639f68f-46f5-4854-b520-3fc11e40f3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7137f_row10_col0, #T_7137f_row10_col1, #T_7137f_row10_col2, #T_7137f_row10_col3, #T_7137f_row10_col4, #T_7137f_row10_col5, #T_7137f_row10_col6, #T_7137f_row10_col7, #T_7137f_row10_col8, #T_7137f_row10_col9, #T_7137f_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7137f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7137f_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_7137f_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_7137f_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_7137f_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_7137f_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_7137f_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_7137f_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_7137f_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_7137f_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_7137f_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_7137f_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7137f_row0_col0\" class=\"data row0 col0\" >0.9627</td>\n",
       "      <td id=\"T_7137f_row0_col1\" class=\"data row0 col1\" >0.9904</td>\n",
       "      <td id=\"T_7137f_row0_col2\" class=\"data row0 col2\" >0.9627</td>\n",
       "      <td id=\"T_7137f_row0_col3\" class=\"data row0 col3\" >0.9623</td>\n",
       "      <td id=\"T_7137f_row0_col4\" class=\"data row0 col4\" >0.9622</td>\n",
       "      <td id=\"T_7137f_row0_col5\" class=\"data row0 col5\" >0.8906</td>\n",
       "      <td id=\"T_7137f_row0_col6\" class=\"data row0 col6\" >0.8914</td>\n",
       "      <td id=\"T_7137f_row0_col7\" class=\"data row0 col7\" >0.9353</td>\n",
       "      <td id=\"T_7137f_row0_col8\" class=\"data row0 col8\" >0.0373</td>\n",
       "      <td id=\"T_7137f_row0_col9\" class=\"data row0 col9\" >0.8424</td>\n",
       "      <td id=\"T_7137f_row0_col10\" class=\"data row0 col10\" >0.0958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7137f_row1_col0\" class=\"data row1 col0\" >0.9627</td>\n",
       "      <td id=\"T_7137f_row1_col1\" class=\"data row1 col1\" >0.9900</td>\n",
       "      <td id=\"T_7137f_row1_col2\" class=\"data row1 col2\" >0.9627</td>\n",
       "      <td id=\"T_7137f_row1_col3\" class=\"data row1 col3\" >0.9623</td>\n",
       "      <td id=\"T_7137f_row1_col4\" class=\"data row1 col4\" >0.9622</td>\n",
       "      <td id=\"T_7137f_row1_col5\" class=\"data row1 col5\" >0.8906</td>\n",
       "      <td id=\"T_7137f_row1_col6\" class=\"data row1 col6\" >0.8914</td>\n",
       "      <td id=\"T_7137f_row1_col7\" class=\"data row1 col7\" >0.9355</td>\n",
       "      <td id=\"T_7137f_row1_col8\" class=\"data row1 col8\" >0.0373</td>\n",
       "      <td id=\"T_7137f_row1_col9\" class=\"data row1 col9\" >0.8424</td>\n",
       "      <td id=\"T_7137f_row1_col10\" class=\"data row1 col10\" >0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7137f_row2_col0\" class=\"data row2 col0\" >0.9632</td>\n",
       "      <td id=\"T_7137f_row2_col1\" class=\"data row2 col1\" >0.9898</td>\n",
       "      <td id=\"T_7137f_row2_col2\" class=\"data row2 col2\" >0.9632</td>\n",
       "      <td id=\"T_7137f_row2_col3\" class=\"data row2 col3\" >0.9629</td>\n",
       "      <td id=\"T_7137f_row2_col4\" class=\"data row2 col4\" >0.9627</td>\n",
       "      <td id=\"T_7137f_row2_col5\" class=\"data row2 col5\" >0.8919</td>\n",
       "      <td id=\"T_7137f_row2_col6\" class=\"data row2 col6\" >0.8928</td>\n",
       "      <td id=\"T_7137f_row2_col7\" class=\"data row2 col7\" >0.9353</td>\n",
       "      <td id=\"T_7137f_row2_col8\" class=\"data row2 col8\" >0.0368</td>\n",
       "      <td id=\"T_7137f_row2_col9\" class=\"data row2 col9\" >0.8441</td>\n",
       "      <td id=\"T_7137f_row2_col10\" class=\"data row2 col10\" >0.1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7137f_row3_col0\" class=\"data row3 col0\" >0.9621</td>\n",
       "      <td id=\"T_7137f_row3_col1\" class=\"data row3 col1\" >0.9900</td>\n",
       "      <td id=\"T_7137f_row3_col2\" class=\"data row3 col2\" >0.9621</td>\n",
       "      <td id=\"T_7137f_row3_col3\" class=\"data row3 col3\" >0.9618</td>\n",
       "      <td id=\"T_7137f_row3_col4\" class=\"data row3 col4\" >0.9617</td>\n",
       "      <td id=\"T_7137f_row3_col5\" class=\"data row3 col5\" >0.8892</td>\n",
       "      <td id=\"T_7137f_row3_col6\" class=\"data row3 col6\" >0.8899</td>\n",
       "      <td id=\"T_7137f_row3_col7\" class=\"data row3 col7\" >0.9351</td>\n",
       "      <td id=\"T_7137f_row3_col8\" class=\"data row3 col8\" >0.0379</td>\n",
       "      <td id=\"T_7137f_row3_col9\" class=\"data row3 col9\" >0.8405</td>\n",
       "      <td id=\"T_7137f_row3_col10\" class=\"data row3 col10\" >0.1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7137f_row4_col0\" class=\"data row4 col0\" >0.9638</td>\n",
       "      <td id=\"T_7137f_row4_col1\" class=\"data row4 col1\" >0.9894</td>\n",
       "      <td id=\"T_7137f_row4_col2\" class=\"data row4 col2\" >0.9638</td>\n",
       "      <td id=\"T_7137f_row4_col3\" class=\"data row4 col3\" >0.9635</td>\n",
       "      <td id=\"T_7137f_row4_col4\" class=\"data row4 col4\" >0.9634</td>\n",
       "      <td id=\"T_7137f_row4_col5\" class=\"data row4 col5\" >0.8940</td>\n",
       "      <td id=\"T_7137f_row4_col6\" class=\"data row4 col6\" >0.8946</td>\n",
       "      <td id=\"T_7137f_row4_col7\" class=\"data row4 col7\" >0.9378</td>\n",
       "      <td id=\"T_7137f_row4_col8\" class=\"data row4 col8\" >0.0362</td>\n",
       "      <td id=\"T_7137f_row4_col9\" class=\"data row4 col9\" >0.8470</td>\n",
       "      <td id=\"T_7137f_row4_col10\" class=\"data row4 col10\" >0.1068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7137f_row5_col0\" class=\"data row5 col0\" >0.9654</td>\n",
       "      <td id=\"T_7137f_row5_col1\" class=\"data row5 col1\" >0.9907</td>\n",
       "      <td id=\"T_7137f_row5_col2\" class=\"data row5 col2\" >0.9654</td>\n",
       "      <td id=\"T_7137f_row5_col3\" class=\"data row5 col3\" >0.9652</td>\n",
       "      <td id=\"T_7137f_row5_col4\" class=\"data row5 col4\" >0.9651</td>\n",
       "      <td id=\"T_7137f_row5_col5\" class=\"data row5 col5\" >0.8990</td>\n",
       "      <td id=\"T_7137f_row5_col6\" class=\"data row5 col6\" >0.8996</td>\n",
       "      <td id=\"T_7137f_row5_col7\" class=\"data row5 col7\" >0.9406</td>\n",
       "      <td id=\"T_7137f_row5_col8\" class=\"data row5 col8\" >0.0346</td>\n",
       "      <td id=\"T_7137f_row5_col9\" class=\"data row5 col9\" >0.8537</td>\n",
       "      <td id=\"T_7137f_row5_col10\" class=\"data row5 col10\" >0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7137f_row6_col0\" class=\"data row6 col0\" >0.9646</td>\n",
       "      <td id=\"T_7137f_row6_col1\" class=\"data row6 col1\" >0.9913</td>\n",
       "      <td id=\"T_7137f_row6_col2\" class=\"data row6 col2\" >0.9646</td>\n",
       "      <td id=\"T_7137f_row6_col3\" class=\"data row6 col3\" >0.9643</td>\n",
       "      <td id=\"T_7137f_row6_col4\" class=\"data row6 col4\" >0.9642</td>\n",
       "      <td id=\"T_7137f_row6_col5\" class=\"data row6 col5\" >0.8962</td>\n",
       "      <td id=\"T_7137f_row6_col6\" class=\"data row6 col6\" >0.8970</td>\n",
       "      <td id=\"T_7137f_row6_col7\" class=\"data row6 col7\" >0.9383</td>\n",
       "      <td id=\"T_7137f_row6_col8\" class=\"data row6 col8\" >0.0354</td>\n",
       "      <td id=\"T_7137f_row6_col9\" class=\"data row6 col9\" >0.8499</td>\n",
       "      <td id=\"T_7137f_row6_col10\" class=\"data row6 col10\" >0.0960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7137f_row7_col0\" class=\"data row7 col0\" >0.9622</td>\n",
       "      <td id=\"T_7137f_row7_col1\" class=\"data row7 col1\" >0.9885</td>\n",
       "      <td id=\"T_7137f_row7_col2\" class=\"data row7 col2\" >0.9622</td>\n",
       "      <td id=\"T_7137f_row7_col3\" class=\"data row7 col3\" >0.9619</td>\n",
       "      <td id=\"T_7137f_row7_col4\" class=\"data row7 col4\" >0.9619</td>\n",
       "      <td id=\"T_7137f_row7_col5\" class=\"data row7 col5\" >0.8898</td>\n",
       "      <td id=\"T_7137f_row7_col6\" class=\"data row7 col6\" >0.8903</td>\n",
       "      <td id=\"T_7137f_row7_col7\" class=\"data row7 col7\" >0.9366</td>\n",
       "      <td id=\"T_7137f_row7_col8\" class=\"data row7 col8\" >0.0378</td>\n",
       "      <td id=\"T_7137f_row7_col9\" class=\"data row7 col9\" >0.8416</td>\n",
       "      <td id=\"T_7137f_row7_col10\" class=\"data row7 col10\" >0.1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7137f_row8_col0\" class=\"data row8 col0\" >0.9617</td>\n",
       "      <td id=\"T_7137f_row8_col1\" class=\"data row8 col1\" >0.9901</td>\n",
       "      <td id=\"T_7137f_row8_col2\" class=\"data row8 col2\" >0.9617</td>\n",
       "      <td id=\"T_7137f_row8_col3\" class=\"data row8 col3\" >0.9613</td>\n",
       "      <td id=\"T_7137f_row8_col4\" class=\"data row8 col4\" >0.9613</td>\n",
       "      <td id=\"T_7137f_row8_col5\" class=\"data row8 col5\" >0.8880</td>\n",
       "      <td id=\"T_7137f_row8_col6\" class=\"data row8 col6\" >0.8886</td>\n",
       "      <td id=\"T_7137f_row8_col7\" class=\"data row8 col7\" >0.9354</td>\n",
       "      <td id=\"T_7137f_row8_col8\" class=\"data row8 col8\" >0.0383</td>\n",
       "      <td id=\"T_7137f_row8_col9\" class=\"data row8 col9\" >0.8392</td>\n",
       "      <td id=\"T_7137f_row8_col10\" class=\"data row8 col10\" >0.1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7137f_row9_col0\" class=\"data row9 col0\" >0.9611</td>\n",
       "      <td id=\"T_7137f_row9_col1\" class=\"data row9 col1\" >0.9898</td>\n",
       "      <td id=\"T_7137f_row9_col2\" class=\"data row9 col2\" >0.9611</td>\n",
       "      <td id=\"T_7137f_row9_col3\" class=\"data row9 col3\" >0.9608</td>\n",
       "      <td id=\"T_7137f_row9_col4\" class=\"data row9 col4\" >0.9607</td>\n",
       "      <td id=\"T_7137f_row9_col5\" class=\"data row9 col5\" >0.8861</td>\n",
       "      <td id=\"T_7137f_row9_col6\" class=\"data row9 col6\" >0.8869</td>\n",
       "      <td id=\"T_7137f_row9_col7\" class=\"data row9 col7\" >0.9330</td>\n",
       "      <td id=\"T_7137f_row9_col8\" class=\"data row9 col8\" >0.0389</td>\n",
       "      <td id=\"T_7137f_row9_col9\" class=\"data row9 col9\" >0.8364</td>\n",
       "      <td id=\"T_7137f_row9_col10\" class=\"data row9 col10\" >0.1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_7137f_row10_col0\" class=\"data row10 col0\" >0.9629</td>\n",
       "      <td id=\"T_7137f_row10_col1\" class=\"data row10 col1\" >0.9900</td>\n",
       "      <td id=\"T_7137f_row10_col2\" class=\"data row10 col2\" >0.9629</td>\n",
       "      <td id=\"T_7137f_row10_col3\" class=\"data row10 col3\" >0.9626</td>\n",
       "      <td id=\"T_7137f_row10_col4\" class=\"data row10 col4\" >0.9625</td>\n",
       "      <td id=\"T_7137f_row10_col5\" class=\"data row10 col5\" >0.8915</td>\n",
       "      <td id=\"T_7137f_row10_col6\" class=\"data row10 col6\" >0.8923</td>\n",
       "      <td id=\"T_7137f_row10_col7\" class=\"data row10 col7\" >0.9363</td>\n",
       "      <td id=\"T_7137f_row10_col8\" class=\"data row10 col8\" >0.0371</td>\n",
       "      <td id=\"T_7137f_row10_col9\" class=\"data row10 col9\" >0.8437</td>\n",
       "      <td id=\"T_7137f_row10_col10\" class=\"data row10 col10\" >0.1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7137f_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_7137f_row11_col0\" class=\"data row11 col0\" >0.0013</td>\n",
       "      <td id=\"T_7137f_row11_col1\" class=\"data row11 col1\" >0.0007</td>\n",
       "      <td id=\"T_7137f_row11_col2\" class=\"data row11 col2\" >0.0013</td>\n",
       "      <td id=\"T_7137f_row11_col3\" class=\"data row11 col3\" >0.0013</td>\n",
       "      <td id=\"T_7137f_row11_col4\" class=\"data row11 col4\" >0.0013</td>\n",
       "      <td id=\"T_7137f_row11_col5\" class=\"data row11 col5\" >0.0037</td>\n",
       "      <td id=\"T_7137f_row11_col6\" class=\"data row11 col6\" >0.0037</td>\n",
       "      <td id=\"T_7137f_row11_col7\" class=\"data row11 col7\" >0.0020</td>\n",
       "      <td id=\"T_7137f_row11_col8\" class=\"data row11 col8\" >0.0013</td>\n",
       "      <td id=\"T_7137f_row11_col9\" class=\"data row11 col9\" >0.0049</td>\n",
       "      <td id=\"T_7137f_row11_col10\" class=\"data row11 col10\" >0.0223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a47102c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.010640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007701 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007656 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007601 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007770 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007489 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007489 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007436 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007863 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007479 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007443 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007509 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007413 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007797 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.009162 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007513 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007490 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007658 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007799 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007848 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007539 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008048 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007935 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007552 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008475 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007736 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007655 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007384 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007552 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007826 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008039 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.009034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.009204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.009137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007913 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.009154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008052 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007754 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007544 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007474 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.010799 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007539 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.009094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.009282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007512 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008046 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007801 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007647 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007513 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007415 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.067150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007481 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.012469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007652 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007658 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007322 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007889 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007442 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008131 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.016559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007797 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008656 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.009324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.009095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007925 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007828 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007544 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007650 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007631 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007924 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007647 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008183 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007479 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007474 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007754 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.066929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.008062 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.008169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.92 MB) transferred to GPU in 0.007542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.93 MB) transferred to GPU in 0.007744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.577751533396215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.577751533396215\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9469401484093165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9469401484093165\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007889 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008521 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.47249560494289544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.47249560494289544\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8507583632933011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8507583632933011\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005745 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005990 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005375 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006162 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005652 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005768 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.006403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005411 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005419 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005647 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005813 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005913 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005630 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005770 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005666 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005621 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.006039 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005699 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.006397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.04 MB) transferred to GPU in 0.005369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005828 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005322 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.02 MB) transferred to GPU in 0.005358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.03 MB) transferred to GPU in 0.005831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5106996309955579, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5106996309955579\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.488806196154268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.488806196154268\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009074 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008518 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006209 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006914 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006244 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007017 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007045 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.007034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.85 MB) transferred to GPU in 0.006269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.005991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.84 MB) transferred to GPU in 0.006651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008114 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.059608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009312 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008291 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007829 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007799 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007867 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007857 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007949 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007864 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009333 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009958 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.010992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007658 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007973 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.010038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007481 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007861 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.065577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007051 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008296 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008329 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007826 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007983 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008310 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007745 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009850 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007600 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007539 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007911 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.010093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008616 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.061820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007812 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007985 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007321 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007489 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007935 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.010091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007523 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.019555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007748 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008280 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008281 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.010260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007632 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007459 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008094 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.008261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.007987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.007994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.008337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.77 MB) transferred to GPU in 0.009269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.78 MB) transferred to GPU in 0.009634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344721685501781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344721685501781\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9113237491700239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9113237491700239\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.016486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007650 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007863 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.010285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007413 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.064096 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007928 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.060362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008251 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.007903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.013398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009173 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009333 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.009887 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.063615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.016978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.008396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009317 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.009790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.062785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.066966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007854 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.012082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007833 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.008492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.010436 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008799 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007443 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.064478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008419 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.008203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007889 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.010043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008011 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007810 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.063738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007802 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.064613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007937 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.067742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.008005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009494 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009864 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.009926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008654 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.016354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007847 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.060895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007752 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008417 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.007947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.009882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.063355 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008030 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009359 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.010033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008504 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.063226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008544 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.007896 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.010099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009833 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.010072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008302 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.062485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007538 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007809 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.016836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007973 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.064496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.008008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.007992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.87 MB) transferred to GPU in 0.066932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.008298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.007966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.85 MB) transferred to GPU in 0.009982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.86 MB) transferred to GPU in 0.009630 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6414349827959188, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6414349827959188\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9309374569724495, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9309374569724495\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008601 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008468 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008115 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008117 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007699 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008183 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008017 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008275 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007988 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008113 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007943 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008234 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008404 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007894 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007799 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008044 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007813 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008396 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007719 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.009037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008110 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008136 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007785 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.008335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007946 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007977 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.008089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007788 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.04 MB) transferred to GPU in 0.007692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.03 MB) transferred to GPU in 0.007629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.689051528086398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.689051528086398\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9737680050960643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9737680050960643\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006263 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006647 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006205 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006051 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006669 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008275 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007442 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009962 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009248 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009287 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008754 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009632 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009490 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007432 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006589 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006210 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006882 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007078 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008931 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009703 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009518 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009314 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006467 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.015197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006209 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.038816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006131 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.014871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006275 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006768 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007944 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008883 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009521 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009518 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006456 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006651 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007142 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006445 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008725 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009205 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009047 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006626 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006305 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007789 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006059 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006594 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006896 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009397 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008443 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009473 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008813 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009713 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009224 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010158 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008081 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006546 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.066001 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006177 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006361 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006097 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007107 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006138 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007896 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007758 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008788 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.010488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009105 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008788 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009266 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008944 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009544 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008478 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006959 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006049 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006197 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006725 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.064586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006531 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.015139 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007797 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006508 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006940 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008246 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007533 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008304 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008828 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009700 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008865 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006340 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006736 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.011114 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.016374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006157 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.014757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006682 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.007633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007819 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008453 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008976 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009284 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009046 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009815 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009545 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006451 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006055 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006567 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.012592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009631 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.010552 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008707 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006970 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006749 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.005878 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006781 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.064035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006529 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.060252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.006530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.012120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.006893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.005961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.006930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.007548 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.008255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.007357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008074 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009119 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.009891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.008733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.010077 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.46 MB) transferred to GPU in 0.009457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.48 MB) transferred to GPU in 0.008826 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.47 MB) transferred to GPU in 0.009268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9302861600263775, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9302861600263775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5961499044456269, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5961499044456269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007405 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007159 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007388 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007048 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007322 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006640 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007299 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007404 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007009 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007088 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006906 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006978 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007110 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008124 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007316 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006783 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006345 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006748 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006875 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007176 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007221 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007598 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007021 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007155 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007205 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007232 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006593 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006376 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006777 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007130 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006711 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007684 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006744 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006545 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007264 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007530 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006556 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007343 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006935 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007180 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007403 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007173 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007204 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006658 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006761 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006887 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007098 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007304 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006821 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007239 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007300 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007458 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006540 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006937 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006769 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007399 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007165 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007144 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006625 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007086 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007063 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007069 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006655 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007073 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006922 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006732 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007649 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006990 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007863 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007048 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006885 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007361 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007172 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006826 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006576 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007301 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006866 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006939 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006630 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007443 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007008 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006420 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.009148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007165 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007386 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006786 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006912 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007081 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007277 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006252 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007371 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006567 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006791 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007189 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007126 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006856 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.008465 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007018 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007028 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.007255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007491 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006601 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.007309 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.49 MB) transferred to GPU in 0.006660 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (3.48 MB) transferred to GPU in 0.006968 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4251429559072824, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4251429559072824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8403875377305812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8403875377305812\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155205, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225315 -> initscore=-1.234958\n",
      "[LightGBM] [Info] Start training from score -1.234958\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006918 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006797 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.005979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007131 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006250 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006852 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.007236 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.007025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006806 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007208 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006722 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.007134 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.008503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007079 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006631 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006513 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007196 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006364 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006090 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007076 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006702 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006242 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.062688 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006594 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006861 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006957 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007426 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006965 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.008415 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006661 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006035 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007289 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006748 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007215 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007240 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006768 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007133 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006838 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007368 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.008473 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.010294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007100 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.014951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006596 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006944 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.005951 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006583 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007104 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007000 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006681 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006621 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006524 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006569 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007122 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006608 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.012352 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.008763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34971, number of negative: 120235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008460 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225320 -> initscore=-1.234929\n",
      "[LightGBM] [Info] Start training from score -1.234929\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007108 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006784 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006888 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006486 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007185 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006902 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.005875 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007190 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.062065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006840 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006725 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006862 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007425 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006807 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007304 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008212 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006639 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006449 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006538 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.065805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006175 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006701 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006844 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006873 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006941 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007282 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006751 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007209 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006561 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.008580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.008667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.008016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006823 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006813 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007382 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006618 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006361 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006745 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006476 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006960 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006743 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007183 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006943 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006956 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.008466 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.009182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.015238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006517 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006920 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007021 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006247 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006381 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006652 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006793 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006828 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007313 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006231 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.007034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.008587 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007857 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007139 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006663 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006775 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006461 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.015016 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007042 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006731 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006972 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.016782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006686 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.007033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006843 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.014923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007318 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006790 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006764 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006887 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006558 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 34970, number of negative: 120236\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 155206, number of used features: 27\n",
      "[LightGBM] [Info] Using GPU Device: Quadro P620, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (4.14 MB) transferred to GPU in 0.007987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.225313 -> initscore=-1.234966\n",
      "[LightGBM] [Info] Start training from score -1.234966\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007207 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006872 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006704 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.015347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006933 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.007128 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006782 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.007322 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.006484 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.006808 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.96 MB) transferred to GPU in 0.008457 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.94 MB) transferred to GPU in 0.009857 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 27 dense feature groups (2.95 MB) transferred to GPU in 0.008735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5465236515624101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5465236515624101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7112971856812584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112971856812584\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 17:16:35 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "lightgbm_tuned_model, lightgbm_tuner = tune_model(lightgbm, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "726152d4-3607-4022-9cf7-0f77a7b5461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(bagging_fraction=0.6852666247712094, bagging_freq=6,\n",
      "               boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               device='gpu', feature_fraction=0.8474520015916083,\n",
      "               importance_type='split', learning_rate=0.44396855094714766,\n",
      "               max_depth=-1, min_child_samples=18, min_child_weight=0.001,\n",
      "               min_split_gain=0.9165929677584167, n_estimators=46, n_jobs=-1,\n",
      "               num_leaves=126, objective=None, random_state=5991,\n",
      "               reg_alpha=3.497504838524017e-10,\n",
      "               reg_lambda=1.0536166171914501e-05, subsample=1.0,\n",
      "               subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "print(lightgbm_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6de13ba-59cf-4048-8b12-75912f3fd0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21d15c3e07c4cbaa65cbd84709b6ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31cadee3-f0e3-4f9d-89ba-517d8944a485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewness</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entropy</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mean</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>shape</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clearance</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ptp</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>std</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>energy</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rms</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>impulse</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>crest</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0             meanFreq         325\n",
       "1            bandwidth         312\n",
       "2          meanWavelet         287\n",
       "3           medianFreq         281\n",
       "4    spectral_flatness         278\n",
       "5             skewness         182\n",
       "6       entropyWavelet         167\n",
       "7    spectral_skewness         162\n",
       "8    spectral_kurtosis         112\n",
       "9           varWavelet         109\n",
       "10             entropy         107\n",
       "11                mean         100\n",
       "12               shape          71\n",
       "13    spectral_entropy          66\n",
       "14            kurtosis          58\n",
       "15           clearance          48\n",
       "16                 ptp          42\n",
       "17       energyWavelet          35\n",
       "18      varSpectrogram          35\n",
       "19                 std          34\n",
       "20  entropySpectrogram          32\n",
       "21   energySpectrogram          31\n",
       "22              energy          30\n",
       "23                 rms          26\n",
       "24             impulse          25\n",
       "25     meanSpectrogram          24\n",
       "26               crest          21"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm_top_features = get_feature_importance_df(lightgbm, features_df_training_normalized)\n",
    "lightgbm_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a98679d5-380e-41c6-8111-6d2d19340e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_78bff\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_78bff_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_78bff_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_78bff_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_78bff_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_78bff_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_78bff_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_78bff_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_78bff_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_78bff_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_78bff_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_78bff_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_78bff_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_78bff_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_78bff_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_78bff_row0_col1\" class=\"data row0 col1\" >0.9641</td>\n",
       "      <td id=\"T_78bff_row0_col2\" class=\"data row0 col2\" >0.9891</td>\n",
       "      <td id=\"T_78bff_row0_col3\" class=\"data row0 col3\" >0.9641</td>\n",
       "      <td id=\"T_78bff_row0_col4\" class=\"data row0 col4\" >0.9638</td>\n",
       "      <td id=\"T_78bff_row0_col5\" class=\"data row0 col5\" >0.9636</td>\n",
       "      <td id=\"T_78bff_row0_col6\" class=\"data row0 col6\" >0.8948</td>\n",
       "      <td id=\"T_78bff_row0_col7\" class=\"data row0 col7\" >0.8955</td>\n",
       "      <td id=\"T_78bff_row0_col8\" class=\"data row0 col8\" >0.9378</td>\n",
       "      <td id=\"T_78bff_row0_col9\" class=\"data row0 col9\" >0.0359</td>\n",
       "      <td id=\"T_78bff_row0_col10\" class=\"data row0 col10\" >0.8480</td>\n",
       "      <td id=\"T_78bff_row0_col11\" class=\"data row0 col11\" >0.1351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a47095f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8474520015916083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8474520015916083\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6852666247712094, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6852666247712094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    }
   ],
   "source": [
    "predictions_lightgbm = predict_model(lightgbm_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba6b509-625a-40ed-be97-3ff2178817cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_46464\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_46464_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_46464_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_46464_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_46464_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_46464_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_46464_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_46464_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_46464_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_46464_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_46464_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_46464_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_46464_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_46464_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_46464_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_46464_row0_col1\" class=\"data row0 col1\" >0.9521</td>\n",
       "      <td id=\"T_46464_row0_col2\" class=\"data row0 col2\" >0.9867</td>\n",
       "      <td id=\"T_46464_row0_col3\" class=\"data row0 col3\" >0.9521</td>\n",
       "      <td id=\"T_46464_row0_col4\" class=\"data row0 col4\" >0.9530</td>\n",
       "      <td id=\"T_46464_row0_col5\" class=\"data row0 col5\" >0.9504</td>\n",
       "      <td id=\"T_46464_row0_col6\" class=\"data row0 col6\" >0.8541</td>\n",
       "      <td id=\"T_46464_row0_col7\" class=\"data row0 col7\" >0.8597</td>\n",
       "      <td id=\"T_46464_row0_col8\" class=\"data row0 col8\" >0.9018</td>\n",
       "      <td id=\"T_46464_row0_col9\" class=\"data row0 col9\" >0.0479</td>\n",
       "      <td id=\"T_46464_row0_col10\" class=\"data row0 col10\" >0.7920</td>\n",
       "      <td id=\"T_46464_row0_col11\" class=\"data row0 col11\" >0.1141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a59ce5dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_lightgbm = predict_model(lightgbm, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34173f0e-3b07-47ec-9bc1-6a932d4a3f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93960</th>\n",
       "      <td>0.552991</td>\n",
       "      <td>0.014920</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.425787</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.032911</td>\n",
       "      <td>0.065007</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>0.052295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.990514</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>2.437040e-08</td>\n",
       "      <td>3.937714e-07</td>\n",
       "      <td>2.481956e-08</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.552591</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.004507</td>\n",
       "      <td>0.423880</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.079416</td>\n",
       "      <td>0.019870</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.057731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.990030</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>5.488772e-11</td>\n",
       "      <td>1.345334e-09</td>\n",
       "      <td>6.012260e-11</td>\n",
       "      <td>healthy</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33266</th>\n",
       "      <td>0.552529</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.423978</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.011199</td>\n",
       "      <td>0.088883</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.017229</td>\n",
       "      <td>0.056826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.990020</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.947031e-10</td>\n",
       "      <td>3.991882e-09</td>\n",
       "      <td>1.963406e-10</td>\n",
       "      <td>healthy</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.9978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238774</th>\n",
       "      <td>0.551901</td>\n",
       "      <td>0.055206</td>\n",
       "      <td>0.060463</td>\n",
       "      <td>0.426438</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.151513</td>\n",
       "      <td>0.102395</td>\n",
       "      <td>0.025197</td>\n",
       "      <td>0.076742</td>\n",
       "      <td>0.058213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.993469</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>2.727015e-06</td>\n",
       "      <td>3.764719e-05</td>\n",
       "      <td>2.924497e-06</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210317</th>\n",
       "      <td>0.551909</td>\n",
       "      <td>0.021319</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.425487</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.052560</td>\n",
       "      <td>0.076691</td>\n",
       "      <td>0.020320</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.990896</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>1.558190e-07</td>\n",
       "      <td>2.279038e-06</td>\n",
       "      <td>1.575380e-07</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241132</th>\n",
       "      <td>0.551992</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.024539</td>\n",
       "      <td>0.424149</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.073776</td>\n",
       "      <td>0.113589</td>\n",
       "      <td>0.031148</td>\n",
       "      <td>0.056360</td>\n",
       "      <td>0.086858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>3.148032e-07</td>\n",
       "      <td>4.255994e-06</td>\n",
       "      <td>3.150592e-07</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.7937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214791</th>\n",
       "      <td>0.551964</td>\n",
       "      <td>0.024686</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.424141</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.082537</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>0.029353</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.991052</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>4.999926e-07</td>\n",
       "      <td>6.492028e-06</td>\n",
       "      <td>5.006992e-07</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.8331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165165</th>\n",
       "      <td>0.552578</td>\n",
       "      <td>0.092093</td>\n",
       "      <td>0.100734</td>\n",
       "      <td>0.411164</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>0.384982</td>\n",
       "      <td>0.169760</td>\n",
       "      <td>0.044862</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.083977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.992674</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>5.570935e-06</td>\n",
       "      <td>8.848780e-05</td>\n",
       "      <td>6.855859e-06</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217690</th>\n",
       "      <td>0.552060</td>\n",
       "      <td>0.089336</td>\n",
       "      <td>0.097720</td>\n",
       "      <td>0.429396</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.214175</td>\n",
       "      <td>0.078653</td>\n",
       "      <td>0.019528</td>\n",
       "      <td>0.080315</td>\n",
       "      <td>0.055886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.993681</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>0.008986</td>\n",
       "      <td>3.116306e-05</td>\n",
       "      <td>3.357050e-04</td>\n",
       "      <td>3.242283e-05</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.7248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187170</th>\n",
       "      <td>0.552055</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>0.045607</td>\n",
       "      <td>0.425012</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.107392</td>\n",
       "      <td>0.080719</td>\n",
       "      <td>0.020103</td>\n",
       "      <td>0.055484</td>\n",
       "      <td>0.057011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>7.472535e-07</td>\n",
       "      <td>1.110549e-05</td>\n",
       "      <td>8.107659e-07</td>\n",
       "      <td>damaged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.9989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61590 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "93960   0.552991  0.014920  0.016479  0.425787  0.002338  0.032911  0.065007   \n",
       "371     0.552591  0.003928  0.004507  0.423880  0.002797  0.010123  0.079416   \n",
       "33266   0.552529  0.003798  0.004368  0.423978  0.002739  0.011199  0.088883   \n",
       "238774  0.551901  0.055206  0.060463  0.426438  0.002858  0.151513  0.102395   \n",
       "210317  0.551909  0.021319  0.023460  0.425487  0.003348  0.052560  0.076691   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "241132  0.551992  0.022300  0.024539  0.424149  0.005463  0.073776  0.113589   \n",
       "214791  0.551964  0.024686  0.027143  0.424141  0.005454  0.082537  0.110840   \n",
       "165165  0.552578  0.092093  0.100734  0.411164  0.008022  0.384982  0.169760   \n",
       "217690  0.552060  0.089336  0.097720  0.429396  0.002707  0.214175  0.078653   \n",
       "187170  0.552055  0.041592  0.045607  0.425012  0.002743  0.107392  0.080719   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "93960   0.016130   0.027697  0.052295  ...    0.000159        0.990514   \n",
       "371     0.019870   0.016115  0.057731  ...    0.000032        0.990030   \n",
       "33266   0.021954   0.017229  0.056826  ...    0.000035        0.990020   \n",
       "238774  0.025197   0.076742  0.058213  ...    0.006693        0.993469   \n",
       "210317  0.020320   0.039388  0.069841  ...    0.001073        0.990896   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "241132  0.031148   0.056360  0.086858  ...    0.001233        0.990859   \n",
       "214791  0.029353   0.056729  0.077346  ...    0.001427        0.991052   \n",
       "165165  0.044862   0.158203  0.083977  ...    0.017095        0.992674   \n",
       "217690  0.019528   0.080315  0.055886  ...    0.017375        0.993681   \n",
       "187170  0.020103   0.055484  0.057011  ...    0.003089        0.992766   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "93960        0.000349         0.000205    2.437040e-08        3.937714e-07   \n",
       "371          0.000038         0.000016    5.488772e-11        1.345334e-09   \n",
       "33266        0.000035         0.000016    1.947031e-10        3.991882e-09   \n",
       "238774       0.007301         0.003240    2.727015e-06        3.764719e-05   \n",
       "210317       0.001099         0.000473    1.558190e-07        2.279038e-06   \n",
       "...               ...              ...             ...                 ...   \n",
       "241132       0.001203         0.000570    3.148032e-07        4.255994e-06   \n",
       "214791       0.001472         0.000726    4.999926e-07        6.492028e-06   \n",
       "165165       0.017984         0.007493    5.570935e-06        8.848780e-05   \n",
       "217690       0.019038         0.008986    3.116306e-05        3.357050e-04   \n",
       "187170       0.003856         0.001802    7.472535e-07        1.110549e-05   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "93960        2.481956e-08  damaged           damaged            0.9990  \n",
       "371          6.012260e-11  healthy           healthy            0.9804  \n",
       "33266        1.963406e-10  healthy           healthy            0.9978  \n",
       "238774       2.924497e-06  damaged           damaged            0.9298  \n",
       "210317       1.575380e-07  damaged           damaged            0.9209  \n",
       "...                   ...      ...               ...               ...  \n",
       "241132       3.150592e-07  damaged           damaged            0.7937  \n",
       "214791       5.006992e-07  damaged           damaged            0.8331  \n",
       "165165       6.855859e-06  damaged           damaged            0.9999  \n",
       "217690       3.242283e-05  damaged           damaged            0.7248  \n",
       "187170       8.107659e-07  damaged           damaged            0.9989  \n",
       "\n",
       "[61590 rows x 30 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789da63d-eb26-4f36-9559-4f8dc35b2c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258524</th>\n",
       "      <td>0.551992</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.420304</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.007740</td>\n",
       "      <td>0.113671</td>\n",
       "      <td>0.029747</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.074810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.989986</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4.953652e-11</td>\n",
       "      <td>1.070947e-09</td>\n",
       "      <td>5.001858e-11</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207745</th>\n",
       "      <td>0.551983</td>\n",
       "      <td>0.064504</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>0.422450</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.129561</td>\n",
       "      <td>0.058001</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.054014</td>\n",
       "      <td>0.047083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>0.992818</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>1.246231e-05</td>\n",
       "      <td>1.371142e-04</td>\n",
       "      <td>1.267123e-05</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.8420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>0.551969</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>0.426228</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.114917</td>\n",
       "      <td>0.071149</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.058699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>2.272309e-06</td>\n",
       "      <td>3.087725e-05</td>\n",
       "      <td>2.403533e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207040</th>\n",
       "      <td>0.552078</td>\n",
       "      <td>0.061361</td>\n",
       "      <td>0.067186</td>\n",
       "      <td>0.426386</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.172687</td>\n",
       "      <td>0.088008</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>5.222147e-06</td>\n",
       "      <td>6.780543e-05</td>\n",
       "      <td>5.558231e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254381</th>\n",
       "      <td>0.552025</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.423788</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>0.085672</td>\n",
       "      <td>0.023867</td>\n",
       "      <td>0.049086</td>\n",
       "      <td>0.084438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.991014</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>6.144647e-07</td>\n",
       "      <td>7.738488e-06</td>\n",
       "      <td>6.121063e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.8711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200497</th>\n",
       "      <td>0.551902</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>0.425166</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.075176</td>\n",
       "      <td>0.090681</td>\n",
       "      <td>0.024281</td>\n",
       "      <td>0.051047</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.991111</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>5.012502e-07</td>\n",
       "      <td>6.812054e-06</td>\n",
       "      <td>5.060598e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.5772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260610</th>\n",
       "      <td>0.552007</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>0.062526</td>\n",
       "      <td>0.424157</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.146886</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020677</td>\n",
       "      <td>0.066490</td>\n",
       "      <td>0.065090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.993342</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>4.189857e-06</td>\n",
       "      <td>5.363786e-05</td>\n",
       "      <td>4.399530e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258687</th>\n",
       "      <td>0.552024</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.425718</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.079928</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.043132</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.990960</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>3.881280e-07</td>\n",
       "      <td>5.331666e-06</td>\n",
       "      <td>3.902922e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205706</th>\n",
       "      <td>0.551965</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.024582</td>\n",
       "      <td>0.425170</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.063984</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.065854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>2.401605e-07</td>\n",
       "      <td>3.405097e-06</td>\n",
       "      <td>2.429444e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201271</th>\n",
       "      <td>0.551941</td>\n",
       "      <td>0.024814</td>\n",
       "      <td>0.027281</td>\n",
       "      <td>0.424357</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.061563</td>\n",
       "      <td>0.074165</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.065033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.991035</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>3.764377e-07</td>\n",
       "      <td>5.222327e-06</td>\n",
       "      <td>3.792063e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.6709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2953 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "258524  0.551992  0.002439  0.002697  0.420304  0.004412  0.007740  0.113671   \n",
       "207745  0.551983  0.064504  0.070614  0.422450  0.001964  0.129561  0.058001   \n",
       "255723  0.551969  0.049521  0.054260  0.426228  0.002760  0.114917  0.071149   \n",
       "207040  0.552078  0.061361  0.067186  0.426386  0.003138  0.172687  0.088008   \n",
       "254381  0.552025  0.025862  0.028432  0.423788  0.004653  0.072528  0.085672   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "200497  0.551902  0.026917  0.029575  0.425166  0.003908  0.075176  0.090681   \n",
       "260610  0.552007  0.057093  0.062526  0.424157  0.003021  0.146886  0.080000   \n",
       "258687  0.552024  0.023971  0.026367  0.425718  0.003892  0.063684  0.079928   \n",
       "205706  0.551965  0.022341  0.024582  0.425170  0.003709  0.063984  0.088835   \n",
       "201271  0.551941  0.024814  0.027281  0.424357  0.003260  0.061563  0.074165   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "258524  0.029747   0.017695  0.074810  ...    0.000011        0.989986   \n",
       "207745  0.014165   0.054014  0.047083  ...    0.009980        0.992818   \n",
       "255723  0.018054   0.055952  0.058699  ...    0.005488        0.992932   \n",
       "207040  0.022221   0.073193  0.061610  ...    0.008198        0.993852   \n",
       "254381  0.023867   0.049086  0.084438  ...    0.001710        0.991014   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "200497  0.024281   0.051047  0.076014  ...    0.001828        0.991111   \n",
       "260610  0.020677   0.066490  0.065090  ...    0.007284        0.993342   \n",
       "258687  0.021201   0.043132  0.071010  ...    0.001458        0.990960   \n",
       "205706  0.022831   0.044268  0.065854  ...    0.001195        0.990959   \n",
       "201271  0.019299   0.041013  0.065033  ...    0.001545        0.991035   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "258524       0.000014         0.000008    4.953652e-11        1.070947e-09   \n",
       "207745       0.009960         0.004548    1.246231e-05        1.371142e-04   \n",
       "255723       0.005880         0.002727    2.272309e-06        3.087725e-05   \n",
       "207040       0.009018         0.004295    5.222147e-06        6.780543e-05   \n",
       "254381       0.001616         0.000722    6.144647e-07        7.738488e-06   \n",
       "...               ...              ...             ...                 ...   \n",
       "200497       0.001748         0.000831    5.012502e-07        6.812054e-06   \n",
       "260610       0.007811         0.003527    4.189857e-06        5.363786e-05   \n",
       "258687       0.001389         0.000688    3.881280e-07        5.331666e-06   \n",
       "205706       0.001207         0.000591    2.401605e-07        3.405097e-06   \n",
       "201271       0.001487         0.000697    3.764377e-07        5.222327e-06   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "258524       5.001858e-11  healthy           damaged            0.6944  \n",
       "207745       1.267123e-05  healthy           damaged            0.8420  \n",
       "255723       2.403533e-06  healthy           damaged            0.7625  \n",
       "207040       5.558231e-06  healthy           damaged            0.6417  \n",
       "254381       6.121063e-07  healthy           damaged            0.8711  \n",
       "...                   ...      ...               ...               ...  \n",
       "200497       5.060598e-07  healthy           damaged            0.5772  \n",
       "260610       4.399530e-06  healthy           damaged            0.5103  \n",
       "258687       3.902922e-07  healthy           damaged            0.6065  \n",
       "205706       2.429444e-07  healthy           damaged            0.6497  \n",
       "201271       3.792063e-07  healthy           damaged            0.6709  \n",
       "\n",
       "[2953 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d3df1-3151-4046-8abc-d66039145875",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6aa747c-a901-435f-b4ae-0e0cf24659a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_32969_row10_col0, #T_32969_row10_col1, #T_32969_row10_col2, #T_32969_row10_col3, #T_32969_row10_col4, #T_32969_row10_col5, #T_32969_row10_col6, #T_32969_row10_col7, #T_32969_row10_col8, #T_32969_row10_col9, #T_32969_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_32969\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_32969_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_32969_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_32969_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_32969_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_32969_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_32969_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_32969_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_32969_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_32969_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_32969_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_32969_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_32969_row0_col0\" class=\"data row0 col0\" >0.9769</td>\n",
       "      <td id=\"T_32969_row0_col1\" class=\"data row0 col1\" >0.9954</td>\n",
       "      <td id=\"T_32969_row0_col2\" class=\"data row0 col2\" >0.9769</td>\n",
       "      <td id=\"T_32969_row0_col3\" class=\"data row0 col3\" >0.9771</td>\n",
       "      <td id=\"T_32969_row0_col4\" class=\"data row0 col4\" >0.9766</td>\n",
       "      <td id=\"T_32969_row0_col5\" class=\"data row0 col5\" >0.9320</td>\n",
       "      <td id=\"T_32969_row0_col6\" class=\"data row0 col6\" >0.9333</td>\n",
       "      <td id=\"T_32969_row0_col7\" class=\"data row0 col7\" >0.9531</td>\n",
       "      <td id=\"T_32969_row0_col8\" class=\"data row0 col8\" >0.0231</td>\n",
       "      <td id=\"T_32969_row0_col9\" class=\"data row0 col9\" >0.8988</td>\n",
       "      <td id=\"T_32969_row0_col10\" class=\"data row0 col10\" >0.0757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_32969_row1_col0\" class=\"data row1 col0\" >0.9759</td>\n",
       "      <td id=\"T_32969_row1_col1\" class=\"data row1 col1\" >0.9952</td>\n",
       "      <td id=\"T_32969_row1_col2\" class=\"data row1 col2\" >0.9759</td>\n",
       "      <td id=\"T_32969_row1_col3\" class=\"data row1 col3\" >0.9761</td>\n",
       "      <td id=\"T_32969_row1_col4\" class=\"data row1 col4\" >0.9755</td>\n",
       "      <td id=\"T_32969_row1_col5\" class=\"data row1 col5\" >0.9289</td>\n",
       "      <td id=\"T_32969_row1_col6\" class=\"data row1 col6\" >0.9302</td>\n",
       "      <td id=\"T_32969_row1_col7\" class=\"data row1 col7\" >0.9512</td>\n",
       "      <td id=\"T_32969_row1_col8\" class=\"data row1 col8\" >0.0241</td>\n",
       "      <td id=\"T_32969_row1_col9\" class=\"data row1 col9\" >0.8943</td>\n",
       "      <td id=\"T_32969_row1_col10\" class=\"data row1 col10\" >0.0762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_32969_row2_col0\" class=\"data row2 col0\" >0.9754</td>\n",
       "      <td id=\"T_32969_row2_col1\" class=\"data row2 col1\" >0.9956</td>\n",
       "      <td id=\"T_32969_row2_col2\" class=\"data row2 col2\" >0.9754</td>\n",
       "      <td id=\"T_32969_row2_col3\" class=\"data row2 col3\" >0.9756</td>\n",
       "      <td id=\"T_32969_row2_col4\" class=\"data row2 col4\" >0.9750</td>\n",
       "      <td id=\"T_32969_row2_col5\" class=\"data row2 col5\" >0.9273</td>\n",
       "      <td id=\"T_32969_row2_col6\" class=\"data row2 col6\" >0.9287</td>\n",
       "      <td id=\"T_32969_row2_col7\" class=\"data row2 col7\" >0.9503</td>\n",
       "      <td id=\"T_32969_row2_col8\" class=\"data row2 col8\" >0.0246</td>\n",
       "      <td id=\"T_32969_row2_col9\" class=\"data row2 col9\" >0.8921</td>\n",
       "      <td id=\"T_32969_row2_col10\" class=\"data row2 col10\" >0.0759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_32969_row3_col0\" class=\"data row3 col0\" >0.9755</td>\n",
       "      <td id=\"T_32969_row3_col1\" class=\"data row3 col1\" >0.9951</td>\n",
       "      <td id=\"T_32969_row3_col2\" class=\"data row3 col2\" >0.9755</td>\n",
       "      <td id=\"T_32969_row3_col3\" class=\"data row3 col3\" >0.9758</td>\n",
       "      <td id=\"T_32969_row3_col4\" class=\"data row3 col4\" >0.9751</td>\n",
       "      <td id=\"T_32969_row3_col5\" class=\"data row3 col5\" >0.9278</td>\n",
       "      <td id=\"T_32969_row3_col6\" class=\"data row3 col6\" >0.9292</td>\n",
       "      <td id=\"T_32969_row3_col7\" class=\"data row3 col7\" >0.9502</td>\n",
       "      <td id=\"T_32969_row3_col8\" class=\"data row3 col8\" >0.0245</td>\n",
       "      <td id=\"T_32969_row3_col9\" class=\"data row3 col9\" >0.8927</td>\n",
       "      <td id=\"T_32969_row3_col10\" class=\"data row3 col10\" >0.0763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_32969_row4_col0\" class=\"data row4 col0\" >0.9768</td>\n",
       "      <td id=\"T_32969_row4_col1\" class=\"data row4 col1\" >0.9950</td>\n",
       "      <td id=\"T_32969_row4_col2\" class=\"data row4 col2\" >0.9768</td>\n",
       "      <td id=\"T_32969_row4_col3\" class=\"data row4 col3\" >0.9769</td>\n",
       "      <td id=\"T_32969_row4_col4\" class=\"data row4 col4\" >0.9765</td>\n",
       "      <td id=\"T_32969_row4_col5\" class=\"data row4 col5\" >0.9318</td>\n",
       "      <td id=\"T_32969_row4_col6\" class=\"data row4 col6\" >0.9329</td>\n",
       "      <td id=\"T_32969_row4_col7\" class=\"data row4 col7\" >0.9540</td>\n",
       "      <td id=\"T_32969_row4_col8\" class=\"data row4 col8\" >0.0232</td>\n",
       "      <td id=\"T_32969_row4_col9\" class=\"data row4 col9\" >0.8986</td>\n",
       "      <td id=\"T_32969_row4_col10\" class=\"data row4 col10\" >0.0758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_32969_row5_col0\" class=\"data row5 col0\" >0.9757</td>\n",
       "      <td id=\"T_32969_row5_col1\" class=\"data row5 col1\" >0.9949</td>\n",
       "      <td id=\"T_32969_row5_col2\" class=\"data row5 col2\" >0.9757</td>\n",
       "      <td id=\"T_32969_row5_col3\" class=\"data row5 col3\" >0.9758</td>\n",
       "      <td id=\"T_32969_row5_col4\" class=\"data row5 col4\" >0.9754</td>\n",
       "      <td id=\"T_32969_row5_col5\" class=\"data row5 col5\" >0.9285</td>\n",
       "      <td id=\"T_32969_row5_col6\" class=\"data row5 col6\" >0.9297</td>\n",
       "      <td id=\"T_32969_row5_col7\" class=\"data row5 col7\" >0.9520</td>\n",
       "      <td id=\"T_32969_row5_col8\" class=\"data row5 col8\" >0.0243</td>\n",
       "      <td id=\"T_32969_row5_col9\" class=\"data row5 col9\" >0.8940</td>\n",
       "      <td id=\"T_32969_row5_col10\" class=\"data row5 col10\" >0.0774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_32969_row6_col0\" class=\"data row6 col0\" >0.9777</td>\n",
       "      <td id=\"T_32969_row6_col1\" class=\"data row6 col1\" >0.9963</td>\n",
       "      <td id=\"T_32969_row6_col2\" class=\"data row6 col2\" >0.9777</td>\n",
       "      <td id=\"T_32969_row6_col3\" class=\"data row6 col3\" >0.9780</td>\n",
       "      <td id=\"T_32969_row6_col4\" class=\"data row6 col4\" >0.9774</td>\n",
       "      <td id=\"T_32969_row6_col5\" class=\"data row6 col5\" >0.9344</td>\n",
       "      <td id=\"T_32969_row6_col6\" class=\"data row6 col6\" >0.9356</td>\n",
       "      <td id=\"T_32969_row6_col7\" class=\"data row6 col7\" >0.9545</td>\n",
       "      <td id=\"T_32969_row6_col8\" class=\"data row6 col8\" >0.0223</td>\n",
       "      <td id=\"T_32969_row6_col9\" class=\"data row6 col9\" >0.9023</td>\n",
       "      <td id=\"T_32969_row6_col10\" class=\"data row6 col10\" >0.0728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_32969_row7_col0\" class=\"data row7 col0\" >0.9767</td>\n",
       "      <td id=\"T_32969_row7_col1\" class=\"data row7 col1\" >0.9956</td>\n",
       "      <td id=\"T_32969_row7_col2\" class=\"data row7 col2\" >0.9767</td>\n",
       "      <td id=\"T_32969_row7_col3\" class=\"data row7 col3\" >0.9770</td>\n",
       "      <td id=\"T_32969_row7_col4\" class=\"data row7 col4\" >0.9764</td>\n",
       "      <td id=\"T_32969_row7_col5\" class=\"data row7 col5\" >0.9314</td>\n",
       "      <td id=\"T_32969_row7_col6\" class=\"data row7 col6\" >0.9328</td>\n",
       "      <td id=\"T_32969_row7_col7\" class=\"data row7 col7\" >0.9522</td>\n",
       "      <td id=\"T_32969_row7_col8\" class=\"data row7 col8\" >0.0233</td>\n",
       "      <td id=\"T_32969_row7_col9\" class=\"data row7 col9\" >0.8979</td>\n",
       "      <td id=\"T_32969_row7_col10\" class=\"data row7 col10\" >0.0762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_32969_row8_col0\" class=\"data row8 col0\" >0.9741</td>\n",
       "      <td id=\"T_32969_row8_col1\" class=\"data row8 col1\" >0.9950</td>\n",
       "      <td id=\"T_32969_row8_col2\" class=\"data row8 col2\" >0.9741</td>\n",
       "      <td id=\"T_32969_row8_col3\" class=\"data row8 col3\" >0.9743</td>\n",
       "      <td id=\"T_32969_row8_col4\" class=\"data row8 col4\" >0.9737</td>\n",
       "      <td id=\"T_32969_row8_col5\" class=\"data row8 col5\" >0.9238</td>\n",
       "      <td id=\"T_32969_row8_col6\" class=\"data row8 col6\" >0.9251</td>\n",
       "      <td id=\"T_32969_row8_col7\" class=\"data row8 col7\" >0.9488</td>\n",
       "      <td id=\"T_32969_row8_col8\" class=\"data row8 col8\" >0.0259</td>\n",
       "      <td id=\"T_32969_row8_col9\" class=\"data row8 col9\" >0.8872</td>\n",
       "      <td id=\"T_32969_row8_col10\" class=\"data row8 col10\" >0.0782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_32969_row9_col0\" class=\"data row9 col0\" >0.9759</td>\n",
       "      <td id=\"T_32969_row9_col1\" class=\"data row9 col1\" >0.9954</td>\n",
       "      <td id=\"T_32969_row9_col2\" class=\"data row9 col2\" >0.9759</td>\n",
       "      <td id=\"T_32969_row9_col3\" class=\"data row9 col3\" >0.9762</td>\n",
       "      <td id=\"T_32969_row9_col4\" class=\"data row9 col4\" >0.9756</td>\n",
       "      <td id=\"T_32969_row9_col5\" class=\"data row9 col5\" >0.9290</td>\n",
       "      <td id=\"T_32969_row9_col6\" class=\"data row9 col6\" >0.9304</td>\n",
       "      <td id=\"T_32969_row9_col7\" class=\"data row9 col7\" >0.9509</td>\n",
       "      <td id=\"T_32969_row9_col8\" class=\"data row9 col8\" >0.0241</td>\n",
       "      <td id=\"T_32969_row9_col9\" class=\"data row9 col9\" >0.8945</td>\n",
       "      <td id=\"T_32969_row9_col10\" class=\"data row9 col10\" >0.0747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_32969_row10_col0\" class=\"data row10 col0\" >0.9761</td>\n",
       "      <td id=\"T_32969_row10_col1\" class=\"data row10 col1\" >0.9953</td>\n",
       "      <td id=\"T_32969_row10_col2\" class=\"data row10 col2\" >0.9761</td>\n",
       "      <td id=\"T_32969_row10_col3\" class=\"data row10 col3\" >0.9763</td>\n",
       "      <td id=\"T_32969_row10_col4\" class=\"data row10 col4\" >0.9757</td>\n",
       "      <td id=\"T_32969_row10_col5\" class=\"data row10 col5\" >0.9295</td>\n",
       "      <td id=\"T_32969_row10_col6\" class=\"data row10 col6\" >0.9308</td>\n",
       "      <td id=\"T_32969_row10_col7\" class=\"data row10 col7\" >0.9517</td>\n",
       "      <td id=\"T_32969_row10_col8\" class=\"data row10 col8\" >0.0239</td>\n",
       "      <td id=\"T_32969_row10_col9\" class=\"data row10 col9\" >0.8952</td>\n",
       "      <td id=\"T_32969_row10_col10\" class=\"data row10 col10\" >0.0759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32969_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_32969_row11_col0\" class=\"data row11 col0\" >0.0010</td>\n",
       "      <td id=\"T_32969_row11_col1\" class=\"data row11 col1\" >0.0004</td>\n",
       "      <td id=\"T_32969_row11_col2\" class=\"data row11 col2\" >0.0010</td>\n",
       "      <td id=\"T_32969_row11_col3\" class=\"data row11 col3\" >0.0010</td>\n",
       "      <td id=\"T_32969_row11_col4\" class=\"data row11 col4\" >0.0010</td>\n",
       "      <td id=\"T_32969_row11_col5\" class=\"data row11 col5\" >0.0029</td>\n",
       "      <td id=\"T_32969_row11_col6\" class=\"data row11 col6\" >0.0028</td>\n",
       "      <td id=\"T_32969_row11_col7\" class=\"data row11 col7\" >0.0017</td>\n",
       "      <td id=\"T_32969_row11_col8\" class=\"data row11 col8\" >0.0010</td>\n",
       "      <td id=\"T_32969_row11_col9\" class=\"data row11 col9\" >0.0041</td>\n",
       "      <td id=\"T_32969_row11_col10\" class=\"data row11 col10\" >0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a47134c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 17:19:49 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "rf = create_model('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7824407f-4b19-4413-aa1a-12494fe53e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0c1e8_row10_col0, #T_0c1e8_row10_col1, #T_0c1e8_row10_col2, #T_0c1e8_row10_col3, #T_0c1e8_row10_col4, #T_0c1e8_row10_col5, #T_0c1e8_row10_col6, #T_0c1e8_row10_col7, #T_0c1e8_row10_col8, #T_0c1e8_row10_col9, #T_0c1e8_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0c1e8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0c1e8_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_0c1e8_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_0c1e8_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_0c1e8_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_0c1e8_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_0c1e8_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_0c1e8_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_0c1e8_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_0c1e8_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_0c1e8_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_0c1e8_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0c1e8_row0_col0\" class=\"data row0 col0\" >0.9295</td>\n",
       "      <td id=\"T_0c1e8_row0_col1\" class=\"data row0 col1\" >0.9826</td>\n",
       "      <td id=\"T_0c1e8_row0_col2\" class=\"data row0 col2\" >0.9295</td>\n",
       "      <td id=\"T_0c1e8_row0_col3\" class=\"data row0 col3\" >0.9327</td>\n",
       "      <td id=\"T_0c1e8_row0_col4\" class=\"data row0 col4\" >0.9306</td>\n",
       "      <td id=\"T_0c1e8_row0_col5\" class=\"data row0 col5\" >0.8045</td>\n",
       "      <td id=\"T_0c1e8_row0_col6\" class=\"data row0 col6\" >0.8058</td>\n",
       "      <td id=\"T_0c1e8_row0_col7\" class=\"data row0 col7\" >0.9154</td>\n",
       "      <td id=\"T_0c1e8_row0_col8\" class=\"data row0 col8\" >0.0705</td>\n",
       "      <td id=\"T_0c1e8_row0_col9\" class=\"data row0 col9\" >0.7398</td>\n",
       "      <td id=\"T_0c1e8_row0_col10\" class=\"data row0 col10\" >0.1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0c1e8_row1_col0\" class=\"data row1 col0\" >0.9296</td>\n",
       "      <td id=\"T_0c1e8_row1_col1\" class=\"data row1 col1\" >0.9810</td>\n",
       "      <td id=\"T_0c1e8_row1_col2\" class=\"data row1 col2\" >0.9296</td>\n",
       "      <td id=\"T_0c1e8_row1_col3\" class=\"data row1 col3\" >0.9322</td>\n",
       "      <td id=\"T_0c1e8_row1_col4\" class=\"data row1 col4\" >0.9305</td>\n",
       "      <td id=\"T_0c1e8_row1_col5\" class=\"data row1 col5\" >0.8039</td>\n",
       "      <td id=\"T_0c1e8_row1_col6\" class=\"data row1 col6\" >0.8049</td>\n",
       "      <td id=\"T_0c1e8_row1_col7\" class=\"data row1 col7\" >0.9133</td>\n",
       "      <td id=\"T_0c1e8_row1_col8\" class=\"data row1 col8\" >0.0704</td>\n",
       "      <td id=\"T_0c1e8_row1_col9\" class=\"data row1 col9\" >0.7388</td>\n",
       "      <td id=\"T_0c1e8_row1_col10\" class=\"data row1 col10\" >0.1665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0c1e8_row2_col0\" class=\"data row2 col0\" >0.9295</td>\n",
       "      <td id=\"T_0c1e8_row2_col1\" class=\"data row2 col1\" >0.9821</td>\n",
       "      <td id=\"T_0c1e8_row2_col2\" class=\"data row2 col2\" >0.9295</td>\n",
       "      <td id=\"T_0c1e8_row2_col3\" class=\"data row2 col3\" >0.9328</td>\n",
       "      <td id=\"T_0c1e8_row2_col4\" class=\"data row2 col4\" >0.9306</td>\n",
       "      <td id=\"T_0c1e8_row2_col5\" class=\"data row2 col5\" >0.8047</td>\n",
       "      <td id=\"T_0c1e8_row2_col6\" class=\"data row2 col6\" >0.8061</td>\n",
       "      <td id=\"T_0c1e8_row2_col7\" class=\"data row2 col7\" >0.9158</td>\n",
       "      <td id=\"T_0c1e8_row2_col8\" class=\"data row2 col8\" >0.0705</td>\n",
       "      <td id=\"T_0c1e8_row2_col9\" class=\"data row2 col9\" >0.7402</td>\n",
       "      <td id=\"T_0c1e8_row2_col10\" class=\"data row2 col10\" >0.1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0c1e8_row3_col0\" class=\"data row3 col0\" >0.9242</td>\n",
       "      <td id=\"T_0c1e8_row3_col1\" class=\"data row3 col1\" >0.9802</td>\n",
       "      <td id=\"T_0c1e8_row3_col2\" class=\"data row3 col2\" >0.9242</td>\n",
       "      <td id=\"T_0c1e8_row3_col3\" class=\"data row3 col3\" >0.9280</td>\n",
       "      <td id=\"T_0c1e8_row3_col4\" class=\"data row3 col4\" >0.9254</td>\n",
       "      <td id=\"T_0c1e8_row3_col5\" class=\"data row3 col5\" >0.7904</td>\n",
       "      <td id=\"T_0c1e8_row3_col6\" class=\"data row3 col6\" >0.7921</td>\n",
       "      <td id=\"T_0c1e8_row3_col7\" class=\"data row3 col7\" >0.9098</td>\n",
       "      <td id=\"T_0c1e8_row3_col8\" class=\"data row3 col8\" >0.0758</td>\n",
       "      <td id=\"T_0c1e8_row3_col9\" class=\"data row3 col9\" >0.7241</td>\n",
       "      <td id=\"T_0c1e8_row3_col10\" class=\"data row3 col10\" >0.1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0c1e8_row4_col0\" class=\"data row4 col0\" >0.9306</td>\n",
       "      <td id=\"T_0c1e8_row4_col1\" class=\"data row4 col1\" >0.9817</td>\n",
       "      <td id=\"T_0c1e8_row4_col2\" class=\"data row4 col2\" >0.9306</td>\n",
       "      <td id=\"T_0c1e8_row4_col3\" class=\"data row4 col3\" >0.9330</td>\n",
       "      <td id=\"T_0c1e8_row4_col4\" class=\"data row4 col4\" >0.9315</td>\n",
       "      <td id=\"T_0c1e8_row4_col5\" class=\"data row4 col5\" >0.8063</td>\n",
       "      <td id=\"T_0c1e8_row4_col6\" class=\"data row4 col6\" >0.8071</td>\n",
       "      <td id=\"T_0c1e8_row4_col7\" class=\"data row4 col7\" >0.9135</td>\n",
       "      <td id=\"T_0c1e8_row4_col8\" class=\"data row4 col8\" >0.0694</td>\n",
       "      <td id=\"T_0c1e8_row4_col9\" class=\"data row4 col9\" >0.7413</td>\n",
       "      <td id=\"T_0c1e8_row4_col10\" class=\"data row4 col10\" >0.1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0c1e8_row5_col0\" class=\"data row5 col0\" >0.9270</td>\n",
       "      <td id=\"T_0c1e8_row5_col1\" class=\"data row5 col1\" >0.9824</td>\n",
       "      <td id=\"T_0c1e8_row5_col2\" class=\"data row5 col2\" >0.9270</td>\n",
       "      <td id=\"T_0c1e8_row5_col3\" class=\"data row5 col3\" >0.9314</td>\n",
       "      <td id=\"T_0c1e8_row5_col4\" class=\"data row5 col4\" >0.9284</td>\n",
       "      <td id=\"T_0c1e8_row5_col5\" class=\"data row5 col5\" >0.7992</td>\n",
       "      <td id=\"T_0c1e8_row5_col6\" class=\"data row5 col6\" >0.8013</td>\n",
       "      <td id=\"T_0c1e8_row5_col7\" class=\"data row5 col7\" >0.9162</td>\n",
       "      <td id=\"T_0c1e8_row5_col8\" class=\"data row5 col8\" >0.0730</td>\n",
       "      <td id=\"T_0c1e8_row5_col9\" class=\"data row5 col9\" >0.7346</td>\n",
       "      <td id=\"T_0c1e8_row5_col10\" class=\"data row5 col10\" >0.1640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0c1e8_row6_col0\" class=\"data row6 col0\" >0.9337</td>\n",
       "      <td id=\"T_0c1e8_row6_col1\" class=\"data row6 col1\" >0.9829</td>\n",
       "      <td id=\"T_0c1e8_row6_col2\" class=\"data row6 col2\" >0.9337</td>\n",
       "      <td id=\"T_0c1e8_row6_col3\" class=\"data row6 col3\" >0.9359</td>\n",
       "      <td id=\"T_0c1e8_row6_col4\" class=\"data row6 col4\" >0.9345</td>\n",
       "      <td id=\"T_0c1e8_row6_col5\" class=\"data row6 col5\" >0.8148</td>\n",
       "      <td id=\"T_0c1e8_row6_col6\" class=\"data row6 col6\" >0.8156</td>\n",
       "      <td id=\"T_0c1e8_row6_col7\" class=\"data row6 col7\" >0.9179</td>\n",
       "      <td id=\"T_0c1e8_row6_col8\" class=\"data row6 col8\" >0.0663</td>\n",
       "      <td id=\"T_0c1e8_row6_col9\" class=\"data row6 col9\" >0.7513</td>\n",
       "      <td id=\"T_0c1e8_row6_col10\" class=\"data row6 col10\" >0.1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0c1e8_row7_col0\" class=\"data row7 col0\" >0.9287</td>\n",
       "      <td id=\"T_0c1e8_row7_col1\" class=\"data row7 col1\" >0.9823</td>\n",
       "      <td id=\"T_0c1e8_row7_col2\" class=\"data row7 col2\" >0.9287</td>\n",
       "      <td id=\"T_0c1e8_row7_col3\" class=\"data row7 col3\" >0.9324</td>\n",
       "      <td id=\"T_0c1e8_row7_col4\" class=\"data row7 col4\" >0.9299</td>\n",
       "      <td id=\"T_0c1e8_row7_col5\" class=\"data row7 col5\" >0.8030</td>\n",
       "      <td id=\"T_0c1e8_row7_col6\" class=\"data row7 col6\" >0.8047</td>\n",
       "      <td id=\"T_0c1e8_row7_col7\" class=\"data row7 col7\" >0.9161</td>\n",
       "      <td id=\"T_0c1e8_row7_col8\" class=\"data row7 col8\" >0.0713</td>\n",
       "      <td id=\"T_0c1e8_row7_col9\" class=\"data row7 col9\" >0.7385</td>\n",
       "      <td id=\"T_0c1e8_row7_col10\" class=\"data row7 col10\" >0.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0c1e8_row8_col0\" class=\"data row8 col0\" >0.9331</td>\n",
       "      <td id=\"T_0c1e8_row8_col1\" class=\"data row8 col1\" >0.9822</td>\n",
       "      <td id=\"T_0c1e8_row8_col2\" class=\"data row8 col2\" >0.9331</td>\n",
       "      <td id=\"T_0c1e8_row8_col3\" class=\"data row8 col3\" >0.9355</td>\n",
       "      <td id=\"T_0c1e8_row8_col4\" class=\"data row8 col4\" >0.9339</td>\n",
       "      <td id=\"T_0c1e8_row8_col5\" class=\"data row8 col5\" >0.8134</td>\n",
       "      <td id=\"T_0c1e8_row8_col6\" class=\"data row8 col6\" >0.8143</td>\n",
       "      <td id=\"T_0c1e8_row8_col7\" class=\"data row8 col7\" >0.9178</td>\n",
       "      <td id=\"T_0c1e8_row8_col8\" class=\"data row8 col8\" >0.0669</td>\n",
       "      <td id=\"T_0c1e8_row8_col9\" class=\"data row8 col9\" >0.7498</td>\n",
       "      <td id=\"T_0c1e8_row8_col10\" class=\"data row8 col10\" >0.1662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0c1e8_row9_col0\" class=\"data row9 col0\" >0.9263</td>\n",
       "      <td id=\"T_0c1e8_row9_col1\" class=\"data row9 col1\" >0.9817</td>\n",
       "      <td id=\"T_0c1e8_row9_col2\" class=\"data row9 col2\" >0.9263</td>\n",
       "      <td id=\"T_0c1e8_row9_col3\" class=\"data row9 col3\" >0.9302</td>\n",
       "      <td id=\"T_0c1e8_row9_col4\" class=\"data row9 col4\" >0.9276</td>\n",
       "      <td id=\"T_0c1e8_row9_col5\" class=\"data row9 col5\" >0.7966</td>\n",
       "      <td id=\"T_0c1e8_row9_col6\" class=\"data row9 col6\" >0.7984</td>\n",
       "      <td id=\"T_0c1e8_row9_col7\" class=\"data row9 col7\" >0.9135</td>\n",
       "      <td id=\"T_0c1e8_row9_col8\" class=\"data row9 col8\" >0.0737</td>\n",
       "      <td id=\"T_0c1e8_row9_col9\" class=\"data row9 col9\" >0.7313</td>\n",
       "      <td id=\"T_0c1e8_row9_col10\" class=\"data row9 col10\" >0.1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_0c1e8_row10_col0\" class=\"data row10 col0\" >0.9292</td>\n",
       "      <td id=\"T_0c1e8_row10_col1\" class=\"data row10 col1\" >0.9819</td>\n",
       "      <td id=\"T_0c1e8_row10_col2\" class=\"data row10 col2\" >0.9292</td>\n",
       "      <td id=\"T_0c1e8_row10_col3\" class=\"data row10 col3\" >0.9324</td>\n",
       "      <td id=\"T_0c1e8_row10_col4\" class=\"data row10 col4\" >0.9303</td>\n",
       "      <td id=\"T_0c1e8_row10_col5\" class=\"data row10 col5\" >0.8037</td>\n",
       "      <td id=\"T_0c1e8_row10_col6\" class=\"data row10 col6\" >0.8050</td>\n",
       "      <td id=\"T_0c1e8_row10_col7\" class=\"data row10 col7\" >0.9149</td>\n",
       "      <td id=\"T_0c1e8_row10_col8\" class=\"data row10 col8\" >0.0708</td>\n",
       "      <td id=\"T_0c1e8_row10_col9\" class=\"data row10 col9\" >0.7390</td>\n",
       "      <td id=\"T_0c1e8_row10_col10\" class=\"data row10 col10\" >0.1652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c1e8_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_0c1e8_row11_col0\" class=\"data row11 col0\" >0.0028</td>\n",
       "      <td id=\"T_0c1e8_row11_col1\" class=\"data row11 col1\" >0.0008</td>\n",
       "      <td id=\"T_0c1e8_row11_col2\" class=\"data row11 col2\" >0.0028</td>\n",
       "      <td id=\"T_0c1e8_row11_col3\" class=\"data row11 col3\" >0.0022</td>\n",
       "      <td id=\"T_0c1e8_row11_col4\" class=\"data row11 col4\" >0.0026</td>\n",
       "      <td id=\"T_0c1e8_row11_col5\" class=\"data row11 col5\" >0.0069</td>\n",
       "      <td id=\"T_0c1e8_row11_col6\" class=\"data row11 col6\" >0.0066</td>\n",
       "      <td id=\"T_0c1e8_row11_col7\" class=\"data row11 col7\" >0.0023</td>\n",
       "      <td id=\"T_0c1e8_row11_col8\" class=\"data row11 col8\" >0.0028</td>\n",
       "      <td id=\"T_0c1e8_row11_col9\" class=\"data row11 col9\" >0.0076</td>\n",
       "      <td id=\"T_0c1e8_row11_col10\" class=\"data row11 col10\" >0.0019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a1a1477d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:25:54 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "rf_tuned_model, rf_tuner = tune_model(rf, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "736d1b67-d2e4-46d3-9032-24263faee3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_samples_leaf=1,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       monotonic_cst=None, n_estimators=100, n_jobs=-1,\n",
      "                       oob_score=False, random_state=5991, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rf_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "325ec679-4da9-4e79-b98a-252ad97e6ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_110fe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_110fe_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_110fe_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_110fe_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_110fe_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_110fe_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_110fe_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_110fe_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_110fe_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_110fe_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_110fe_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_110fe_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_110fe_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_110fe_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_110fe_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_110fe_row0_col1\" class=\"data row0 col1\" >0.9792</td>\n",
       "      <td id=\"T_110fe_row0_col2\" class=\"data row0 col2\" >0.9964</td>\n",
       "      <td id=\"T_110fe_row0_col3\" class=\"data row0 col3\" >0.9792</td>\n",
       "      <td id=\"T_110fe_row0_col4\" class=\"data row0 col4\" >0.9793</td>\n",
       "      <td id=\"T_110fe_row0_col5\" class=\"data row0 col5\" >0.9789</td>\n",
       "      <td id=\"T_110fe_row0_col6\" class=\"data row0 col6\" >0.9388</td>\n",
       "      <td id=\"T_110fe_row0_col7\" class=\"data row0 col7\" >0.9398</td>\n",
       "      <td id=\"T_110fe_row0_col8\" class=\"data row0 col8\" >0.9579</td>\n",
       "      <td id=\"T_110fe_row0_col9\" class=\"data row0 col9\" >0.0208</td>\n",
       "      <td id=\"T_110fe_row0_col10\" class=\"data row0 col10\" >0.9086</td>\n",
       "      <td id=\"T_110fe_row0_col11\" class=\"data row0 col11\" >0.0693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a47150c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_rf = predict_model(rf_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50123496-3039-4e57-9520-b92cc6865e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60019979bbd4ce183050fc221dc4069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a31be2f-b8c2-46e9-92bc-4ba605a72cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>0.103067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>std</td>\n",
       "      <td>0.064350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>0.058213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>0.056627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>0.053412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>0.051480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>0.051151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>energy</td>\n",
       "      <td>0.049902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rms</td>\n",
       "      <td>0.039340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clearance</td>\n",
       "      <td>0.037982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>0.033457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>skewness</td>\n",
       "      <td>0.033451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.032872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>0.032405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>0.031019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>0.028005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>0.023858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.020106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shape</td>\n",
       "      <td>0.018546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>crest</td>\n",
       "      <td>0.018169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.017576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>impulse</td>\n",
       "      <td>0.015793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>0.009959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>0.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>0.003422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0          meanWavelet    0.103067\n",
       "1           medianFreq    0.080374\n",
       "2                  std    0.064350\n",
       "3             meanFreq    0.058213\n",
       "4     spectral_entropy    0.056627\n",
       "5        energyWavelet    0.053412\n",
       "6            bandwidth    0.051480\n",
       "7    spectral_flatness    0.051151\n",
       "8               energy    0.049902\n",
       "9                  rms    0.039340\n",
       "10           clearance    0.037982\n",
       "11      entropyWavelet    0.033457\n",
       "12            skewness    0.033451\n",
       "13                 ptp    0.032872\n",
       "14     meanSpectrogram    0.032405\n",
       "15   spectral_kurtosis    0.031700\n",
       "16          varWavelet    0.031019\n",
       "17   spectral_skewness    0.028005\n",
       "18            kurtosis    0.023858\n",
       "19             entropy    0.020106\n",
       "20               shape    0.018546\n",
       "21               crest    0.018169\n",
       "22                mean    0.017576\n",
       "23             impulse    0.015793\n",
       "24  entropySpectrogram    0.009959\n",
       "25      varSpectrogram    0.003763\n",
       "26   energySpectrogram    0.003422"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_top_features = get_feature_importance_df(rf, features_df_training_normalized)\n",
    "rf_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e8fcdb-1e5a-4ef1-9c0f-538581f3f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3930f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3930f_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3930f_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_3930f_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_3930f_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_3930f_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_3930f_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_3930f_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_3930f_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_3930f_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_3930f_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_3930f_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_3930f_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3930f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3930f_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_3930f_row0_col1\" class=\"data row0 col1\" >0.9792</td>\n",
       "      <td id=\"T_3930f_row0_col2\" class=\"data row0 col2\" >0.9964</td>\n",
       "      <td id=\"T_3930f_row0_col3\" class=\"data row0 col3\" >0.9792</td>\n",
       "      <td id=\"T_3930f_row0_col4\" class=\"data row0 col4\" >0.9793</td>\n",
       "      <td id=\"T_3930f_row0_col5\" class=\"data row0 col5\" >0.9789</td>\n",
       "      <td id=\"T_3930f_row0_col6\" class=\"data row0 col6\" >0.9388</td>\n",
       "      <td id=\"T_3930f_row0_col7\" class=\"data row0 col7\" >0.9398</td>\n",
       "      <td id=\"T_3930f_row0_col8\" class=\"data row0 col8\" >0.9579</td>\n",
       "      <td id=\"T_3930f_row0_col9\" class=\"data row0 col9\" >0.0208</td>\n",
       "      <td id=\"T_3930f_row0_col10\" class=\"data row0 col10\" >0.9086</td>\n",
       "      <td id=\"T_3930f_row0_col11\" class=\"data row0 col11\" >0.0693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a5a982a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_rf = predict_model(rf, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2f4d83-272f-4b78-a3fa-f351cddf7020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>0.551969</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>0.426228</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.114917</td>\n",
       "      <td>0.071149</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.058699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>2.272309e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>2.403533e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207040</th>\n",
       "      <td>0.552078</td>\n",
       "      <td>0.061361</td>\n",
       "      <td>0.067186</td>\n",
       "      <td>0.426386</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.172687</td>\n",
       "      <td>0.088008</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>5.222147e-06</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>5.558231e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254381</th>\n",
       "      <td>0.552025</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.423788</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>0.085672</td>\n",
       "      <td>0.023867</td>\n",
       "      <td>0.049086</td>\n",
       "      <td>0.084438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.991014</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>6.144647e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>6.121063e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202856</th>\n",
       "      <td>0.551827</td>\n",
       "      <td>0.054470</td>\n",
       "      <td>0.059656</td>\n",
       "      <td>0.425756</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.140550</td>\n",
       "      <td>0.082431</td>\n",
       "      <td>0.021076</td>\n",
       "      <td>0.065754</td>\n",
       "      <td>0.063237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>0.993346</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>3.546623e-06</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>3.744839e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212285</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>0.425232</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.176425</td>\n",
       "      <td>0.084050</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>0.054543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.993919</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>8.933521e-06</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>9.383444e-06</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240598</th>\n",
       "      <td>0.551993</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>0.424818</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.071024</td>\n",
       "      <td>0.090784</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.051131</td>\n",
       "      <td>0.080478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>6.234846e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>6.257166e-07</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202298</th>\n",
       "      <td>0.551931</td>\n",
       "      <td>0.048106</td>\n",
       "      <td>0.052713</td>\n",
       "      <td>0.426111</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.082664</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>0.055979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.992947</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>2.248988e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>2.374636e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206916</th>\n",
       "      <td>0.552133</td>\n",
       "      <td>0.052712</td>\n",
       "      <td>0.057748</td>\n",
       "      <td>0.427125</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.126636</td>\n",
       "      <td>0.072806</td>\n",
       "      <td>0.018312</td>\n",
       "      <td>0.058405</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.993283</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>3.320081e-06</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>3.503566e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200497</th>\n",
       "      <td>0.551902</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>0.425166</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.075176</td>\n",
       "      <td>0.090681</td>\n",
       "      <td>0.024281</td>\n",
       "      <td>0.051047</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.991111</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>5.012502e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5.060598e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258687</th>\n",
       "      <td>0.552024</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.425718</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.079928</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.043132</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.990960</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>3.881280e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>3.902922e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1284 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "255723  0.551969  0.049521  0.054260  0.426228  0.002760  0.114917  0.071149   \n",
       "207040  0.552078  0.061361  0.067186  0.426386  0.003138  0.172687  0.088008   \n",
       "254381  0.552025  0.025862  0.028432  0.423788  0.004653  0.072528  0.085672   \n",
       "202856  0.551827  0.054470  0.059656  0.425756  0.003219  0.140550  0.082431   \n",
       "212285  0.551888  0.067535  0.073920  0.425232  0.002661  0.176425  0.084050   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "240598  0.551993  0.026294  0.028902  0.424818  0.004320  0.071024  0.090784   \n",
       "202298  0.551931  0.048106  0.052713  0.426111  0.002825  0.130462  0.082664   \n",
       "206916  0.552133  0.052712  0.057748  0.427125  0.002644  0.126636  0.072806   \n",
       "200497  0.551902  0.026917  0.029575  0.425166  0.003908  0.075176  0.090681   \n",
       "258687  0.552024  0.023971  0.026367  0.425718  0.003892  0.063684  0.079928   \n",
       "\n",
       "         impulse  clearance     shape  ...  varWavelet  entropyWavelet  \\\n",
       "255723  0.018054   0.055952  0.058699  ...    0.005488        0.992932   \n",
       "207040  0.022221   0.073193  0.061610  ...    0.008198        0.993852   \n",
       "254381  0.023867   0.049086  0.084438  ...    0.001710        0.991014   \n",
       "202856  0.021076   0.065754  0.063237  ...    0.006539        0.993346   \n",
       "212285  0.020632   0.072623  0.054543  ...    0.009619        0.993919   \n",
       "...          ...        ...       ...  ...         ...             ...   \n",
       "240598  0.024753   0.051131  0.080478  ...    0.001796        0.991025   \n",
       "202298  0.020452   0.060604  0.055979  ...    0.005076        0.992947   \n",
       "206916  0.018312   0.058405  0.057251  ...    0.006104        0.993283   \n",
       "200497  0.024281   0.051047  0.076014  ...    0.001828        0.991111   \n",
       "258687  0.021201   0.043132  0.071010  ...    0.001458        0.990960   \n",
       "\n",
       "        energyWavelet  meanSpectrogram  varSpectrogram  entropySpectrogram  \\\n",
       "255723       0.005880         0.002727    2.272309e-06            0.000031   \n",
       "207040       0.009018         0.004295    5.222147e-06            0.000068   \n",
       "254381       0.001616         0.000722    6.144647e-07            0.000008   \n",
       "202856       0.007109         0.003369    3.546623e-06            0.000046   \n",
       "212285       0.010887         0.005161    8.933521e-06            0.000109   \n",
       "...               ...              ...             ...                 ...   \n",
       "240598       0.001669         0.000843    6.234846e-07            0.000008   \n",
       "202298       0.005549         0.002682    2.248988e-06            0.000031   \n",
       "206916       0.006661         0.003247    3.320081e-06            0.000044   \n",
       "200497       0.001748         0.000831    5.012502e-07            0.000007   \n",
       "258687       0.001389         0.000688    3.881280e-07            0.000005   \n",
       "\n",
       "        energySpectrogram    Label  prediction_label  prediction_score  \n",
       "255723       2.403533e-06  healthy           damaged              0.67  \n",
       "207040       5.558231e-06  healthy           damaged              0.67  \n",
       "254381       6.121063e-07  healthy           damaged              0.87  \n",
       "202856       3.744839e-06  healthy           damaged              0.86  \n",
       "212285       9.383444e-06  damaged           healthy              0.75  \n",
       "...                   ...      ...               ...               ...  \n",
       "240598       6.257166e-07  damaged           healthy              0.58  \n",
       "202298       2.374636e-06  healthy           damaged              0.59  \n",
       "206916       3.503566e-06  healthy           damaged              0.88  \n",
       "200497       5.060598e-07  healthy           damaged              0.59  \n",
       "258687       3.902922e-07  healthy           damaged              0.54  \n",
       "\n",
       "[1284 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc359a-4de6-48f8-a573-41c6d084a7ca",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "811f8d9f-491d-4f38-9ce9-e06344b3ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_45395_row10_col0, #T_45395_row10_col1, #T_45395_row10_col2, #T_45395_row10_col3, #T_45395_row10_col4, #T_45395_row10_col5, #T_45395_row10_col6, #T_45395_row10_col7, #T_45395_row10_col8, #T_45395_row10_col9, #T_45395_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_45395\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_45395_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_45395_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_45395_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_45395_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_45395_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_45395_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_45395_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_45395_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_45395_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_45395_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_45395_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_45395_row0_col0\" class=\"data row0 col0\" >0.8371</td>\n",
       "      <td id=\"T_45395_row0_col1\" class=\"data row0 col1\" >0.8319</td>\n",
       "      <td id=\"T_45395_row0_col2\" class=\"data row0 col2\" >0.8371</td>\n",
       "      <td id=\"T_45395_row0_col3\" class=\"data row0 col3\" >0.8384</td>\n",
       "      <td id=\"T_45395_row0_col4\" class=\"data row0 col4\" >0.8377</td>\n",
       "      <td id=\"T_45395_row0_col5\" class=\"data row0 col5\" >0.5369</td>\n",
       "      <td id=\"T_45395_row0_col6\" class=\"data row0 col6\" >0.5369</td>\n",
       "      <td id=\"T_45395_row0_col7\" class=\"data row0 col7\" >0.7704</td>\n",
       "      <td id=\"T_45395_row0_col8\" class=\"data row0 col8\" >0.1629</td>\n",
       "      <td id=\"T_45395_row0_col9\" class=\"data row0 col9\" >0.4731</td>\n",
       "      <td id=\"T_45395_row0_col10\" class=\"data row0 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_45395_row1_col0\" class=\"data row1 col0\" >0.8502</td>\n",
       "      <td id=\"T_45395_row1_col1\" class=\"data row1 col1\" >0.8260</td>\n",
       "      <td id=\"T_45395_row1_col2\" class=\"data row1 col2\" >0.8502</td>\n",
       "      <td id=\"T_45395_row1_col3\" class=\"data row1 col3\" >0.8414</td>\n",
       "      <td id=\"T_45395_row1_col4\" class=\"data row1 col4\" >0.8413</td>\n",
       "      <td id=\"T_45395_row1_col5\" class=\"data row1 col5\" >0.5254</td>\n",
       "      <td id=\"T_45395_row1_col6\" class=\"data row1 col6\" >0.5354</td>\n",
       "      <td id=\"T_45395_row1_col7\" class=\"data row1 col7\" >0.7375</td>\n",
       "      <td id=\"T_45395_row1_col8\" class=\"data row1 col8\" >0.1498</td>\n",
       "      <td id=\"T_45395_row1_col9\" class=\"data row1 col9\" >0.4446</td>\n",
       "      <td id=\"T_45395_row1_col10\" class=\"data row1 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_45395_row2_col0\" class=\"data row2 col0\" >0.8415</td>\n",
       "      <td id=\"T_45395_row2_col1\" class=\"data row2 col1\" >0.8229</td>\n",
       "      <td id=\"T_45395_row2_col2\" class=\"data row2 col2\" >0.8415</td>\n",
       "      <td id=\"T_45395_row2_col3\" class=\"data row2 col3\" >0.8373</td>\n",
       "      <td id=\"T_45395_row2_col4\" class=\"data row2 col4\" >0.8390</td>\n",
       "      <td id=\"T_45395_row2_col5\" class=\"data row2 col5\" >0.5321</td>\n",
       "      <td id=\"T_45395_row2_col6\" class=\"data row2 col6\" >0.5329</td>\n",
       "      <td id=\"T_45395_row2_col7\" class=\"data row2 col7\" >0.7583</td>\n",
       "      <td id=\"T_45395_row2_col8\" class=\"data row2 col8\" >0.1585</td>\n",
       "      <td id=\"T_45395_row2_col9\" class=\"data row2 col9\" >0.4631</td>\n",
       "      <td id=\"T_45395_row2_col10\" class=\"data row2 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_45395_row3_col0\" class=\"data row3 col0\" >0.8359</td>\n",
       "      <td id=\"T_45395_row3_col1\" class=\"data row3 col1\" >0.8205</td>\n",
       "      <td id=\"T_45395_row3_col2\" class=\"data row3 col2\" >0.8359</td>\n",
       "      <td id=\"T_45395_row3_col3\" class=\"data row3 col3\" >0.8338</td>\n",
       "      <td id=\"T_45395_row3_col4\" class=\"data row3 col4\" >0.8348</td>\n",
       "      <td id=\"T_45395_row3_col5\" class=\"data row3 col5\" >0.5235</td>\n",
       "      <td id=\"T_45395_row3_col6\" class=\"data row3 col6\" >0.5237</td>\n",
       "      <td id=\"T_45395_row3_col7\" class=\"data row3 col7\" >0.7583</td>\n",
       "      <td id=\"T_45395_row3_col8\" class=\"data row3 col8\" >0.1641</td>\n",
       "      <td id=\"T_45395_row3_col9\" class=\"data row3 col9\" >0.4586</td>\n",
       "      <td id=\"T_45395_row3_col10\" class=\"data row3 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_45395_row4_col0\" class=\"data row4 col0\" >0.8430</td>\n",
       "      <td id=\"T_45395_row4_col1\" class=\"data row4 col1\" >0.8288</td>\n",
       "      <td id=\"T_45395_row4_col2\" class=\"data row4 col2\" >0.8430</td>\n",
       "      <td id=\"T_45395_row4_col3\" class=\"data row4 col3\" >0.8406</td>\n",
       "      <td id=\"T_45395_row4_col4\" class=\"data row4 col4\" >0.8417</td>\n",
       "      <td id=\"T_45395_row4_col5\" class=\"data row4 col5\" >0.5429</td>\n",
       "      <td id=\"T_45395_row4_col6\" class=\"data row4 col6\" >0.5431</td>\n",
       "      <td id=\"T_45395_row4_col7\" class=\"data row4 col7\" >0.7671</td>\n",
       "      <td id=\"T_45395_row4_col8\" class=\"data row4 col8\" >0.1570</td>\n",
       "      <td id=\"T_45395_row4_col9\" class=\"data row4 col9\" >0.4744</td>\n",
       "      <td id=\"T_45395_row4_col10\" class=\"data row4 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_45395_row5_col0\" class=\"data row5 col0\" >0.8312</td>\n",
       "      <td id=\"T_45395_row5_col1\" class=\"data row5 col1\" >0.8241</td>\n",
       "      <td id=\"T_45395_row5_col2\" class=\"data row5 col2\" >0.8312</td>\n",
       "      <td id=\"T_45395_row5_col3\" class=\"data row5 col3\" >0.8316</td>\n",
       "      <td id=\"T_45395_row5_col4\" class=\"data row5 col4\" >0.8314</td>\n",
       "      <td id=\"T_45395_row5_col5\" class=\"data row5 col5\" >0.5177</td>\n",
       "      <td id=\"T_45395_row5_col6\" class=\"data row5 col6\" >0.5177</td>\n",
       "      <td id=\"T_45395_row5_col7\" class=\"data row5 col7\" >0.7595</td>\n",
       "      <td id=\"T_45395_row5_col8\" class=\"data row5 col8\" >0.1688</td>\n",
       "      <td id=\"T_45395_row5_col9\" class=\"data row5 col9\" >0.4564</td>\n",
       "      <td id=\"T_45395_row5_col10\" class=\"data row5 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_45395_row6_col0\" class=\"data row6 col0\" >0.8424</td>\n",
       "      <td id=\"T_45395_row6_col1\" class=\"data row6 col1\" >0.8242</td>\n",
       "      <td id=\"T_45395_row6_col2\" class=\"data row6 col2\" >0.8424</td>\n",
       "      <td id=\"T_45395_row6_col3\" class=\"data row6 col3\" >0.8383</td>\n",
       "      <td id=\"T_45395_row6_col4\" class=\"data row6 col4\" >0.8400</td>\n",
       "      <td id=\"T_45395_row6_col5\" class=\"data row6 col5\" >0.5352</td>\n",
       "      <td id=\"T_45395_row6_col6\" class=\"data row6 col6\" >0.5359</td>\n",
       "      <td id=\"T_45395_row6_col7\" class=\"data row6 col7\" >0.7599</td>\n",
       "      <td id=\"T_45395_row6_col8\" class=\"data row6 col8\" >0.1576</td>\n",
       "      <td id=\"T_45395_row6_col9\" class=\"data row6 col9\" >0.4657</td>\n",
       "      <td id=\"T_45395_row6_col10\" class=\"data row6 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_45395_row7_col0\" class=\"data row7 col0\" >0.8327</td>\n",
       "      <td id=\"T_45395_row7_col1\" class=\"data row7 col1\" >0.8218</td>\n",
       "      <td id=\"T_45395_row7_col2\" class=\"data row7 col2\" >0.8327</td>\n",
       "      <td id=\"T_45395_row7_col3\" class=\"data row7 col3\" >0.8324</td>\n",
       "      <td id=\"T_45395_row7_col4\" class=\"data row7 col4\" >0.8325</td>\n",
       "      <td id=\"T_45395_row7_col5\" class=\"data row7 col5\" >0.5199</td>\n",
       "      <td id=\"T_45395_row7_col6\" class=\"data row7 col6\" >0.5199</td>\n",
       "      <td id=\"T_45395_row7_col7\" class=\"data row7 col7\" >0.7595</td>\n",
       "      <td id=\"T_45395_row7_col8\" class=\"data row7 col8\" >0.1673</td>\n",
       "      <td id=\"T_45395_row7_col9\" class=\"data row7 col9\" >0.4575</td>\n",
       "      <td id=\"T_45395_row7_col10\" class=\"data row7 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_45395_row8_col0\" class=\"data row8 col0\" >0.8379</td>\n",
       "      <td id=\"T_45395_row8_col1\" class=\"data row8 col1\" >0.8253</td>\n",
       "      <td id=\"T_45395_row8_col2\" class=\"data row8 col2\" >0.8379</td>\n",
       "      <td id=\"T_45395_row8_col3\" class=\"data row8 col3\" >0.8366</td>\n",
       "      <td id=\"T_45395_row8_col4\" class=\"data row8 col4\" >0.8372</td>\n",
       "      <td id=\"T_45395_row8_col5\" class=\"data row8 col5\" >0.5319</td>\n",
       "      <td id=\"T_45395_row8_col6\" class=\"data row8 col6\" >0.5319</td>\n",
       "      <td id=\"T_45395_row8_col7\" class=\"data row8 col7\" >0.7637</td>\n",
       "      <td id=\"T_45395_row8_col8\" class=\"data row8 col8\" >0.1621</td>\n",
       "      <td id=\"T_45395_row8_col9\" class=\"data row8 col9\" >0.4664</td>\n",
       "      <td id=\"T_45395_row8_col10\" class=\"data row8 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_45395_row9_col0\" class=\"data row9 col0\" >0.8393</td>\n",
       "      <td id=\"T_45395_row9_col1\" class=\"data row9 col1\" >0.8279</td>\n",
       "      <td id=\"T_45395_row9_col2\" class=\"data row9 col2\" >0.8393</td>\n",
       "      <td id=\"T_45395_row9_col3\" class=\"data row9 col3\" >0.8377</td>\n",
       "      <td id=\"T_45395_row9_col4\" class=\"data row9 col4\" >0.8385</td>\n",
       "      <td id=\"T_45395_row9_col5\" class=\"data row9 col5\" >0.5349</td>\n",
       "      <td id=\"T_45395_row9_col6\" class=\"data row9 col6\" >0.5350</td>\n",
       "      <td id=\"T_45395_row9_col7\" class=\"data row9 col7\" >0.7646</td>\n",
       "      <td id=\"T_45395_row9_col8\" class=\"data row9 col8\" >0.1607</td>\n",
       "      <td id=\"T_45395_row9_col9\" class=\"data row9 col9\" >0.4685</td>\n",
       "      <td id=\"T_45395_row9_col10\" class=\"data row9 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_45395_row10_col0\" class=\"data row10 col0\" >0.8391</td>\n",
       "      <td id=\"T_45395_row10_col1\" class=\"data row10 col1\" >0.8254</td>\n",
       "      <td id=\"T_45395_row10_col2\" class=\"data row10 col2\" >0.8391</td>\n",
       "      <td id=\"T_45395_row10_col3\" class=\"data row10 col3\" >0.8368</td>\n",
       "      <td id=\"T_45395_row10_col4\" class=\"data row10 col4\" >0.8374</td>\n",
       "      <td id=\"T_45395_row10_col5\" class=\"data row10 col5\" >0.5300</td>\n",
       "      <td id=\"T_45395_row10_col6\" class=\"data row10 col6\" >0.5313</td>\n",
       "      <td id=\"T_45395_row10_col7\" class=\"data row10 col7\" >0.7599</td>\n",
       "      <td id=\"T_45395_row10_col8\" class=\"data row10 col8\" >0.1609</td>\n",
       "      <td id=\"T_45395_row10_col9\" class=\"data row10 col9\" >0.4628</td>\n",
       "      <td id=\"T_45395_row10_col10\" class=\"data row10 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45395_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_45395_row11_col0\" class=\"data row11 col0\" >0.0052</td>\n",
       "      <td id=\"T_45395_row11_col1\" class=\"data row11 col1\" >0.0033</td>\n",
       "      <td id=\"T_45395_row11_col2\" class=\"data row11 col2\" >0.0052</td>\n",
       "      <td id=\"T_45395_row11_col3\" class=\"data row11 col3\" >0.0031</td>\n",
       "      <td id=\"T_45395_row11_col4\" class=\"data row11 col4\" >0.0033</td>\n",
       "      <td id=\"T_45395_row11_col5\" class=\"data row11 col5\" >0.0077</td>\n",
       "      <td id=\"T_45395_row11_col6\" class=\"data row11 col6\" >0.0077</td>\n",
       "      <td id=\"T_45395_row11_col7\" class=\"data row11 col7\" >0.0084</td>\n",
       "      <td id=\"T_45395_row11_col8\" class=\"data row11 col8\" >0.0052</td>\n",
       "      <td id=\"T_45395_row11_col9\" class=\"data row11 col9\" >0.0084</td>\n",
       "      <td id=\"T_45395_row11_col10\" class=\"data row11 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a5a85fd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:26:13 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "svm = create_model('svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fd446a9-4a12-4741-ba02-e0ca04b335d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_19f38_row10_col0, #T_19f38_row10_col1, #T_19f38_row10_col2, #T_19f38_row10_col3, #T_19f38_row10_col4, #T_19f38_row10_col5, #T_19f38_row10_col6, #T_19f38_row10_col7, #T_19f38_row10_col8, #T_19f38_row10_col9, #T_19f38_row10_col10 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_19f38\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_19f38_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_19f38_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_19f38_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_19f38_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_19f38_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_19f38_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_19f38_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_19f38_level0_col7\" class=\"col_heading level0 col7\" >Balance Acc</th>\n",
       "      <th id=\"T_19f38_level0_col8\" class=\"col_heading level0 col8\" >Hamming Loss</th>\n",
       "      <th id=\"T_19f38_level0_col9\" class=\"col_heading level0 col9\" >Jaccard Score</th>\n",
       "      <th id=\"T_19f38_level0_col10\" class=\"col_heading level0 col10\" >Log Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_19f38_row0_col0\" class=\"data row0 col0\" >0.8792</td>\n",
       "      <td id=\"T_19f38_row0_col1\" class=\"data row0 col1\" >0.8751</td>\n",
       "      <td id=\"T_19f38_row0_col2\" class=\"data row0 col2\" >0.8792</td>\n",
       "      <td id=\"T_19f38_row0_col3\" class=\"data row0 col3\" >0.8769</td>\n",
       "      <td id=\"T_19f38_row0_col4\" class=\"data row0 col4\" >0.8694</td>\n",
       "      <td id=\"T_19f38_row0_col5\" class=\"data row0 col5\" >0.6056</td>\n",
       "      <td id=\"T_19f38_row0_col6\" class=\"data row0 col6\" >0.6261</td>\n",
       "      <td id=\"T_19f38_row0_col7\" class=\"data row0 col7\" >0.7657</td>\n",
       "      <td id=\"T_19f38_row0_col8\" class=\"data row0 col8\" >0.1208</td>\n",
       "      <td id=\"T_19f38_row0_col9\" class=\"data row0 col9\" >0.5105</td>\n",
       "      <td id=\"T_19f38_row0_col10\" class=\"data row0 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_19f38_row1_col0\" class=\"data row1 col0\" >0.8498</td>\n",
       "      <td id=\"T_19f38_row1_col1\" class=\"data row1 col1\" >0.8689</td>\n",
       "      <td id=\"T_19f38_row1_col2\" class=\"data row1 col2\" >0.8498</td>\n",
       "      <td id=\"T_19f38_row1_col3\" class=\"data row1 col3\" >0.8557</td>\n",
       "      <td id=\"T_19f38_row1_col4\" class=\"data row1 col4\" >0.8253</td>\n",
       "      <td id=\"T_19f38_row1_col5\" class=\"data row1 col5\" >0.4595</td>\n",
       "      <td id=\"T_19f38_row1_col6\" class=\"data row1 col6\" >0.5192</td>\n",
       "      <td id=\"T_19f38_row1_col7\" class=\"data row1 col7\" >0.6830</td>\n",
       "      <td id=\"T_19f38_row1_col8\" class=\"data row1 col8\" >0.1502</td>\n",
       "      <td id=\"T_19f38_row1_col9\" class=\"data row1 col9\" >0.3626</td>\n",
       "      <td id=\"T_19f38_row1_col10\" class=\"data row1 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_19f38_row2_col0\" class=\"data row2 col0\" >0.8542</td>\n",
       "      <td id=\"T_19f38_row2_col1\" class=\"data row2 col1\" >0.8666</td>\n",
       "      <td id=\"T_19f38_row2_col2\" class=\"data row2 col2\" >0.8542</td>\n",
       "      <td id=\"T_19f38_row2_col3\" class=\"data row2 col3\" >0.8570</td>\n",
       "      <td id=\"T_19f38_row2_col4\" class=\"data row2 col4\" >0.8331</td>\n",
       "      <td id=\"T_19f38_row2_col5\" class=\"data row2 col5\" >0.4855</td>\n",
       "      <td id=\"T_19f38_row2_col6\" class=\"data row2 col6\" >0.5353</td>\n",
       "      <td id=\"T_19f38_row2_col7\" class=\"data row2 col7\" >0.6971</td>\n",
       "      <td id=\"T_19f38_row2_col8\" class=\"data row2 col8\" >0.1458</td>\n",
       "      <td id=\"T_19f38_row2_col9\" class=\"data row2 col9\" >0.3885</td>\n",
       "      <td id=\"T_19f38_row2_col10\" class=\"data row2 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_19f38_row3_col0\" class=\"data row3 col0\" >0.8576</td>\n",
       "      <td id=\"T_19f38_row3_col1\" class=\"data row3 col1\" >0.8652</td>\n",
       "      <td id=\"T_19f38_row3_col2\" class=\"data row3 col2\" >0.8576</td>\n",
       "      <td id=\"T_19f38_row3_col3\" class=\"data row3 col3\" >0.8597</td>\n",
       "      <td id=\"T_19f38_row3_col4\" class=\"data row3 col4\" >0.8383</td>\n",
       "      <td id=\"T_19f38_row3_col5\" class=\"data row3 col5\" >0.5027</td>\n",
       "      <td id=\"T_19f38_row3_col6\" class=\"data row3 col6\" >0.5483</td>\n",
       "      <td id=\"T_19f38_row3_col7\" class=\"data row3 col7\" >0.7061</td>\n",
       "      <td id=\"T_19f38_row3_col8\" class=\"data row3 col8\" >0.1424</td>\n",
       "      <td id=\"T_19f38_row3_col9\" class=\"data row3 col9\" >0.4051</td>\n",
       "      <td id=\"T_19f38_row3_col10\" class=\"data row3 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_19f38_row4_col0\" class=\"data row4 col0\" >0.8674</td>\n",
       "      <td id=\"T_19f38_row4_col1\" class=\"data row4 col1\" >0.8759</td>\n",
       "      <td id=\"T_19f38_row4_col2\" class=\"data row4 col2\" >0.8674</td>\n",
       "      <td id=\"T_19f38_row4_col3\" class=\"data row4 col3\" >0.8710</td>\n",
       "      <td id=\"T_19f38_row4_col4\" class=\"data row4 col4\" >0.8507</td>\n",
       "      <td id=\"T_19f38_row4_col5\" class=\"data row4 col5\" >0.5418</td>\n",
       "      <td id=\"T_19f38_row4_col6\" class=\"data row4 col6\" >0.5844</td>\n",
       "      <td id=\"T_19f38_row4_col7\" class=\"data row4 col7\" >0.7246</td>\n",
       "      <td id=\"T_19f38_row4_col8\" class=\"data row4 col8\" >0.1326</td>\n",
       "      <td id=\"T_19f38_row4_col9\" class=\"data row4 col9\" >0.4412</td>\n",
       "      <td id=\"T_19f38_row4_col10\" class=\"data row4 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_19f38_row5_col0\" class=\"data row5 col0\" >0.8757</td>\n",
       "      <td id=\"T_19f38_row5_col1\" class=\"data row5 col1\" >0.8707</td>\n",
       "      <td id=\"T_19f38_row5_col2\" class=\"data row5 col2\" >0.8757</td>\n",
       "      <td id=\"T_19f38_row5_col3\" class=\"data row5 col3\" >0.8709</td>\n",
       "      <td id=\"T_19f38_row5_col4\" class=\"data row5 col4\" >0.8681</td>\n",
       "      <td id=\"T_19f38_row5_col5\" class=\"data row5 col5\" >0.6049</td>\n",
       "      <td id=\"T_19f38_row5_col6\" class=\"data row5 col6\" >0.6174</td>\n",
       "      <td id=\"T_19f38_row5_col7\" class=\"data row5 col7\" >0.7724</td>\n",
       "      <td id=\"T_19f38_row5_col8\" class=\"data row5 col8\" >0.1243</td>\n",
       "      <td id=\"T_19f38_row5_col9\" class=\"data row5 col9\" >0.5145</td>\n",
       "      <td id=\"T_19f38_row5_col10\" class=\"data row5 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_19f38_row6_col0\" class=\"data row6 col0\" >0.8563</td>\n",
       "      <td id=\"T_19f38_row6_col1\" class=\"data row6 col1\" >0.8685</td>\n",
       "      <td id=\"T_19f38_row6_col2\" class=\"data row6 col2\" >0.8563</td>\n",
       "      <td id=\"T_19f38_row6_col3\" class=\"data row6 col3\" >0.8588</td>\n",
       "      <td id=\"T_19f38_row6_col4\" class=\"data row6 col4\" >0.8363</td>\n",
       "      <td id=\"T_19f38_row6_col5\" class=\"data row6 col5\" >0.4960</td>\n",
       "      <td id=\"T_19f38_row6_col6\" class=\"data row6 col6\" >0.5435</td>\n",
       "      <td id=\"T_19f38_row6_col7\" class=\"data row6 col7\" >0.7025</td>\n",
       "      <td id=\"T_19f38_row6_col8\" class=\"data row6 col8\" >0.1437</td>\n",
       "      <td id=\"T_19f38_row6_col9\" class=\"data row6 col9\" >0.3985</td>\n",
       "      <td id=\"T_19f38_row6_col10\" class=\"data row6 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_19f38_row7_col0\" class=\"data row7 col0\" >0.8727</td>\n",
       "      <td id=\"T_19f38_row7_col1\" class=\"data row7 col1\" >0.8668</td>\n",
       "      <td id=\"T_19f38_row7_col2\" class=\"data row7 col2\" >0.8727</td>\n",
       "      <td id=\"T_19f38_row7_col3\" class=\"data row7 col3\" >0.8677</td>\n",
       "      <td id=\"T_19f38_row7_col4\" class=\"data row7 col4\" >0.8641</td>\n",
       "      <td id=\"T_19f38_row7_col5\" class=\"data row7 col5\" >0.5918</td>\n",
       "      <td id=\"T_19f38_row7_col6\" class=\"data row7 col6\" >0.6062</td>\n",
       "      <td id=\"T_19f38_row7_col7\" class=\"data row7 col7\" >0.7644</td>\n",
       "      <td id=\"T_19f38_row7_col8\" class=\"data row7 col8\" >0.1273</td>\n",
       "      <td id=\"T_19f38_row7_col9\" class=\"data row7 col9\" >0.5010</td>\n",
       "      <td id=\"T_19f38_row7_col10\" class=\"data row7 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_19f38_row8_col0\" class=\"data row8 col0\" >0.8755</td>\n",
       "      <td id=\"T_19f38_row8_col1\" class=\"data row8 col1\" >0.8688</td>\n",
       "      <td id=\"T_19f38_row8_col2\" class=\"data row8 col2\" >0.8755</td>\n",
       "      <td id=\"T_19f38_row8_col3\" class=\"data row8 col3\" >0.8702</td>\n",
       "      <td id=\"T_19f38_row8_col4\" class=\"data row8 col4\" >0.8701</td>\n",
       "      <td id=\"T_19f38_row8_col5\" class=\"data row8 col5\" >0.6150</td>\n",
       "      <td id=\"T_19f38_row8_col6\" class=\"data row8 col6\" >0.6216</td>\n",
       "      <td id=\"T_19f38_row8_col7\" class=\"data row8 col7\" >0.7848</td>\n",
       "      <td id=\"T_19f38_row8_col8\" class=\"data row8 col8\" >0.1245</td>\n",
       "      <td id=\"T_19f38_row8_col9\" class=\"data row8 col9\" >0.5286</td>\n",
       "      <td id=\"T_19f38_row8_col10\" class=\"data row8 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_19f38_row9_col0\" class=\"data row9 col0\" >0.8698</td>\n",
       "      <td id=\"T_19f38_row9_col1\" class=\"data row9 col1\" >0.8704</td>\n",
       "      <td id=\"T_19f38_row9_col2\" class=\"data row9 col2\" >0.8698</td>\n",
       "      <td id=\"T_19f38_row9_col3\" class=\"data row9 col3\" >0.8646</td>\n",
       "      <td id=\"T_19f38_row9_col4\" class=\"data row9 col4\" >0.8657</td>\n",
       "      <td id=\"T_19f38_row9_col5\" class=\"data row9 col5\" >0.6049</td>\n",
       "      <td id=\"T_19f38_row9_col6\" class=\"data row9 col6\" >0.6085</td>\n",
       "      <td id=\"T_19f38_row9_col7\" class=\"data row9 col7\" >0.7855</td>\n",
       "      <td id=\"T_19f38_row9_col8\" class=\"data row9 col8\" >0.1302</td>\n",
       "      <td id=\"T_19f38_row9_col9\" class=\"data row9 col9\" >0.5224</td>\n",
       "      <td id=\"T_19f38_row9_col10\" class=\"data row9 col10\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_19f38_row10_col0\" class=\"data row10 col0\" >0.8658</td>\n",
       "      <td id=\"T_19f38_row10_col1\" class=\"data row10 col1\" >0.8697</td>\n",
       "      <td id=\"T_19f38_row10_col2\" class=\"data row10 col2\" >0.8658</td>\n",
       "      <td id=\"T_19f38_row10_col3\" class=\"data row10 col3\" >0.8652</td>\n",
       "      <td id=\"T_19f38_row10_col4\" class=\"data row10 col4\" >0.8521</td>\n",
       "      <td id=\"T_19f38_row10_col5\" class=\"data row10 col5\" >0.5508</td>\n",
       "      <td id=\"T_19f38_row10_col6\" class=\"data row10 col6\" >0.5810</td>\n",
       "      <td id=\"T_19f38_row10_col7\" class=\"data row10 col7\" >0.7386</td>\n",
       "      <td id=\"T_19f38_row10_col8\" class=\"data row10 col8\" >0.1342</td>\n",
       "      <td id=\"T_19f38_row10_col9\" class=\"data row10 col9\" >0.4573</td>\n",
       "      <td id=\"T_19f38_row10_col10\" class=\"data row10 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f38_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_19f38_row11_col0\" class=\"data row11 col0\" >0.0099</td>\n",
       "      <td id=\"T_19f38_row11_col1\" class=\"data row11 col1\" >0.0033</td>\n",
       "      <td id=\"T_19f38_row11_col2\" class=\"data row11 col2\" >0.0099</td>\n",
       "      <td id=\"T_19f38_row11_col3\" class=\"data row11 col3\" >0.0068</td>\n",
       "      <td id=\"T_19f38_row11_col4\" class=\"data row11 col4\" >0.0165</td>\n",
       "      <td id=\"T_19f38_row11_col5\" class=\"data row11 col5\" >0.0571</td>\n",
       "      <td id=\"T_19f38_row11_col6\" class=\"data row11 col6\" >0.0385</td>\n",
       "      <td id=\"T_19f38_row11_col7\" class=\"data row11 col7\" >0.0377</td>\n",
       "      <td id=\"T_19f38_row11_col8\" class=\"data row11 col8\" >0.0099</td>\n",
       "      <td id=\"T_19f38_row11_col9\" class=\"data row11 col9\" >0.0612</td>\n",
       "      <td id=\"T_19f38_row11_col10\" class=\"data row11 col10\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a5a9a80d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 18:27:29 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "svm_tuned_model, svm_tuner = tune_model(svm, search_library = 'optuna', return_tuner=True, n_iter=num_iterations_tuning, optimize=optimized_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "459771ad-d31e-4e53-aedc-578b117a8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=1.5824807951899164e-05, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0015152815364764712,\n",
      "              fit_intercept=False, l1_ratio=0.7894968232784667,\n",
      "              learning_rate='optimal', loss='hinge', max_iter=1000,\n",
      "              n_iter_no_change=5, n_jobs=-1, penalty='l1', power_t=0.5,\n",
      "              random_state=5991, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(svm_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13975b65-4bce-412d-ae0c-f163ab5a76fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ce205\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ce205_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_ce205_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_ce205_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_ce205_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_ce205_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_ce205_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_ce205_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_ce205_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_ce205_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_ce205_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_ce205_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_ce205_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ce205_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ce205_row0_col0\" class=\"data row0 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_ce205_row0_col1\" class=\"data row0 col1\" >0.8790</td>\n",
       "      <td id=\"T_ce205_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_ce205_row0_col3\" class=\"data row0 col3\" >0.8790</td>\n",
       "      <td id=\"T_ce205_row0_col4\" class=\"data row0 col4\" >0.8747</td>\n",
       "      <td id=\"T_ce205_row0_col5\" class=\"data row0 col5\" >0.8714</td>\n",
       "      <td id=\"T_ce205_row0_col6\" class=\"data row0 col6\" >0.6150</td>\n",
       "      <td id=\"T_ce205_row0_col7\" class=\"data row0 col7\" >0.6279</td>\n",
       "      <td id=\"T_ce205_row0_col8\" class=\"data row0 col8\" >0.7768</td>\n",
       "      <td id=\"T_ce205_row0_col9\" class=\"data row0 col9\" >0.1210</td>\n",
       "      <td id=\"T_ce205_row0_col10\" class=\"data row0 col10\" >0.5239</td>\n",
       "      <td id=\"T_ce205_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a0bfb3d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_svm = predict_model(svm_tuned_model, data = features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d6dd58a-03ef-438f-9194-e28dc7e12d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835936b0eb8a4510b9b446ae20051554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9ad3a02-579f-47c2-97b0-1dd8a2079bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clearance</td>\n",
       "      <td>8.123575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skewness</td>\n",
       "      <td>5.706661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spectral_flatness</td>\n",
       "      <td>4.921232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectral_kurtosis</td>\n",
       "      <td>4.793021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meanWavelet</td>\n",
       "      <td>4.465211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spectral_skewness</td>\n",
       "      <td>4.383496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>3.915387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>medianFreq</td>\n",
       "      <td>3.805859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ptp</td>\n",
       "      <td>3.391994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rms</td>\n",
       "      <td>2.846113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meanFreq</td>\n",
       "      <td>2.756911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>std</td>\n",
       "      <td>2.499976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bandwidth</td>\n",
       "      <td>2.472553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crest</td>\n",
       "      <td>2.111543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>energyWavelet</td>\n",
       "      <td>1.575774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>varWavelet</td>\n",
       "      <td>1.529095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>impulse</td>\n",
       "      <td>0.777286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>meanSpectrogram</td>\n",
       "      <td>0.663641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>energy</td>\n",
       "      <td>0.493525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kurtosis</td>\n",
       "      <td>0.335378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shape</td>\n",
       "      <td>0.282589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.179485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.126031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>entropyWavelet</td>\n",
       "      <td>0.092582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>varSpectrogram</td>\n",
       "      <td>0.015955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>energySpectrogram</td>\n",
       "      <td>0.015832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>entropySpectrogram</td>\n",
       "      <td>0.015288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features  importance\n",
       "0            clearance    8.123575\n",
       "1             skewness    5.706661\n",
       "2    spectral_flatness    4.921232\n",
       "3    spectral_kurtosis    4.793021\n",
       "4          meanWavelet    4.465211\n",
       "5    spectral_skewness    4.383496\n",
       "6     spectral_entropy    3.915387\n",
       "7           medianFreq    3.805859\n",
       "8                  ptp    3.391994\n",
       "9                  rms    2.846113\n",
       "10            meanFreq    2.756911\n",
       "11                 std    2.499976\n",
       "12           bandwidth    2.472553\n",
       "13               crest    2.111543\n",
       "14       energyWavelet    1.575774\n",
       "15          varWavelet    1.529095\n",
       "16             impulse    0.777286\n",
       "17     meanSpectrogram    0.663641\n",
       "18              energy    0.493525\n",
       "19            kurtosis    0.335378\n",
       "20               shape    0.282589\n",
       "21                mean    0.179485\n",
       "22             entropy    0.126031\n",
       "23      entropyWavelet    0.092582\n",
       "24      varSpectrogram    0.015955\n",
       "25   energySpectrogram    0.015832\n",
       "26  entropySpectrogram    0.015288"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_top_features = get_svm_feature_importance_df(svm, features_df_training_normalized)\n",
    "svm_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "049dda80-ba01-474f-b66d-250dcb0e8d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6020d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6020d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_6020d_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_6020d_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_6020d_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_6020d_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_6020d_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_6020d_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_6020d_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_6020d_level0_col8\" class=\"col_heading level0 col8\" >Balance Acc</th>\n",
       "      <th id=\"T_6020d_level0_col9\" class=\"col_heading level0 col9\" >Hamming Loss</th>\n",
       "      <th id=\"T_6020d_level0_col10\" class=\"col_heading level0 col10\" >Jaccard Score</th>\n",
       "      <th id=\"T_6020d_level0_col11\" class=\"col_heading level0 col11\" >Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6020d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6020d_row0_col0\" class=\"data row0 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_6020d_row0_col1\" class=\"data row0 col1\" >0.8547</td>\n",
       "      <td id=\"T_6020d_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_6020d_row0_col3\" class=\"data row0 col3\" >0.8547</td>\n",
       "      <td id=\"T_6020d_row0_col4\" class=\"data row0 col4\" >0.8468</td>\n",
       "      <td id=\"T_6020d_row0_col5\" class=\"data row0 col5\" >0.8469</td>\n",
       "      <td id=\"T_6020d_row0_col6\" class=\"data row0 col6\" >0.5437</td>\n",
       "      <td id=\"T_6020d_row0_col7\" class=\"data row0 col7\" >0.5524</td>\n",
       "      <td id=\"T_6020d_row0_col8\" class=\"data row0 col8\" >0.7478</td>\n",
       "      <td id=\"T_6020d_row0_col9\" class=\"data row0 col9\" >0.1453</td>\n",
       "      <td id=\"T_6020d_row0_col10\" class=\"data row0 col10\" >0.4619</td>\n",
       "      <td id=\"T_6020d_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23a5a98dad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_svm = predict_model(svm, data=features_df_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d4e08c3-a8c6-4423-a289-1be18b4402e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>rms</th>\n",
       "      <th>std</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>ptp</th>\n",
       "      <th>crest</th>\n",
       "      <th>impulse</th>\n",
       "      <th>clearance</th>\n",
       "      <th>shape</th>\n",
       "      <th>...</th>\n",
       "      <th>meanWavelet</th>\n",
       "      <th>varWavelet</th>\n",
       "      <th>entropyWavelet</th>\n",
       "      <th>energyWavelet</th>\n",
       "      <th>meanSpectrogram</th>\n",
       "      <th>varSpectrogram</th>\n",
       "      <th>entropySpectrogram</th>\n",
       "      <th>energySpectrogram</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118771</th>\n",
       "      <td>0.552559</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.432376</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>6.606178e-07</td>\n",
       "      <td>0.989957</td>\n",
       "      <td>9.603716e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.281038e-12</td>\n",
       "      <td>3.123863e-11</td>\n",
       "      <td>1.283364e-12</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261113</th>\n",
       "      <td>0.551990</td>\n",
       "      <td>0.066536</td>\n",
       "      <td>0.072833</td>\n",
       "      <td>0.425662</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.161186</td>\n",
       "      <td>0.077418</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>0.055364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084564</td>\n",
       "      <td>9.588812e-03</td>\n",
       "      <td>0.994078</td>\n",
       "      <td>1.059881e-02</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>7.198084e-06</td>\n",
       "      <td>9.151653e-05</td>\n",
       "      <td>7.663102e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207745</th>\n",
       "      <td>0.551983</td>\n",
       "      <td>0.064504</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>0.422450</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.129561</td>\n",
       "      <td>0.058001</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.054014</td>\n",
       "      <td>0.047083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071651</td>\n",
       "      <td>9.979729e-03</td>\n",
       "      <td>0.992818</td>\n",
       "      <td>9.959937e-03</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>1.246231e-05</td>\n",
       "      <td>1.371142e-04</td>\n",
       "      <td>1.267123e-05</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45208</th>\n",
       "      <td>0.552708</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.016141</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.033954</td>\n",
       "      <td>0.073914</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.049267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>5.208895e-04</td>\n",
       "      <td>0.990540</td>\n",
       "      <td>5.172486e-04</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>3.886344e-08</td>\n",
       "      <td>6.216078e-07</td>\n",
       "      <td>3.925577e-08</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255723</th>\n",
       "      <td>0.551969</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>0.426228</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.114917</td>\n",
       "      <td>0.071149</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.058699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>5.488139e-03</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>5.879610e-03</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>2.272309e-06</td>\n",
       "      <td>3.087725e-05</td>\n",
       "      <td>2.403533e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260610</th>\n",
       "      <td>0.552007</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>0.062526</td>\n",
       "      <td>0.424157</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.146886</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020677</td>\n",
       "      <td>0.066490</td>\n",
       "      <td>0.065090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070105</td>\n",
       "      <td>7.284229e-03</td>\n",
       "      <td>0.993342</td>\n",
       "      <td>7.811357e-03</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>4.189857e-06</td>\n",
       "      <td>5.363786e-05</td>\n",
       "      <td>4.399530e-06</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258687</th>\n",
       "      <td>0.552024</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.425718</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.079928</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.043132</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024665</td>\n",
       "      <td>1.457979e-03</td>\n",
       "      <td>0.990960</td>\n",
       "      <td>1.389366e-03</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>3.881280e-07</td>\n",
       "      <td>5.331666e-06</td>\n",
       "      <td>3.902922e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83458</th>\n",
       "      <td>0.552707</td>\n",
       "      <td>0.022745</td>\n",
       "      <td>0.025044</td>\n",
       "      <td>0.421023</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.052496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028913</td>\n",
       "      <td>1.133101e-03</td>\n",
       "      <td>0.991231</td>\n",
       "      <td>1.248235e-03</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>8.785566e-08</td>\n",
       "      <td>1.485983e-06</td>\n",
       "      <td>9.285375e-08</td>\n",
       "      <td>damaged</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205706</th>\n",
       "      <td>0.551965</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.024582</td>\n",
       "      <td>0.425170</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.063984</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.065854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>1.195106e-03</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>1.207305e-03</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>2.401605e-07</td>\n",
       "      <td>3.405097e-06</td>\n",
       "      <td>2.429444e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201271</th>\n",
       "      <td>0.551941</td>\n",
       "      <td>0.024814</td>\n",
       "      <td>0.027281</td>\n",
       "      <td>0.424357</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.061563</td>\n",
       "      <td>0.074165</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.065033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>1.545471e-03</td>\n",
       "      <td>0.991035</td>\n",
       "      <td>1.487214e-03</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>3.764377e-07</td>\n",
       "      <td>5.222327e-06</td>\n",
       "      <td>3.792063e-07</td>\n",
       "      <td>healthy</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8946 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean       rms       std  skewness  kurtosis       ptp     crest  \\\n",
       "118771  0.552559  0.000958  0.001264  0.432376  0.002516  0.002636  0.072893   \n",
       "261113  0.551990  0.066536  0.072833  0.425662  0.002485  0.161186  0.077418   \n",
       "207745  0.551983  0.064504  0.070614  0.422450  0.001964  0.129561  0.058001   \n",
       "45208   0.552708  0.014590  0.016141  0.422904  0.002285  0.033954  0.073914   \n",
       "255723  0.551969  0.049521  0.054260  0.426228  0.002760  0.114917  0.071149   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "260610  0.552007  0.057093  0.062526  0.424157  0.003021  0.146886  0.080000   \n",
       "258687  0.552024  0.023971  0.026367  0.425718  0.003892  0.063684  0.079928   \n",
       "83458   0.552707  0.022745  0.025044  0.421023  0.002244  0.047887  0.059128   \n",
       "205706  0.551965  0.022341  0.024582  0.425170  0.003709  0.063984  0.088835   \n",
       "201271  0.551941  0.024814  0.027281  0.424357  0.003260  0.061563  0.074165   \n",
       "\n",
       "         impulse  clearance     shape  ...  meanWavelet    varWavelet  \\\n",
       "118771  0.018157   0.007135  0.055219  ...     0.000994  6.606178e-07   \n",
       "261113  0.019200   0.068356  0.055364  ...     0.084564  9.588812e-03   \n",
       "207745  0.014165   0.054014  0.047083  ...     0.071651  9.979729e-03   \n",
       "45208   0.017874   0.029609  0.049267  ...     0.016197  5.208895e-04   \n",
       "255723  0.018054   0.055952  0.058699  ...     0.060751  5.488139e-03   \n",
       "...          ...        ...       ...  ...          ...           ...   \n",
       "260610  0.020677   0.066490  0.065090  ...     0.070105  7.284229e-03   \n",
       "258687  0.021201   0.043132  0.071010  ...     0.024665  1.457979e-03   \n",
       "83458   0.014821   0.032424  0.052496  ...     0.028913  1.133101e-03   \n",
       "205706  0.022831   0.044268  0.065854  ...     0.025421  1.195106e-03   \n",
       "201271  0.019299   0.041013  0.065033  ...     0.026001  1.545471e-03   \n",
       "\n",
       "        entropyWavelet  energyWavelet  meanSpectrogram  varSpectrogram  \\\n",
       "118771        0.989957   9.603716e-07         0.000001    1.281038e-12   \n",
       "261113        0.994078   1.059881e-02         0.005050    7.198084e-06   \n",
       "207745        0.992818   9.959937e-03         0.004548    1.246231e-05   \n",
       "45208         0.990540   5.172486e-04         0.000233    3.886344e-08   \n",
       "255723        0.992932   5.879610e-03         0.002727    2.272309e-06   \n",
       "...                ...            ...              ...             ...   \n",
       "260610        0.993342   7.811357e-03         0.003527    4.189857e-06   \n",
       "258687        0.990960   1.389366e-03         0.000688    3.881280e-07   \n",
       "83458         0.991231   1.248235e-03         0.000533    8.785566e-08   \n",
       "205706        0.990959   1.207305e-03         0.000591    2.401605e-07   \n",
       "201271        0.991035   1.487214e-03         0.000697    3.764377e-07   \n",
       "\n",
       "        entropySpectrogram  energySpectrogram    Label  prediction_label  \n",
       "118771        3.123863e-11       1.283364e-12  healthy           damaged  \n",
       "261113        9.151653e-05       7.663102e-06  healthy           damaged  \n",
       "207745        1.371142e-04       1.267123e-05  healthy           damaged  \n",
       "45208         6.216078e-07       3.925577e-08  damaged           healthy  \n",
       "255723        3.087725e-05       2.403533e-06  healthy           damaged  \n",
       "...                    ...                ...      ...               ...  \n",
       "260610        5.363786e-05       4.399530e-06  healthy           damaged  \n",
       "258687        5.331666e-06       3.902922e-07  healthy           damaged  \n",
       "83458         1.485983e-06       9.285375e-08  damaged           healthy  \n",
       "205706        3.405097e-06       2.429444e-07  healthy           damaged  \n",
       "201271        5.222327e-06       3.792063e-07  healthy           damaged  \n",
       "\n",
       "[8946 rows x 29 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_incorrect_predictions(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378342ee-8ffa-4f37-9937-a2051463887a",
   "metadata": {},
   "source": [
    "# Experiment Setup (DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c085121-3acd-4d2e-9fb4-5c19183fe186",
   "metadata": {},
   "source": [
    "## Configure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fee3568a-b1fa-4505-bd7f-88819ba246fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (157669, 28) | Val Shape: (39418, 28) | Test Shape: (49272, 28)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(features_df_training_normalized, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Train Shape: {train.shape} | Val Shape: {val.shape} | Test Shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41773c3d-7a5e-43b7-8682-2aea065f0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Label\"\n",
    "\n",
    "categorical_cols = [\n",
    "    col\n",
    "    for col in features_df_training_normalized.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "    if col != target\n",
    "]\n",
    "\n",
    "continuous_cols = features_df_training_normalized.select_dtypes(include=[\"number\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a28e6e44-1bea-4842-8482-9708cc2c1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Label\n",
      "Categorical inputs: []\n",
      "Continuous inputs: ['mean', 'rms', 'std', 'skewness', 'kurtosis', 'ptp', 'crest', 'impulse', 'clearance', 'shape', 'energy', 'entropy', 'meanFreq', 'medianFreq', 'bandwidth', 'spectral_flatness', 'spectral_entropy', 'spectral_skewness', 'spectral_kurtosis', 'meanWavelet', 'varWavelet', 'entropyWavelet', 'energyWavelet', 'meanSpectrogram', 'varSpectrogram', 'entropySpectrogram', 'energySpectrogram']\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:\", target)\n",
    "print(\"Categorical inputs:\", categorical_cols)  \n",
    "print(\"Continuous inputs:\", continuous_cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f339072-98d8-491e-a93c-cdcce0517bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    target=[target],\n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c70fc27-0191-47e7-b183-e78605871c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU: Yes\n"
     ]
    }
   ],
   "source": [
    "available_gpu=1 if torch.cuda.is_available() else 0\n",
    "print(f\"Available GPU: {'Yes' if available_gpu else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a562ad-72cd-4216-8f77-36c195d0f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    max_epochs=20,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "experiment_config = ExperimentConfig(\n",
    "        project_name=\"TEST\",\n",
    "        run_name=\"test\",\n",
    "        log_target=\"tensorboard\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c24af441-0171-4f3f-a221-95eedb193867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feeb2f9-8fcd-45c6-99f4-566f19f8f14e",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c76181c3-dfb9-4f2f-ae38-efd79cc13346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TabNet_Optimization(trial):\n",
    "    n_d     = trial.suggest_int(\"n_d\", 4, 64)\n",
    "    n_a     = trial.suggest_int(\"n_a\", 4, 64)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 3, 10)\n",
    "    gamma   = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
    "    embedding_dropout = trial.suggest_float(\"embedding_dropout\", 0, 1)\n",
    "    lr      = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "    \n",
    "    tabnet_config = TabNetModelConfig(\n",
    "        task=\"classification\",\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        learning_rate=lr,\n",
    "        n_independent=2,\n",
    "        metrics=[\n",
    "            \"auroc\",\n",
    "            \"recall\",\n",
    "            \"precision\",\n",
    "            \"f1_score\",\n",
    "            \"cohen_kappa\",\n",
    "            \"matthews_corrcoef\",\n",
    "            \"hamming_distance\",\n",
    "            \"jaccard_index\",\n",
    "        ],\n",
    "        metrics_prob_input=[\n",
    "            True,   # auroc\n",
    "            False,  # recall\n",
    "            False,  # precision\n",
    "            False,  # f1_score\n",
    "            False,  # cohen_kappa\n",
    "            False,  # matthews_corrcoef\n",
    "            False,  # hamming_distance\n",
    "            False,  # jaccard_index\n",
    "        ],\n",
    "        metrics_params=[\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # auroc\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # recall\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # precision\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # f1_score\n",
    "            {\"num_classes\": 2},                      # cohen_kappa\n",
    "            {},                                      # matthews_corrcoef\n",
    "            {},                                      # hamming_distance\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # jaccard_index\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tabnet_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=tabnet_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    tabnet_model.fit(train=train, validation=val)\n",
    "\n",
    "    preds_df = tabnet_model.predict(val)\n",
    "    y_pred = preds_df[\"Label_prediction\"].to_numpy()\n",
    "    y_true = val[\"Label\"].to_numpy()\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97f728e4-7cc5-4933-adfd-776aba50e437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">327</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m327\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">379</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m379\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">399</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m399\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">561</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m561\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">624</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m624\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">675</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:33\u001b[0m,\u001b[1;36m675\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7f8cc64bdb4853b618376b9e5d77ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_9539e73f-34fd-4046-b596-1b0be1001827.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_9539e73f-34fd-4046-b596-1b0be1001827.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">797</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006918309709189364</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:49\u001b[0m,\u001b[1;36m797\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.006918309709189364\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:27:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">813</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:27:49\u001b[0m,\u001b[1;36m813\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone    70.2 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone    70.2 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 70.2 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 70.2 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 203                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 70.2 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 70.2 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 203                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb190ddb8d74171858b7af7942b3062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">017</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:00\u001b[0m,\u001b[1;36m017\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">033</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:00\u001b[0m,\u001b[1;36m033\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">176</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m176\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">223</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m223\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m248\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">390</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m390\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">452</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m452\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">483</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:06\u001b[0m,\u001b[1;36m483\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06013076f1cb475098c6af718f28c98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_ec86099a-a9ac-465e-91f4-48d83d9b27e0.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_ec86099a-a9ac-465e-91f4-48d83d9b27e0.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:23\u001b[0m,\u001b[1;36m192\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:38:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:38:23\u001b[0m,\u001b[1;36m192\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     290 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     290 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 290 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 290 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 227                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 290 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 290 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 227                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f0a1f5bd3d45019ee56d9d0188f63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">158</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:18\u001b[0m,\u001b[1;36m158\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:18\u001b[0m,\u001b[1;36m160\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">470</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m470\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m512\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">542</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m542\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">690</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m690\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">777</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m777\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">823</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:26\u001b[0m,\u001b[1;36m823\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6533c630c74ad2aa844fc9f9e53518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_28186f5d-95ce-4537-854b-38c3e371a53f.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_28186f5d-95ce-4537-854b-38c3e371a53f.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">643</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005754399373371567</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:38\u001b[0m,\u001b[1;36m643\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.005754399373371567\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:45:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">650</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:45:38\u001b[0m,\u001b[1;36m650\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     356 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     356 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 356 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 356 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 131                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 356 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 356 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 131                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27385cfc54d415599204b93e97ba3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">950</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:24\u001b[0m,\u001b[1;36m950\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">952</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:24\u001b[0m,\u001b[1;36m952\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">347</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m347\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">378</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m378\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">410</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m410\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">581</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m581\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">653</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m653\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">691</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:31\u001b[0m,\u001b[1;36m691\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e04b14d01904d02ae3d3b408138daa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.05248074602497723\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_5c2f0720-db5e-4caa-bb1f-639c911246e8.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_5c2f0720-db5e-4caa-bb1f-639c911246e8.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">024</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05248074602497723</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:49\u001b[0m,\u001b[1;36m024\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.05248074602497723\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:52:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">024</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m18:52:49\u001b[0m,\u001b[1;36m024\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     191 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     191 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 191 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 191 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 227                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 191 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 191 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 227                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9fee66daed43f79319c766e02a1667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:02</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">668</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:02\u001b[0m,\u001b[1;36m668\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:02</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">668</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:02\u001b[0m,\u001b[1;36m668\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m101\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">132</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m132\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">163</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m163\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">319</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m319\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">382</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m382\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">413</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:11\u001b[0m,\u001b[1;36m413\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5337e8957d8e4c05b79e8c92aed3ae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.036307805477010104\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_fa7330cd-923e-41d3-ac5b-ccf10cf36eed.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_fa7330cd-923e-41d3-ac5b-ccf10cf36eed.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036307805477010104</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:22\u001b[0m,\u001b[1;36m170\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.036307805477010104\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:01:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:01:22\u001b[0m,\u001b[1;36m170\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     109 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     109 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 109 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 109 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 107                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 109 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 109 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 107                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c02858ee344e1f89ff18f15bfe3344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">805</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:30\u001b[0m,\u001b[1;36m805\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">805</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:30\u001b[0m,\u001b[1;36m805\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">023</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m023\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">070</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m070\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m101\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">258</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m258\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m304\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">336</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:35\u001b[0m,\u001b[1;36m336\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c454638dd4344541ba41004b4dcb16eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.04365158322401657\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_7ab27e55-fb8a-4c4b-8506-422788cb3e48.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_7ab27e55-fb8a-4c4b-8506-422788cb3e48.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04365158322401657</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:52\u001b[0m,\u001b[1;36m211\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.04365158322401657\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:08:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">227</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:08:52\u001b[0m,\u001b[1;36m227\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone    84.6 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone    84.6 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 84.6 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 84.6 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 227                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 84.6 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 84.6 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 227                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c28cee6bfce4f4ba9c20f7391bc125c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">767</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:19\u001b[0m,\u001b[1;36m767\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">767</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:19\u001b[0m,\u001b[1;36m767\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">488</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m488\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">532</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m532\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">557</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m557\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">703</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m703\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">750</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m750\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">781</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:26\u001b[0m,\u001b[1;36m781\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2956be3d0df3459ebac24040fb3d698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_f5bda70d-cd9e-48c8-803a-521bab960005.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_f5bda70d-cd9e-48c8-803a-521bab960005.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">306</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006918309709189364</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:37\u001b[0m,\u001b[1;36m306\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.006918309709189364\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:14:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">310</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:14:37\u001b[0m,\u001b[1;36m310\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone    95.0 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone    95.0 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 95.0 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 95.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 107                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 95.0 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 95.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 107                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fde59153e746118d71b124e496ad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:42</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">583</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:42\u001b[0m,\u001b[1;36m583\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:42</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">585</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:42\u001b[0m,\u001b[1;36m585\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">940</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:46\u001b[0m,\u001b[1;36m940\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">970</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:46\u001b[0m,\u001b[1;36m970\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">001</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:47\u001b[0m,\u001b[1;36m001\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">158</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:47\u001b[0m,\u001b[1;36m158\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:47\u001b[0m,\u001b[1;36m204\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">236</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:47\u001b[0m,\u001b[1;36m236\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bf7af504024d7aaf78379526b11fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.19054607179632482\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_f1ee2ac1-7ab0-420f-82f2-344aaaefe117.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_f1ee2ac1-7ab0-420f-82f2-344aaaefe117.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">757</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.19054607179632482</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:58\u001b[0m,\u001b[1;36m757\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.19054607179632482\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:20:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">757</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:20:58\u001b[0m,\u001b[1;36m757\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone    77.7 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone    77.7 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 77.7 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 77.7 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 131                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 77.7 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 77.7 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 131                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6928d1bd0f342e593a2e849c7a9352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">446</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:49\u001b[0m,\u001b[1;36m446\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">448</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:49\u001b[0m,\u001b[1;36m448\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">086</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m086\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m133\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">164</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m164\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">316</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m316\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">364</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m364\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:25:54</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">395</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:25:54\u001b[0m,\u001b[1;36m395\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150250d4b1ca4ebd9bf793c8d697d771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_28072af5-6a46-4973-b0ad-7307697541b5.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_28072af5-6a46-4973-b0ad-7307697541b5.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:26:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">212</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:26:06\u001b[0m,\u001b[1;36m212\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:26:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">228</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:26:06\u001b[0m,\u001b[1;36m228\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     238 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     238 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 238 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 238 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 131                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 238 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 238 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 131                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96527a09fd2470b970a8952cd6a2598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:51</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">886</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:51\u001b[0m,\u001b[1;36m886\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:51</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">888</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:51\u001b[0m,\u001b[1;36m888\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">638</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:57\u001b[0m,\u001b[1;36m638\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">715</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:57\u001b[0m,\u001b[1;36m715\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">748</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:57\u001b[0m,\u001b[1;36m748\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">914</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:57\u001b[0m,\u001b[1;36m914\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">981</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:57\u001b[0m,\u001b[1;36m981\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:41:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">022</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:41:58\u001b[0m,\u001b[1;36m022\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6427dfbb76494862960e60e6888709fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.017378008287493765\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_038ba92d-579c-41f8-8434-564b43bb2829.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_038ba92d-579c-41f8-8434-564b43bb2829.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:42:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">301</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017378008287493765</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:42:16\u001b[0m,\u001b[1;36m301\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.017378008287493765\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:42:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">309</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:42:16\u001b[0m,\u001b[1;36m309\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _embedding_layer  Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _backbone         TabNetBackbone     136 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Identity               0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _backbone         TabNetBackbone     136 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Identity               0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 136 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 136 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 251                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 136 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 136 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 251                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359b656a71744be6873e3a31ce1e413b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">375</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:27\u001b[0m,\u001b[1;36m375\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">375</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:27\u001b[0m,\u001b[1;36m375\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabnet_study = optuna.create_study(direction=\"maximize\")\n",
    "tabnet_study.optimize(TabNet_Optimization, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b979953b-bebb-4b1f-90c2-ff9d4ed7d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_d': 57, 'n_a': 43, 'n_steps': 4, 'gamma': 1.128222664472358, 'embedding_dropout': 0.4935754302306584, 'learning_rate': 5.994667027948612e-05}\n",
      "Best F1 score: 0.9063968516829841\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", tabnet_study.best_params)\n",
    "print(\"Best F1 score:\", tabnet_study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "612f2aa4-565e-4598-afcd-e4ee29d63f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=8, state=1, values=[0.9063968516829841], datetime_start=datetime.datetime(2025, 9, 26, 19, 25, 54, 70482), datetime_complete=datetime.datetime(2025, 9, 26, 19, 41, 57, 607393), params={'n_d': 57, 'n_a': 43, 'n_steps': 4, 'gamma': 1.128222664472358, 'embedding_dropout': 0.4935754302306584, 'learning_rate': 5.994667027948612e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_d': IntDistribution(high=64, log=False, low=4, step=1), 'n_a': IntDistribution(high=64, log=False, low=4, step=1), 'n_steps': IntDistribution(high=10, log=False, low=3, step=1), 'gamma': FloatDistribution(high=2.0, log=False, low=1.0, step=None), 'embedding_dropout': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None)}, trial_id=8, value=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet_study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a4748-8644-48c4-9cd9-0d63bc5cde54",
   "metadata": {},
   "source": [
    "## GANDALF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26bfc5a2-13df-4eea-bf2c-d2a22cce451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANDALF_Optimization(trial):\n",
    "    gflu_stages               = trial.suggest_int(\"gflu_stages\", 1, 10)\n",
    "    gflu_dropout              = trial.suggest_float(\"gflu_dropout\", 0.0, 0.5)\n",
    "    gflu_feature_init_sparsity = trial.suggest_float(\"gflu_feature_init_sparsity\", 0.1, 0.9)\n",
    "    learnable_sparsity        = trial.suggest_categorical(\"learnable_sparsity\", [True, False])\n",
    "    embedding_dropout         = trial.suggest_float(\"embedding_dropout\", 0.0, 0.5)\n",
    "    batch_norm_continuous     = trial.suggest_categorical(\"batch_norm_continuous_input\", [True, False])\n",
    "    learning_rate             = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    # 2) build Gandalf config\n",
    "    gandalf_config = GANDALFConfig(\n",
    "        task=\"classification\",\n",
    "        gflu_stages=gflu_stages,\n",
    "        gflu_dropout=gflu_dropout,\n",
    "        gflu_feature_init_sparsity=gflu_feature_init_sparsity,\n",
    "        learnable_sparsity=learnable_sparsity,\n",
    "        embedding_dropout=embedding_dropout,\n",
    "        batch_norm_continuous_input=batch_norm_continuous,\n",
    "        learning_rate=learning_rate,\n",
    "        metrics=[\n",
    "            \"auroc\",\n",
    "            \"recall\",\n",
    "            \"precision\",\n",
    "            \"f1_score\",\n",
    "            \"cohen_kappa\",\n",
    "            \"matthews_corrcoef\",\n",
    "            \"hamming_distance\",\n",
    "            \"jaccard_index\",\n",
    "        ],\n",
    "        metrics_prob_input=[\n",
    "            True,   # auroc\n",
    "            False,  # recall\n",
    "            False,  # precision\n",
    "            False,  # f1_score\n",
    "            False,  # cohen_kappa\n",
    "            False,  # matthews_corrcoef\n",
    "            False,  # hamming_distance\n",
    "            False,  # jaccard_index\n",
    "        ],\n",
    "        metrics_params=[\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # auroc\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # recall\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # precision\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # f1_score\n",
    "            {\"num_classes\": 2},                      # cohen_kappa\n",
    "            {},                                      # matthews_corrcoef\n",
    "            {},                                      # hamming_distance\n",
    "            {\"average\": \"macro\", \"num_classes\": 2},  # jaccard_index\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3) instantiate & train\n",
    "    model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=gandalf_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    model.fit(train=train, validation=val)\n",
    "\n",
    "    # 4) predict & return macro-F1\n",
    "    preds = model.predict(val)\n",
    "    y_pred = preds[\"Label_prediction\"].to_numpy()\n",
    "    y_true = val[\"Label\"].to_numpy()\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f7e9818-c3ab-4d02-90d7-8d845fb10055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m170\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m211\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">239</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m239\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">421</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m421\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">481</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m481\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">527</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:35\u001b[0m,\u001b[1;36m527\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d848d0d331b44d9fbf589ad48fc61af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_4c445a68-7c68-45b3-8e9f-8a37e268fcbc.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_4c445a68-7c68-45b3-8e9f-8a37e268fcbc.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:43\u001b[0m,\u001b[1;36m326\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:52:43</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:52:43\u001b[0m,\u001b[1;36m326\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   26.9 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer       0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   26.9 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer       0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 26.9 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 6                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 27.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 26                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 26.9 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 6                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 27.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 26                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e0d349dc6b4b4196954e1baa1cf311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">827</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:25\u001b[0m,\u001b[1;36m827\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">827</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:25\u001b[0m,\u001b[1;36m827\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">085</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m085\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">116</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m116\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">147</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m147\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">298</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m298\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">335</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m335\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">382</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:28\u001b[0m,\u001b[1;36m382\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d4f2dd6f824b6fbf1f85fa1dd6a534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_3eaae29f-3b8f-49d3-a9ec-e0008db0bef8.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_3eaae29f-3b8f-49d3-a9ec-e0008db0bef8.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">217</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:38\u001b[0m,\u001b[1;36m217\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:55:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">222</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:55:38\u001b[0m,\u001b[1;36m222\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   44.8 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   44.8 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 44.9 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 10                                                                                           \n",
       "<span style=\"font-weight: bold\">Total params</span>: 44.9 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 36                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 44.9 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 10                                                                                           \n",
       "\u001b[1mTotal params\u001b[0m: 44.9 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 36                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c63bc5f12649d28a3463ebb3646f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">809</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:53\u001b[0m,\u001b[1;36m809\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">809</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:53\u001b[0m,\u001b[1;36m809\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">702</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:56\u001b[0m,\u001b[1;36m702\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">749</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:56\u001b[0m,\u001b[1;36m749\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">765</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:56\u001b[0m,\u001b[1;36m765\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">936</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:56\u001b[0m,\u001b[1;36m936\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">982</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:56\u001b[0m,\u001b[1;36m982\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:58:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">004</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:58:57\u001b[0m,\u001b[1;36m004\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bbfff250524c218424f02a355d4db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_0ada10ed-04ba-4b3b-8d92-0901dc71e882.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_0ada10ed-04ba-4b3b-8d92-0901dc71e882.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">049</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:05\u001b[0m,\u001b[1;36m049\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:59:05</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">065</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m19:59:05\u001b[0m,\u001b[1;36m065\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   26.9 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   26.9 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 27.0 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 6                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 27.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 28                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 27.0 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 6                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 27.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 28                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76836d3f13b41f1bd32a8e730a0e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">408</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:14\u001b[0m,\u001b[1;36m408\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">408</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:14\u001b[0m,\u001b[1;36m408\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">738</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:16\u001b[0m,\u001b[1;36m738\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">785</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:16\u001b[0m,\u001b[1;36m785\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">816</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:16\u001b[0m,\u001b[1;36m816\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">991</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:16\u001b[0m,\u001b[1;36m991\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">046</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:17\u001b[0m,\u001b[1;36m046\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:17\u001b[0m,\u001b[1;36m104\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5529cb6c884a5283a3c75df1546831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_8ed06492-28c0-4cf7-863f-83cf663d4e66.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_8ed06492-28c0-4cf7-863f-83cf663d4e66.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">437</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:24\u001b[0m,\u001b[1;36m437\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:05:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">437</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:05:24\u001b[0m,\u001b[1;36m437\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   17.9 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer       0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   17.9 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer       0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 18.0 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 4                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 18.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 22                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 18.0 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 4                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 18.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 22                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b4c462e204b928ca25687b0996c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">785</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:50\u001b[0m,\u001b[1;36m785\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">785</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:50\u001b[0m,\u001b[1;36m785\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:52\u001b[0m,\u001b[1;36m788\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">835</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:52\u001b[0m,\u001b[1;36m835\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">866</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:52\u001b[0m,\u001b[1;36m866\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">027</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:53\u001b[0m,\u001b[1;36m027\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">075</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:53\u001b[0m,\u001b[1;36m075\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:07:53</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">106</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:07:53\u001b[0m,\u001b[1;36m106\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68072b9514364c77b75ddf9fcb73370c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_a5757ba3-62fc-4411-abcb-a7f2909e5228.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_a5757ba3-62fc-4411-abcb-a7f2909e5228.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:02</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">374</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:08:02\u001b[0m,\u001b[1;36m374\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:02</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">378</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:08:02\u001b[0m,\u001b[1;36m378\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   44.8 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   44.8 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 44.9 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 10                                                                                           \n",
       "<span style=\"font-weight: bold\">Total params</span>: 44.9 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 36                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 44.9 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 10                                                                                           \n",
       "\u001b[1mTotal params\u001b[0m: 44.9 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 36                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033e6cf4ef8b427cbd6ee9b36330a844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">661</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:07\u001b[0m,\u001b[1;36m661\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">661</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:07\u001b[0m,\u001b[1;36m661\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">444</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m444\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">491</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m491\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">522</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m522\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m678\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">866</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m866\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">912</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:10\u001b[0m,\u001b[1;36m912\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaefb1cda6e4e76904b9b4cdab48e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_35a359bd-3623-4171-bf63-1d39b4f85e71.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_35a359bd-3623-4171-bf63-1d39b4f85e71.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01445439770745928</span>. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:19\u001b[0m,\u001b[1;36m101\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.01445439770745928\u001b[0m. For plot \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:12:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:12:19\u001b[0m,\u001b[1;36m117\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   26.9 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   26.9 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 27.0 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 27.0 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 28                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 27.0 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 27.0 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 28                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b38707b2862480eb4af2e3380ea9580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">573</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:09\u001b[0m,\u001b[1;36m573\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">573</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:09\u001b[0m,\u001b[1;36m573\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">976</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:11\u001b[0m,\u001b[1;36m976\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">013</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:12\u001b[0m,\u001b[1;36m013\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">034</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:12\u001b[0m,\u001b[1;36m034\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">175</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:12\u001b[0m,\u001b[1;36m175\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:12\u001b[0m,\u001b[1;36m238\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">269</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:12\u001b[0m,\u001b[1;36m269\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e79036983014e799a87f12b3e3895fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_a8bd039a-17f8-4b83-a7f4-f4bf6d507936.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_a8bd039a-17f8-4b83-a7f4-f4bf6d507936.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">990</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:21\u001b[0m,\u001b[1;36m990\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:17:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">994</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:17:21\u001b[0m,\u001b[1;36m994\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   44.8 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   44.8 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 44.9 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 10                                                                                           \n",
       "<span style=\"font-weight: bold\">Total params</span>: 44.9 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 36                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 44.9 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 10                                                                                           \n",
       "\u001b[1mTotal params\u001b[0m: 44.9 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 36                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafe94900d2d4b06bd881a2305f1b768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">237</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:03\u001b[0m,\u001b[1;36m237\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">237</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:03\u001b[0m,\u001b[1;36m237\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">074</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m074\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m105\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">137</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m137\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">293</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m293\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m340\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">371</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:06\u001b[0m,\u001b[1;36m371\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b31dd45471749c9b0b62097dd0e2477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.017378008287493765\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_18a7360b-2b4f-463a-b8b8-b33943e74163.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_18a7360b-2b4f-463a-b8b8-b33943e74163.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">704</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017378008287493765</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:12\u001b[0m,\u001b[1;36m704\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.017378008287493765\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:23:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">704</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:23:12\u001b[0m,\u001b[1;36m704\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone    9.0 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer       0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone    9.0 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer       0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 9.0 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 9.0 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 18                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 9.0 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 9.0 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 18                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4353cbe11e4fcd9be444aac8e86617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">323</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:27\u001b[0m,\u001b[1;36m323\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">323</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:27\u001b[0m,\u001b[1;36m323\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">987</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:28\u001b[0m,\u001b[1;36m987\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">027</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:29\u001b[0m,\u001b[1;36m027\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">045</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:29\u001b[0m,\u001b[1;36m045\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">201</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:29\u001b[0m,\u001b[1;36m201\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:29\u001b[0m,\u001b[1;36m248\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:29\u001b[0m,\u001b[1;36m280\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc243cac61c43b5b90b7b035445ae79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_48716e9f-3d06-46a7-82d3-8c4c2001acda.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_48716e9f-3d06-46a7-82d3-8c4c2001acda.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">048</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025118864315095822</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:36\u001b[0m,\u001b[1;36m048\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.025118864315095822\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:25:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">050</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:25:36\u001b[0m,\u001b[1;36m050\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   13.4 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer       0  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   13.4 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer       0  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 13.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 20                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 13.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 13.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 20                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5122798ccc1f4c618a2413af7ab452f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">939</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:45\u001b[0m,\u001b[1;36m939\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">955</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:45\u001b[0m,\u001b[1;36m955\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">771</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:47\u001b[0m,\u001b[1;36m771\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">818</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:47\u001b[0m,\u001b[1;36m818\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">834</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:47\u001b[0m,\u001b[1;36m834\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">997</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:47\u001b[0m,\u001b[1;36m997\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">040</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:48\u001b[0m,\u001b[1;36m040\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:48\u001b[0m,\u001b[1;36m102\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1552aac2ab3e49f9a472e5f6b3e4f861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.030199517204020192\n",
      "Restoring states from the checkpoint path at H:\\masterarbeit_python\\.lr_find_eeb20192-5e7f-49c3-b1b6-6bdc1212924c.ckpt\n",
      "Restored all states from the checkpoint at H:\\masterarbeit_python\\.lr_find_eeb20192-5e7f-49c3-b1b6-6bdc1212924c.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">570</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030199517204020192</span>. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:57\u001b[0m,\u001b[1;36m570\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.030199517204020192\u001b[0m. For plot\n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:29:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">570</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:29:57\u001b[0m,\u001b[1;36m570\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> _backbone         GANDALFBackbone   40.3 K  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> _embedding_layer  Embedding1dLayer      54  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> _head             Sequential            58  train \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> loss              CrossEntropyLoss       0  train \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m _backbone         GANDALFBackbone   40.3 K  train \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m _embedding_layer  Embedding1dLayer      54  train \n",
       "\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m _head             Sequential            58  train \n",
       "\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m loss              CrossEntropyLoss       0  train \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 40.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 40.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 34                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 40.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 40.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 34                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce440d430994c42b7ced46213a32cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:34:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">398</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:34:44\u001b[0m,\u001b[1;36m398\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:34:44</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">398</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m20:34:44\u001b[0m,\u001b[1;36m398\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gandalf_study = optuna.create_study(direction=\"maximize\")\n",
    "gandalf_study.optimize(GANDALF_Optimization, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b78d45bc-c0e6-47bc-bb27-43fa402152d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gflu_stages': 6, 'gflu_dropout': 0.18162968859851508, 'gflu_feature_init_sparsity': 0.33034707998094537, 'learnable_sparsity': False, 'embedding_dropout': 0.17899966066777218, 'batch_norm_continuous_input': True, 'learning_rate': 4.572560736461007e-05}\n",
      "Best F1 score: 0.8986825496869573\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", gandalf_study.best_params)\n",
    "print(\"Best F1 score:\", gandalf_study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
